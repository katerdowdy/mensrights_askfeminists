{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: 'AskFeminists' vs. 'MensRights'\n",
    "## Part C: Vectorizing & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Vectorizing**](#vector)\n",
    "\n",
    "Before combining the two subreddit dataframes into one for modeling, I tested out different vectorizing options on both to compare the two subreddits. I tried out CountVectorizer and TFIDF on the AskFeminists content setting ngrams = (3-5) to get the top phrases from both: TFIDF was less repetitive than the CountVectorizer so I used TFIDF for vectorizing throughout. \n",
    "\n",
    "I tried vectorizing on ngrams 3-5, 1-2, and just single words, and created two custom lists of stopwords: one with all English stopwords and words in common between the top 100 words (without stopwords) from each subreddit; the second list of stopwords was created after fitting TFIDF on both subreddits (with stopwords = the first custom list) and taking the common words from the top 100 lists of words for each subreddit again. I tested models with both sets of stopwords and ultimately they didn't improve the model over using stopwords = 'english.')\n",
    "\n",
    "[**Modeling**](#model)\n",
    "\n",
    "I primarily tested two models on the lemmatized text: Logistic Regression and Random Forest Classifier. Starting with Logistic Regression, I tried several different parameters for vectorizing with TFIDF and the best parameters were 125,000 max features, ngrams = 1-4, and stopwords = 'English'.\n",
    "\n",
    "The best test score I had ended up being on Logistic Regression (76% accuracy compared to baseline of 50%). This model was overfit on the training data (85% accuracy), and setting lower max_features closed the gap to a 3% difference between training/testing scores but also lowered the test scores. \n",
    "\n",
    "The best training score I had was with RandomForestClassifier, n_estimators = 100 and max_depth = 880, which gave me 98% on training data (but just 72% on the test data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = pd.read_csv('./men_clean_lem')\n",
    "fem = pd.read_csv('./fem_clean_lem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing datasets, removing nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men['lems'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "men.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem['lems'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31221, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28955, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating smaller version for mensrights, same # rows as submissions for askfeminists\n",
    "men = men.sample(29000, replace = False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000, 8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing: Separate Analysis <a name=\"vector\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(3,5), max_features = 10000, stop_words = 'english')\n",
    "# Instantiate TFIDF\n",
    "tfidf = TfidfVectorizer(ngram_range = (3, 5), max_features = 10000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing fem for test\n",
    "fem_vect = vect.fit_transform(fem['lems'])\n",
    "# tfidfing fem for test\n",
    "fem_tfidf = tfidf.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words\n",
    "fem_vect_df = pd.DataFrame(fem_vect.toarray(), columns=vect.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reflect feminist perspective                  160\n",
       "feminist reflect feminist                     155\n",
       "feminist reflect feminist perspective         153\n",
       "come feminist reflect                         89 \n",
       "come feminist reflect feminist                89 \n",
       "come feminist reflect feminist perspective    89 \n",
       "level comment thread                          81 \n",
       "difference men woman                          80 \n",
       "feminist perspective comment                  74 \n",
       "doesn make sense                              74 \n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts\n",
    "vect_counts = fem_vect_df.sum(axis=0)\n",
    "vect_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doesn make sense                41.643637\n",
       "false rape accusation           36.430295\n",
       "difference men woman            32.162463\n",
       "traditional gender role         28.743795\n",
       "just don think                  27.087392\n",
       "reflect feminist perspective    26.147715\n",
       "don really know                 25.665504\n",
       "innocent proven guilty          24.234714\n",
       "rape sexual assault             24.215631\n",
       "gt don think                    23.785815\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at tfidf value counts\n",
    "tfidf_counts = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF seems to have done a better job of ignoring similar phrases than count vectorizer (ex. 'reflect feminist perspective') and the results are more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Top Phrases (TFIDF, ngrams 3-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "men_tfidf = tfidf.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false rape accusation     62.306164\n",
       "men right movement        60.595215\n",
       "men right issue           54.943756\n",
       "international men day     50.388507\n",
       "men right activist        39.744089\n",
       "gender pay gap            33.950820\n",
       "innocent proven guilty    33.152104\n",
       "pay child support         27.741176\n",
       "year old boy              22.977497\n",
       "doesn make sense          22.798229\n",
       "dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling top 10 phrases for men TFIDF with English stopwords, 3 are the same as fem\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing single words and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "fem_tfidf = tfidf.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf = tfidf.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wife ha             976.118614\n",
       "men family          729.171984\n",
       "fellow              677.549175\n",
       "thing               612.694714\n",
       "divorce             607.185712\n",
       "partner violence    605.775949\n",
       "lifestyle           550.282282\n",
       "june                537.683578\n",
       "voting              513.169382\n",
       "greatest            450.885487\n",
       "dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling top 10 words/phrases for fem TFIDF with English stopwords\n",
    "tfidf_counts_fem = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       868.976790\n",
       "men         826.650364\n",
       "wa          579.965131\n",
       "just        491.954841\n",
       "like        476.984193\n",
       "don         462.072782\n",
       "people      423.281936\n",
       "gt          407.147621\n",
       "right       403.265087\n",
       "feminist    391.284568\n",
       "dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling top 10 words/phrases for men TFIDF with English stopwords\n",
    "# interesting that feminist is one of the top 10 words for mensrights\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing single words & bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top ngrams 1-2\n",
    "top_100_fem_ngrams_stop = tfidf_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_ngrams_stop = tfidf_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar ngrams in both\n",
    "fem_men_ngrams = set(top_100_men_ngrams_stop.index) & set(top_100_fem_ngrams_stop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thing'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not a lot of similarity in smaller ngrams\n",
    "fem_men_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't a lot of similarity when comparing ngrams 1-2 between the two subs - AskFeminists has more common 2-word phrases (with stopwords removed) which suggest more similar contextualization of common words such as 'men' and 'women'. MensRights has very few 2-word phrases that are common, so the top 100 words/short phrases come with less context than the list for AskFeminists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing top words (ngrams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(max_features = 50000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "fem_tfidf = tfidf1.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf = tfidf1.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       1058.470342\n",
       "men         802.978127 \n",
       "feminist    751.930089 \n",
       "don         664.663076 \n",
       "think       655.181627 \n",
       "people      645.120908 \n",
       "like        581.666010 \n",
       "just        572.214255 \n",
       "wa          530.030073 \n",
       "gt          473.983548 \n",
       "dtype: float64"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for askfeminists\n",
    "tfidf_counts_fem = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       929.135323\n",
       "men         889.633840\n",
       "wa          594.643131\n",
       "just        509.152025\n",
       "don         493.963959\n",
       "like        489.428134\n",
       "people      441.916170\n",
       "right       433.626604\n",
       "feminist    407.702110\n",
       "gt          406.739804\n",
       "dtype: float64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for mensrights\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top ngrams 1-2\n",
    "top_100_fem_words_stop = tfidf_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_words_stop = tfidf_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar words in both\n",
    "fem_men_words = set(top_100_men_words_stop.index) & set(top_100_fem_words_stop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fem_men_words) # 77 of the words are in the top 100 for both! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actually',\n",
       " 'agree',\n",
       " 'bad',\n",
       " 'believe',\n",
       " 'better',\n",
       " 'case',\n",
       " 'child',\n",
       " 'come',\n",
       " 'comment',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'don',\n",
       " 'equality',\n",
       " 'fact',\n",
       " 'feel',\n",
       " 'female',\n",
       " 'feminism',\n",
       " 'feminist',\n",
       " 'gender',\n",
       " 'girl',\n",
       " 'going',\n",
       " 'good',\n",
       " 'group',\n",
       " 'gt',\n",
       " 'guy',\n",
       " 'ha',\n",
       " 'having',\n",
       " 'help',\n",
       " 'isn',\n",
       " 'issue',\n",
       " 'just',\n",
       " 'know',\n",
       " 'life',\n",
       " 'like',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'make',\n",
       " 'male',\n",
       " 'man',\n",
       " 'mean',\n",
       " 'men',\n",
       " 'need',\n",
       " 'people',\n",
       " 'person',\n",
       " 'point',\n",
       " 'post',\n",
       " 'problem',\n",
       " 'rape',\n",
       " 'read',\n",
       " 'really',\n",
       " 'reason',\n",
       " 'right',\n",
       " 'said',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'sex',\n",
       " 'sexual',\n",
       " 'society',\n",
       " 'sure',\n",
       " 'thanks',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'thought',\n",
       " 'time',\n",
       " 'use',\n",
       " 'victim',\n",
       " 'wa',\n",
       " 'want',\n",
       " 'way',\n",
       " 'white',\n",
       " 'woman',\n",
       " 'word',\n",
       " 'work',\n",
       " 'wrong',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_men_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Stopwords with Count Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect1 = CountVectorizer(max_features = 50000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to vect\n",
    "fem_vect = vect1.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_vect = vect1.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_vect_df = pd.DataFrame(fem_vect.toarray(), columns=vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_vect_df = pd.DataFrame(men_vect.toarray(), columns=vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       22124\n",
       "men         14360\n",
       "people      12063\n",
       "don         11036\n",
       "think       10884\n",
       "like        10296\n",
       "feminist    10242\n",
       "just        9659 \n",
       "wa          9085 \n",
       "gt          7853 \n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for askfeminists\n",
    "vect_counts_fem = fem_vect_df.sum(axis=0)\n",
    "vect_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       17622\n",
       "men         16400\n",
       "wa          10868\n",
       "just        7402 \n",
       "like        6929 \n",
       "people      6385 \n",
       "don         6367 \n",
       "gt          6236 \n",
       "right       5382 \n",
       "feminist    5265 \n",
       "dtype: int64"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for mensrights\n",
    "vect_counts_men = men_vect_df.sum(axis=0)\n",
    "vect_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top words\n",
    "top_100_fem_words_stop = vect_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_words_stop = vect_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar words in both\n",
    "fem_men_words = set(top_100_men_words_stop.index) & set(top_100_fem_words_stop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fem_men_words) # about the same as tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Custom Stopwords, Second Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brian's code, setting stop words equal to intersection terms in top 100 using count vectorizer\n",
    "stop_words_1 = text.ENGLISH_STOP_WORDS.union(['actually',\n",
    " 'agree',\n",
    " 'bad',\n",
    " 'believe',\n",
    " 'better',\n",
    " 'case',\n",
    " 'child',\n",
    " 'come',\n",
    " 'comment',\n",
    " 'did',\n",
    " 'didn',\n",
    " 'doe',\n",
    " 'doesn',\n",
    " 'don',\n",
    " 'equality',\n",
    " 'fact',\n",
    " 'feel',\n",
    " 'female',\n",
    " 'feminism',\n",
    " 'feminist',\n",
    " 'gender',\n",
    " 'girl',\n",
    " 'going',\n",
    " 'good',\n",
    " 'group',\n",
    " 'gt',\n",
    " 'guy',\n",
    " 'ha',\n",
    " 'having',\n",
    " 'help',\n",
    " 'isn',\n",
    " 'issue',\n",
    " 'just',\n",
    " 'know',\n",
    " 'life',\n",
    " 'like',\n",
    " 'look',\n",
    " 'lot',\n",
    " 'make',\n",
    " 'male',\n",
    " 'man',\n",
    " 'mean',\n",
    " 'men',\n",
    " 'need',\n",
    " 'people',\n",
    " 'person',\n",
    " 'point',\n",
    " 'post',\n",
    " 'problem',\n",
    " 'rape',\n",
    " 'read',\n",
    " 'really',\n",
    " 'reason',\n",
    " 'right',\n",
    " 'said',\n",
    " 'say',\n",
    " 'saying',\n",
    " 'sex',\n",
    " 'sexual',\n",
    " 'society',\n",
    " 'sure',\n",
    " 'thanks',\n",
    " 'thing',\n",
    " 'think',\n",
    " 'thought',\n",
    " 'time',\n",
    " 'use',\n",
    " 'victim',\n",
    " 'wa',\n",
    " 'want',\n",
    " 'way',\n",
    " 'white',\n",
    " 'woman',\n",
    " 'word',\n",
    " 'work',\n",
    " 'wrong',\n",
    " 'yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(max_features = 50000, stop_words = stop_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "fem_tfidf = tfidf2.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf = tfidf2.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question      377.321037\n",
       "yes           252.605901\n",
       "idea          223.008711\n",
       "different     218.713998\n",
       "agree         211.648976\n",
       "understand    204.031619\n",
       "answer        201.841974\n",
       "opinion       195.025050\n",
       "example       191.834915\n",
       "trans         182.431184\n",
       "dtype: float64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for askfeminists, after removing most common words\n",
    "tfidf_counts_fem = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "day        203.099967\n",
       "archive    202.091099\n",
       "boy        192.386067\n",
       "law        172.444206\n",
       "sub        169.660586\n",
       "got        163.834252\n",
       "article    163.380445\n",
       "shit       163.243384\n",
       "care       159.536521\n",
       "yes        157.502748\n",
       "dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for mensrights, after removing most common words\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top words\n",
    "top_100_fem_words_stop1 = tfidf_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_words_stop1 = tfidf_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar words in both\n",
    "fem_men_words1 = set(top_100_men_words_stop1.index) & set(top_100_fem_words_stop1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fem_men_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agree',\n",
       " 'aren',\n",
       " 'argument',\n",
       " 'article',\n",
       " 'assault',\n",
       " 'best',\n",
       " 'boy',\n",
       " 'care',\n",
       " 'change',\n",
       " 'claim',\n",
       " 'consent',\n",
       " 'crime',\n",
       " 'day',\n",
       " 'different',\n",
       " 'doing',\n",
       " 'equal',\n",
       " 'evidence',\n",
       " 'exactly',\n",
       " 'example',\n",
       " 'far',\n",
       " 'friend',\n",
       " 'getting',\n",
       " 'got',\n",
       " 'hard',\n",
       " 'hate',\n",
       " 'idea',\n",
       " 'job',\n",
       " 'kind',\n",
       " 'law',\n",
       " 'let',\n",
       " 'making',\n",
       " 'masculinity',\n",
       " 'matter',\n",
       " 'maybe',\n",
       " 'movement',\n",
       " 'opinion',\n",
       " 'place',\n",
       " 'power',\n",
       " 'pretty',\n",
       " 'probably',\n",
       " 'question',\n",
       " 'read',\n",
       " 'real',\n",
       " 'seen',\n",
       " 'sound',\n",
       " 'stop',\n",
       " 'study',\n",
       " 'sub',\n",
       " 'talk',\n",
       " 'talking',\n",
       " 'tell',\n",
       " 'thanks',\n",
       " 'toxic',\n",
       " 'true',\n",
       " 'try',\n",
       " 'understand',\n",
       " 'used',\n",
       " 'violence',\n",
       " 'word',\n",
       " 'wouldn',\n",
       " 'yeah',\n",
       " 'yes'}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_men_words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Custom Stopwords (2nd layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added common words from last version to common words from previous version\n",
    "stop_words_2 = text.ENGLISH_STOP_WORDS.union(['agree',\n",
    " 'aren',\n",
    " 'argument',\n",
    " 'article',\n",
    " 'assault',\n",
    " 'best',\n",
    " 'boy',\n",
    " 'care',\n",
    " 'change',\n",
    " 'claim',\n",
    " 'consent',\n",
    " 'crime',\n",
    " 'day',\n",
    " 'different',\n",
    " 'doing',\n",
    " 'equal',\n",
    " 'evidence',\n",
    " 'exactly',\n",
    " 'example',\n",
    " 'far',\n",
    " 'friend',\n",
    " 'getting',\n",
    " 'got',\n",
    " 'hard',\n",
    " 'hate',\n",
    " 'idea',\n",
    " 'job',\n",
    " 'kind',\n",
    " 'law',\n",
    " 'let',\n",
    " 'making',\n",
    " 'masculinity',\n",
    " 'matter',\n",
    " 'maybe',\n",
    " 'movement',\n",
    " 'opinion',\n",
    " 'place',\n",
    " 'power',\n",
    " 'pretty',\n",
    " 'probably',\n",
    " 'question',\n",
    " 'read',\n",
    " 'real',\n",
    " 'seen',\n",
    " 'sound',\n",
    " 'stop',\n",
    " 'study',\n",
    " 'sub',\n",
    " 'talk',\n",
    " 'talking',\n",
    " 'tell',\n",
    " 'thanks',\n",
    " 'toxic',\n",
    " 'true',\n",
    " 'try',\n",
    " 'understand',\n",
    " 'used',\n",
    " 'violence',\n",
    " 'word',\n",
    " 'wouldn',\n",
    " 'yeah',\n",
    " 'yes',\n",
    "'actually',\n",
    " 'agree',\n",
    " 'bad',\n",
    " 'believe',\n",
    " 'better',\n",
    " 'case',\n",
    " 'child',\n",
    " 'come',\n",
    " 'comment',\n",
    " 'did',\n",
    " 'didn',\n",
    " 'doe',\n",
    " 'doesn',\n",
    " 'don',\n",
    " 'equality',\n",
    " 'fact',\n",
    " 'feel',\n",
    " 'female',\n",
    " 'feminism',\n",
    " 'feminist',\n",
    " 'gender',\n",
    " 'girl',\n",
    " 'going',\n",
    " 'good',\n",
    " 'gt',\n",
    " 'guy',\n",
    " 'ha',\n",
    " 'isn',\n",
    " 'issue',\n",
    " 'just',\n",
    " 'know',\n",
    " 'life',\n",
    " 'like',\n",
    " 'look',\n",
    " 'lot',\n",
    " 'make',\n",
    " 'male',\n",
    " 'man',\n",
    " 'mean',\n",
    " 'men',\n",
    " 'nan',\n",
    " 'need',\n",
    " 'people',\n",
    " 'person',\n",
    " 'point',\n",
    " 'post',\n",
    " 'problem',\n",
    " 'rape',\n",
    " 'read',\n",
    " 'really',\n",
    " 'reason',\n",
    " 'right',\n",
    " 'said',\n",
    " 'say',\n",
    " 'saying',\n",
    " 'sex',\n",
    " 'sexual',\n",
    " 'sure',\n",
    " 'thanks',\n",
    " 'thing',\n",
    " 'think',\n",
    " 'time',\n",
    " 'use',\n",
    " 'wa',\n",
    " 'want',\n",
    " 'way',\n",
    " 'white',\n",
    " 'woman',\n",
    " 'word',\n",
    " 'work',\n",
    " 'wrong',\n",
    " 'yes', 'aren',\n",
    " 'argument',\n",
    " 'assault',\n",
    " 'best',\n",
    " 'boy',\n",
    " 'care',\n",
    " 'change',\n",
    " 'claim',\n",
    " 'consent',\n",
    " 'day',\n",
    " 'different',\n",
    " 'doing',\n",
    " 'equal',\n",
    " 'evidence',\n",
    " 'exactly',\n",
    " 'far',\n",
    " 'getting',\n",
    " 'got',\n",
    " 'group',\n",
    " 'hate',\n",
    " 'having',\n",
    " 'help',\n",
    " 'idea',\n",
    " 'job',\n",
    " 'kind',\n",
    " 'law',\n",
    " 'le',\n",
    " 'let',\n",
    " 'making',\n",
    " 'matter',\n",
    " 'maybe',\n",
    " 'movement',\n",
    " 'ok',\n",
    " 'place',\n",
    " 'power',\n",
    " 'pretty',\n",
    " 'probably',\n",
    " 'question',\n",
    " 'real',\n",
    " 'society',\n",
    " 'sound',\n",
    " 'stop',\n",
    " 'sub',\n",
    " 'support',\n",
    " 'talk',\n",
    " 'talking',\n",
    " 'tell',\n",
    " 'thought',\n",
    " 'true',\n",
    " 'try',\n",
    " 'trying',\n",
    " 'understand',\n",
    " 'used',\n",
    " 'victim',\n",
    " 'violence',\n",
    " 'world',\n",
    " 'wouldn',\n",
    " 'yeah',\n",
    " 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining into one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen = pd.concat([fem, men], axis=0, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57955, 8)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femvmen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'text', 'type', 'subreddit', 'removed', 'deleted',\n",
       "       'clean_text_stop', 'lems'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femvmen.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen = pd.get_dummies(femvmen, columns=['subreddit'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen.drop(columns = \"Unnamed: 0\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen = pd.DataFrame(femvmen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen.to_csv('./femvmen_lem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'text', 'type', 'removed', 'deleted', 'clean_text_stop',\n",
       "       'lems', 'subreddit_MensRights'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femvmen.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Vars/TrainTestSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = femvmen['lems']\n",
    "y = femvmen['subreddit_MensRights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500388\n",
       "0    0.499612\n",
       "Name: subreddit_MensRights, dtype: float64"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True) # about 50% mensrights posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True, \n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500388\n",
       "0    0.499612\n",
       "Name: subreddit_MensRights, dtype: float64"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up multiple vectorizers with english stopwords, ngrams 1-4, different features\n",
    "tfidf = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        'english')\n",
    "tfidf2 = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf3 = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        stop_words_2)\n",
    "tfidf4 = TfidfVectorizer(ngram_range = (1, 4), max_features = 100000, stop_words = \n",
    "                        'english')\n",
    "tfidf5 = TfidfVectorizer(ngram_range = (1, 4), max_features = 100000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf6 = TfidfVectorizer(ngram_range = (1, 4), max_features = 100000, stop_words = \n",
    "                        stop_words_2)\n",
    "tfidf7 = TfidfVectorizer(ngram_range = (1, 4), max_features = 50000, stop_words = \n",
    "                        'english')\n",
    "tfidf8 = TfidfVectorizer(ngram_range = (1, 4), max_features = 50000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf9 = TfidfVectorizer(ngram_range = (1, 4), max_features = 50000, stop_words = \n",
    "                        stop_words_2)\n",
    "tfidf10 = TfidfVectorizer(ngram_range = (1, 4), max_features = 5000, stop_words = \n",
    "                        'english')\n",
    "tfidf11 = TfidfVectorizer(ngram_range = (1, 4), max_features = 5000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf12 = TfidfVectorizer(ngram_range = (1, 4), max_features = 5000, stop_words = \n",
    "                        stop_words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit transform train sets\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_train_tf2 = tfidf2.fit_transform(X_train)\n",
    "X_train_tf3 = tfidf3.fit_transform(X_train)\n",
    "X_train_tf4 = tfidf4.fit_transform(X_train)\n",
    "X_train_tf5 = tfidf5.fit_transform(X_train)\n",
    "X_train_tf6 = tfidf6.fit_transform(X_train)\n",
    "X_train_tf7 = tfidf7.fit_transform(X_train)\n",
    "X_train_tf8 = tfidf8.fit_transform(X_train)\n",
    "X_train_tf9 = tfidf9.fit_transform(X_train)\n",
    "X_train_tf10 = tfidf10.fit_transform(X_train)\n",
    "X_train_tf11 = tfidf11.fit_transform(X_train)\n",
    "X_train_tf12 = tfidf12.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test set\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "X_test_tf2 = tfidf2.transform(X_test)\n",
    "X_test_tf3 = tfidf3.transform(X_test)\n",
    "X_test_tf4 = tfidf4.transform(X_test)\n",
    "X_test_tf5 = tfidf5.transform(X_test)\n",
    "X_test_tf6 = tfidf6.transform(X_test)\n",
    "X_test_tf7 = tfidf7.transform(X_test)\n",
    "X_test_tf8 = tfidf8.transform(X_test)\n",
    "X_test_tf9 = tfidf9.transform(X_test)\n",
    "X_test_tf10 = tfidf10.transform(X_test)\n",
    "X_test_tf11 = tfidf11.transform(X_test)\n",
    "X_test_tf12 = tfidf12.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best versions of vectorized data for logistic regression are with stopwords = 'english' and max features = 125000. With very few features (10K or less) the gap between train and test scores closes to 3% but the test score continues to drop with fewer than 100K features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "X_train_tf = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545854542317315"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7605038391855751"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8391640065568113"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "logreg.fit(X_train_tf2, y_train)\n",
    "logreg.score(X_train_tf2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7380726425675093"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8343326719006126"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "logreg.fit(X_train_tf3, y_train)\n",
    "logreg.score(X_train_tf3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7316883789146752"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8502070571995514"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4\n",
    "logreg.fit(X_train_tf4, y_train)\n",
    "logreg.score(X_train_tf4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7510999913726167"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf4, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355620740229488"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 5\n",
    "logreg.fit(X_train_tf5, y_train)\n",
    "logreg.score(X_train_tf5, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.738849107065827"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf5, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8304287809507377"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 6\n",
    "logreg.fit(X_train_tf6, y_train)\n",
    "logreg.score(X_train_tf6, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.731860926580968"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf6, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8377404883098956"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 7\n",
    "logreg.fit(X_train_tf7, y_train)\n",
    "logreg.score(X_train_tf7, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7490294193771029"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf7, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824346475713916"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 8\n",
    "logreg.fit(X_train_tf8, y_train)\n",
    "logreg.score(X_train_tf8, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7391079285652662"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf8, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8187818134759728"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 9\n",
    "logreg.fit(X_train_tf9, y_train)\n",
    "logreg.score(X_train_tf9, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7308256405832111"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf9, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7789664394789061"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 10\n",
    "logreg.fit(X_train_tf10, y_train)\n",
    "logreg.score(X_train_tf10, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7393667500647054"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf10, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7708135622465706"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 11\n",
    "logreg.fit(X_train_tf11, y_train)\n",
    "logreg.score(X_train_tf11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7247001984298163"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf11, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7659606591320852"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 12\n",
    "logreg.fit(X_train_tf12, y_train)\n",
    "logreg.score(X_train_tf12, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7149512552842723"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf12, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Influential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting on best model\n",
    "logreg.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(coefficients, columns = tfidf2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aa policy</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aabg</th>\n",
       "      <th>aap</th>\n",
       "      <th>aauw</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abctv</th>\n",
       "      <th>abctv scottmorrisonmp</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdicate responsibility</th>\n",
       "      <th>abdication</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abducted</th>\n",
       "      <th>abduction</th>\n",
       "      <th>aberrant</th>\n",
       "      <th>aberration</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhorrent</th>\n",
       "      <th>abide</th>\n",
       "      <th>abide rule</th>\n",
       "      <th>abiding</th>\n",
       "      <th>abigail</th>\n",
       "      <th>ability</th>\n",
       "      <th>ability ability</th>\n",
       "      <th>ability associated</th>\n",
       "      <th>ability associated chess</th>\n",
       "      <th>ability associated chess assumption</th>\n",
       "      <th>ability based</th>\n",
       "      <th>ability birth</th>\n",
       "      <th>ability career</th>\n",
       "      <th>ability change</th>\n",
       "      <th>ability choose</th>\n",
       "      <th>ability coder</th>\n",
       "      <th>ability commit</th>\n",
       "      <th>ability communicate</th>\n",
       "      <th>ability compete</th>\n",
       "      <th>ability consent</th>\n",
       "      <th>ability control</th>\n",
       "      <th>ability decision</th>\n",
       "      <th>ability earn</th>\n",
       "      <th>ability experience</th>\n",
       "      <th>ability influence</th>\n",
       "      <th>ability job</th>\n",
       "      <th>ability lead</th>\n",
       "      <th>ability obtain</th>\n",
       "      <th>ability pay</th>\n",
       "      <th>ability perceive</th>\n",
       "      <th>ability power</th>\n",
       "      <th>ability prevent</th>\n",
       "      <th>ability provide</th>\n",
       "      <th>ability reproduce</th>\n",
       "      <th>ability responsibility</th>\n",
       "      <th>ability speak</th>\n",
       "      <th>ability survive</th>\n",
       "      <th>ability understand</th>\n",
       "      <th>ability wanted</th>\n",
       "      <th>ability willingness</th>\n",
       "      <th>ability willingness negotiate</th>\n",
       "      <th>ability willingness negotiate salary</th>\n",
       "      <th>abit</th>\n",
       "      <th>abit bbc</th>\n",
       "      <th>abject</th>\n",
       "      <th>abject poverty</th>\n",
       "      <th>ablated</th>\n",
       "      <th>able</th>\n",
       "      <th>able abandon</th>\n",
       "      <th>able abandon parental</th>\n",
       "      <th>able abandon parental obligation</th>\n",
       "      <th>able able</th>\n",
       "      <th>able abortion</th>\n",
       "      <th>able accept</th>\n",
       "      <th>able accept trash</th>\n",
       "      <th>able accept trash quality</th>\n",
       "      <th>able access</th>\n",
       "      <th>able accrue</th>\n",
       "      <th>able accrue wealth</th>\n",
       "      <th>able accrue wealth unlike</th>\n",
       "      <th>able achieve</th>\n",
       "      <th>able acknowledge</th>\n",
       "      <th>able act</th>\n",
       "      <th>able address</th>\n",
       "      <th>able afford</th>\n",
       "      <th>able afford able</th>\n",
       "      <th>able afford able day</th>\n",
       "      <th>able afford private</th>\n",
       "      <th>able agree</th>\n",
       "      <th>able analyze</th>\n",
       "      <th>able analyze situation</th>\n",
       "      <th>able angry</th>\n",
       "      <th>able angry allowed</th>\n",
       "      <th>able angry allowed arent</th>\n",
       "      <th>able answer</th>\n",
       "      <th>able answer question</th>\n",
       "      <th>able appeal</th>\n",
       "      <th>able apply</th>\n",
       "      <th>able appreciate</th>\n",
       "      <th>able argument</th>\n",
       "      <th>able ask</th>\n",
       "      <th>able assault</th>\n",
       "      <th>able assist</th>\n",
       "      <th>able assist husband</th>\n",
       "      <th>able assist husband trade</th>\n",
       "      <th>able avoid</th>\n",
       "      <th>able aware</th>\n",
       "      <th>able away</th>\n",
       "      <th>able baby</th>\n",
       "      <th>able beat</th>\n",
       "      <th>able block</th>\n",
       "      <th>able bodied</th>\n",
       "      <th>able bodied ci</th>\n",
       "      <th>able bodied disease</th>\n",
       "      <th>able bodied disease free</th>\n",
       "      <th>able bodied neurotypical</th>\n",
       "      <th>able body</th>\n",
       "      <th>able break</th>\n",
       "      <th>able break agreement</th>\n",
       "      <th>able breastfeed</th>\n",
       "      <th>able breathe</th>\n",
       "      <th>able bring</th>\n",
       "      <th>able build</th>\n",
       "      <th>able buy</th>\n",
       "      <th>able buy house</th>\n",
       "      <th>able care</th>\n",
       "      <th>able catch</th>\n",
       "      <th>able catch damn</th>\n",
       "      <th>able catch damn diagnostic</th>\n",
       "      <th>able change</th>\n",
       "      <th>able change maybe</th>\n",
       "      <th>able change mind</th>\n",
       "      <th>able choice</th>\n",
       "      <th>able choose</th>\n",
       "      <th>able choose father</th>\n",
       "      <th>able claim</th>\n",
       "      <th>able clear</th>\n",
       "      <th>able close</th>\n",
       "      <th>able comfortable</th>\n",
       "      <th>able comfortable home</th>\n",
       "      <th>able comfortable home guarantee</th>\n",
       "      <th>able company</th>\n",
       "      <th>able compete</th>\n",
       "      <th>able compete ci</th>\n",
       "      <th>able compete sport</th>\n",
       "      <th>able compete sport line</th>\n",
       "      <th>able complain</th>\n",
       "      <th>able concede</th>\n",
       "      <th>able concede status</th>\n",
       "      <th>able concede status father</th>\n",
       "      <th>able connect</th>\n",
       "      <th>able consent</th>\n",
       "      <th>able consent tell</th>\n",
       "      <th>able consider</th>\n",
       "      <th>able continue</th>\n",
       "      <th>able contribute</th>\n",
       "      <th>able control</th>\n",
       "      <th>able convince</th>\n",
       "      <th>able convince reddit</th>\n",
       "      <th>able cope</th>\n",
       "      <th>able court</th>\n",
       "      <th>able court order</th>\n",
       "      <th>able create</th>\n",
       "      <th>able cum</th>\n",
       "      <th>able data</th>\n",
       "      <th>able date</th>\n",
       "      <th>able day</th>\n",
       "      <th>able day recover</th>\n",
       "      <th>able day recover clinic</th>\n",
       "      <th>able deal</th>\n",
       "      <th>able decide</th>\n",
       "      <th>able decision</th>\n",
       "      <th>able defend</th>\n",
       "      <th>able definitely</th>\n",
       "      <th>able destroy</th>\n",
       "      <th>able determine</th>\n",
       "      <th>able directly</th>\n",
       "      <th>able discern</th>\n",
       "      <th>able discriminate</th>\n",
       "      <th>able discus</th>\n",
       "      <th>able discussion</th>\n",
       "      <th>able distinguish</th>\n",
       "      <th>able dress</th>\n",
       "      <th>able empathize</th>\n",
       "      <th>able evidence</th>\n",
       "      <th>able experience</th>\n",
       "      <th>able explain</th>\n",
       "      <th>able explain biological</th>\n",
       "      <th>able explain biological basis</th>\n",
       "      <th>able express</th>\n",
       "      <th>able fight</th>\n",
       "      <th>able financial</th>\n",
       "      <th>able force</th>\n",
       "      <th>able forward</th>\n",
       "      <th>able fully</th>\n",
       "      <th>able gain</th>\n",
       "      <th>able game</th>\n",
       "      <th>able handle</th>\n",
       "      <th>able happens</th>\n",
       "      <th>able hold</th>\n",
       "      <th>able identify</th>\n",
       "      <th>able incorporate</th>\n",
       "      <th>able infer</th>\n",
       "      <th>able infer kind</th>\n",
       "      <th>able infer kind iat</th>\n",
       "      <th>able inform</th>\n",
       "      <th>able interpret</th>\n",
       "      <th>able job</th>\n",
       "      <th>able laid</th>\n",
       "      <th>able learn</th>\n",
       "      <th>able leave</th>\n",
       "      <th>able legally</th>\n",
       "      <th>able leverage</th>\n",
       "      <th>able lift</th>\n",
       "      <th>able lift pound</th>\n",
       "      <th>able live</th>\n",
       "      <th>able ma</th>\n",
       "      <th>able ma line</th>\n",
       "      <th>able maintain</th>\n",
       "      <th>able manipulate</th>\n",
       "      <th>able marry</th>\n",
       "      <th>able meet</th>\n",
       "      <th>able million</th>\n",
       "      <th>able navigate</th>\n",
       "      <th>able note</th>\n",
       "      <th>able note moderator</th>\n",
       "      <th>able offer</th>\n",
       "      <th>able opt</th>\n",
       "      <th>able participate</th>\n",
       "      <th>able past</th>\n",
       "      <th>able pay</th>\n",
       "      <th>able perform</th>\n",
       "      <th>able pick</th>\n",
       "      <th>able play</th>\n",
       "      <th>able power</th>\n",
       "      <th>able pregnant</th>\n",
       "      <th>able protect</th>\n",
       "      <th>able prove</th>\n",
       "      <th>able provide</th>\n",
       "      <th>able pull</th>\n",
       "      <th>able read</th>\n",
       "      <th>able recognize</th>\n",
       "      <th>able record</th>\n",
       "      <th>able reduce</th>\n",
       "      <th>able reduced</th>\n",
       "      <th>able reduced free</th>\n",
       "      <th>able reduced free home</th>\n",
       "      <th>able refute</th>\n",
       "      <th>able relate</th>\n",
       "      <th>able remain</th>\n",
       "      <th>able remove</th>\n",
       "      <th>able run</th>\n",
       "      <th>able sell</th>\n",
       "      <th>able separate</th>\n",
       "      <th>able separate conversation</th>\n",
       "      <th>able separate conversation fgm</th>\n",
       "      <th>able set</th>\n",
       "      <th>able sexist</th>\n",
       "      <th>able share</th>\n",
       "      <th>able shut</th>\n",
       "      <th>able sign</th>\n",
       "      <th>able similar</th>\n",
       "      <th>able single</th>\n",
       "      <th>able solve</th>\n",
       "      <th>able speak</th>\n",
       "      <th>able speak clearly</th>\n",
       "      <th>able speak clearly unambiguously</th>\n",
       "      <th>able spend</th>\n",
       "      <th>able spend hour</th>\n",
       "      <th>able spot</th>\n",
       "      <th>able stand</th>\n",
       "      <th>able stay</th>\n",
       "      <th>able stop</th>\n",
       "      <th>able straight</th>\n",
       "      <th>able strong</th>\n",
       "      <th>able study</th>\n",
       "      <th>able subpoena</th>\n",
       "      <th>able subpoena law</th>\n",
       "      <th>able survive</th>\n",
       "      <th>able talk</th>\n",
       "      <th>able talk feeling</th>\n",
       "      <th>able talk feeling scared</th>\n",
       "      <th>able teach</th>\n",
       "      <th>able tell</th>\n",
       "      <th>able toilet</th>\n",
       "      <th>able trust</th>\n",
       "      <th>able try</th>\n",
       "      <th>able turn</th>\n",
       "      <th>able twist</th>\n",
       "      <th>able understand</th>\n",
       "      <th>able understand concept</th>\n",
       "      <th>able understand concept emotional</th>\n",
       "      <th>able vote</th>\n",
       "      <th>able vote learn</th>\n",
       "      <th>able walk</th>\n",
       "      <th>able watch</th>\n",
       "      <th>able wear</th>\n",
       "      <th>able willing</th>\n",
       "      <th>able win</th>\n",
       "      <th>able withdraw</th>\n",
       "      <th>able withdraw consent</th>\n",
       "      <th>able withdraw consent morning</th>\n",
       "      <th>able word</th>\n",
       "      <th>abled</th>\n",
       "      <th>ableism</th>\n",
       "      <th>ableism calling</th>\n",
       "      <th>ableism calling believed</th>\n",
       "      <th>ableism calling believed kid</th>\n",
       "      <th>ableism homophobia</th>\n",
       "      <th>ableist</th>\n",
       "      <th>ableist change</th>\n",
       "      <th>ableist change dime</th>\n",
       "      <th>ableist change dime challenging</th>\n",
       "      <th>ableist language</th>\n",
       "      <th>ableist preference</th>\n",
       "      <th>ableist remark</th>\n",
       "      <th>ableist slur</th>\n",
       "      <th>ableist slur cognitive</th>\n",
       "      <th>ableist slur cognitive developmental</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abnormal behavior</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>abnormality hilarious</th>\n",
       "      <th>abnormality hilarious banana</th>\n",
       "      <th>abnormality hilarious banana useless</th>\n",
       "      <th>abnormally</th>\n",
       "      <th>aboard</th>\n",
       "      <th>aboard plane</th>\n",
       "      <th>abolish</th>\n",
       "      <th>abolish prison</th>\n",
       "      <th>abolished</th>\n",
       "      <th>abolishing</th>\n",
       "      <th>abolishment</th>\n",
       "      <th>abolition</th>\n",
       "      <th>abolitionism</th>\n",
       "      <th>abolitionist</th>\n",
       "      <th>abominable</th>\n",
       "      <th>abomination</th>\n",
       "      <th>abomination important</th>\n",
       "      <th>abomination important moral</th>\n",
       "      <th>abomination important moral consideration</th>\n",
       "      <th>aboriginal</th>\n",
       "      <th>abort</th>\n",
       "      <th>abort adoption</th>\n",
       "      <th>abort baby</th>\n",
       "      <th>abort body</th>\n",
       "      <th>abort fetus</th>\n",
       "      <th>abort force</th>\n",
       "      <th>aborted</th>\n",
       "      <th>aborting</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortion able</th>\n",
       "      <th>abortion abortion</th>\n",
       "      <th>abortion adoption</th>\n",
       "      <th>abortion agree</th>\n",
       "      <th>abortion anti</th>\n",
       "      <th>abortion appointment</th>\n",
       "      <th>abortion aren</th>\n",
       "      <th>abortion arguing</th>\n",
       "      <th>abortion argument</th>\n",
       "      <th>abortion asked</th>\n",
       "      <th>abortion attacked</th>\n",
       "      <th>abortion available</th>\n",
       "      <th>abortion available assault</th>\n",
       "      <th>abortion available assault aside</th>\n",
       "      <th>abortion avoid</th>\n",
       "      <th>abortion avoid supporting</th>\n",
       "      <th>abortion avoid supporting maternity</th>\n",
       "      <th>abortion away</th>\n",
       "      <th>abortion away gay</th>\n",
       "      <th>abortion away gay marriage</th>\n",
       "      <th>abortion baby</th>\n",
       "      <th>abortion basically</th>\n",
       "      <th>abortion bc</th>\n",
       "      <th>abortion begin</th>\n",
       "      <th>abortion best</th>\n",
       "      <th>abortion birth</th>\n",
       "      <th>abortion birth control</th>\n",
       "      <th>abortion birth control purpose</th>\n",
       "      <th>abortion bodily</th>\n",
       "      <th>abortion bodily autonomy</th>\n",
       "      <th>abortion body</th>\n",
       "      <th>abortion body choice</th>\n",
       "      <th>abortion called</th>\n",
       "      <th>abortion came</th>\n",
       "      <th>abortion carry</th>\n",
       "      <th>abortion carrying</th>\n",
       "      <th>abortion carrying term</th>\n",
       "      <th>abortion carrying term analogous</th>\n",
       "      <th>abortion choice</th>\n",
       "      <th>abortion choice body</th>\n",
       "      <th>abortion chooses</th>\n",
       "      <th>abortion chooses half</th>\n",
       "      <th>abortion chooses half pregnancy</th>\n",
       "      <th>abortion clinic</th>\n",
       "      <th>abortion common</th>\n",
       "      <th>abortion completely</th>\n",
       "      <th>abortion consent</th>\n",
       "      <th>abortion consider</th>\n",
       "      <th>abortion contraceptive</th>\n",
       "      <th>abortion control</th>\n",
       "      <th>abortion cost</th>\n",
       "      <th>abortion country</th>\n",
       "      <th>abortion cut</th>\n",
       "      <th>abortion debate</th>\n",
       "      <th>abortion defend</th>\n",
       "      <th>abortion despite</th>\n",
       "      <th>abortion discovered</th>\n",
       "      <th>abortion discovered expecting</th>\n",
       "      <th>abortion discovered expecting son</th>\n",
       "      <th>abortion easy</th>\n",
       "      <th>abortion fetus</th>\n",
       "      <th>abortion financial</th>\n",
       "      <th>abortion fine</th>\n",
       "      <th>abortion form</th>\n",
       "      <th>abortion free</th>\n",
       "      <th>abortion freely</th>\n",
       "      <th>abortion fucked</th>\n",
       "      <th>abortion gendered</th>\n",
       "      <th>abortion happen</th>\n",
       "      <th>abortion illegal</th>\n",
       "      <th>abortion late</th>\n",
       "      <th>abortion law</th>\n",
       "      <th>abortion legal</th>\n",
       "      <th>abortion medical</th>\n",
       "      <th>abortion morally</th>\n",
       "      <th>abortion murder</th>\n",
       "      <th>abortion necessarily</th>\n",
       "      <th>abortion okay</th>\n",
       "      <th>abortion opinion</th>\n",
       "      <th>abortion option</th>\n",
       "      <th>abortion parent</th>\n",
       "      <th>abortion pay</th>\n",
       "      <th>abortion performed</th>\n",
       "      <th>abortion poor</th>\n",
       "      <th>abortion position</th>\n",
       "      <th>abortion possible</th>\n",
       "      <th>abortion pregnancy</th>\n",
       "      <th>abortion pregnant</th>\n",
       "      <th>abortion pretty</th>\n",
       "      <th>abortion pro</th>\n",
       "      <th>abortion pro choice</th>\n",
       "      <th>abortion reproductive</th>\n",
       "      <th>abortion restriction</th>\n",
       "      <th>abortion service</th>\n",
       "      <th>abortion simply</th>\n",
       "      <th>abortion wanted</th>\n",
       "      <th>abortion week</th>\n",
       "      <th>abovementioned</th>\n",
       "      <th>abraham</th>\n",
       "      <th>abrahamic</th>\n",
       "      <th>abrahamic religion</th>\n",
       "      <th>abrahamic religion christianity</th>\n",
       "      <th>abrasion</th>\n",
       "      <th>abrasive</th>\n",
       "      <th>abridging</th>\n",
       "      <th>abridging freedom</th>\n",
       "      <th>abridging freedom speech</th>\n",
       "      <th>abroad</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absence</th>\n",
       "      <th>absence court</th>\n",
       "      <th>absence court order</th>\n",
       "      <th>absence evidence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absent evidence</th>\n",
       "      <th>absent father</th>\n",
       "      <th>absentee</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolute abundance</th>\n",
       "      <th>absolute bare</th>\n",
       "      <th>absolute bare minimum</th>\n",
       "      <th>absolute best</th>\n",
       "      <th>absolute bitch</th>\n",
       "      <th>absolute confidence</th>\n",
       "      <th>absolute garbage</th>\n",
       "      <th>absolute hell</th>\n",
       "      <th>absolute hiv</th>\n",
       "      <th>absolute hiv number</th>\n",
       "      <th>absolute hiv number methodological</th>\n",
       "      <th>absolute nonsense</th>\n",
       "      <th>absolute opposite</th>\n",
       "      <th>absolute opposite love</th>\n",
       "      <th>absolute ops</th>\n",
       "      <th>absolute ops business</th>\n",
       "      <th>absolute ops business ask</th>\n",
       "      <th>absolute outrage</th>\n",
       "      <th>absolute piece</th>\n",
       "      <th>absolute piece shit</th>\n",
       "      <th>...</th>\n",
       "      <th>yes consider</th>\n",
       "      <th>yes consider using</th>\n",
       "      <th>yes consider using gain</th>\n",
       "      <th>yes considered</th>\n",
       "      <th>yes correct</th>\n",
       "      <th>yes course</th>\n",
       "      <th>yes created</th>\n",
       "      <th>yes crime</th>\n",
       "      <th>yes cross</th>\n",
       "      <th>yes culture</th>\n",
       "      <th>yes current</th>\n",
       "      <th>yes currently</th>\n",
       "      <th>yes currently biology</th>\n",
       "      <th>yes currently biology relevance</th>\n",
       "      <th>yes dad</th>\n",
       "      <th>yes dangerous</th>\n",
       "      <th>yes day</th>\n",
       "      <th>yes dead</th>\n",
       "      <th>yes decided</th>\n",
       "      <th>yes decision</th>\n",
       "      <th>yes decision body</th>\n",
       "      <th>yes decision impact</th>\n",
       "      <th>yes decision impact choice</th>\n",
       "      <th>yes default</th>\n",
       "      <th>yes definitely</th>\n",
       "      <th>yes definitely site</th>\n",
       "      <th>yes definitely site wide</th>\n",
       "      <th>yes definition</th>\n",
       "      <th>yes depends</th>\n",
       "      <th>yes difference</th>\n",
       "      <th>yes different</th>\n",
       "      <th>yes directly</th>\n",
       "      <th>yes disagree</th>\n",
       "      <th>yes distinction</th>\n",
       "      <th>yes distinction important</th>\n",
       "      <th>yes distinction important denying</th>\n",
       "      <th>yes doing</th>\n",
       "      <th>yes domination</th>\n",
       "      <th>yes endorsing</th>\n",
       "      <th>yes enthusiastic</th>\n",
       "      <th>yes especially</th>\n",
       "      <th>yes exactly</th>\n",
       "      <th>yes example</th>\n",
       "      <th>yes exception</th>\n",
       "      <th>yes experience</th>\n",
       "      <th>yes explain</th>\n",
       "      <th>yes fair</th>\n",
       "      <th>yes false</th>\n",
       "      <th>yes false accusation</th>\n",
       "      <th>yes feeling</th>\n",
       "      <th>yes feminine</th>\n",
       "      <th>yes fine</th>\n",
       "      <th>yes fix</th>\n",
       "      <th>yes fluid</th>\n",
       "      <th>yes fluid nb</th>\n",
       "      <th>yes fluid nb keen</th>\n",
       "      <th>yes focus</th>\n",
       "      <th>yes free</th>\n",
       "      <th>yes free informed</th>\n",
       "      <th>yes free informed consent</th>\n",
       "      <th>yes freely</th>\n",
       "      <th>yes freely enthusiastically</th>\n",
       "      <th>yes freely enthusiastically given</th>\n",
       "      <th>yes friend</th>\n",
       "      <th>yes fucking</th>\n",
       "      <th>yes gave</th>\n",
       "      <th>yes gay</th>\n",
       "      <th>yes generally</th>\n",
       "      <th>yes got</th>\n",
       "      <th>yes guess</th>\n",
       "      <th>yes happen</th>\n",
       "      <th>yes happened</th>\n",
       "      <th>yes hard</th>\n",
       "      <th>yes hate</th>\n",
       "      <th>yes hear</th>\n",
       "      <th>yes heard</th>\n",
       "      <th>yes hold</th>\n",
       "      <th>yes home</th>\n",
       "      <th>yes home longer</th>\n",
       "      <th>yes home longer allowed</th>\n",
       "      <th>yes homemaker</th>\n",
       "      <th>yes huge</th>\n",
       "      <th>yes human</th>\n",
       "      <th>yes idea</th>\n",
       "      <th>yes important</th>\n",
       "      <th>yes including</th>\n",
       "      <th>yes inequality</th>\n",
       "      <th>yes interesting</th>\n",
       "      <th>yes involved</th>\n",
       "      <th>yes involved ending</th>\n",
       "      <th>yes involved ending innocent</th>\n",
       "      <th>yes job</th>\n",
       "      <th>yes join</th>\n",
       "      <th>yes kid</th>\n",
       "      <th>yes kind</th>\n",
       "      <th>yes law</th>\n",
       "      <th>yes lawyer</th>\n",
       "      <th>yes lead</th>\n",
       "      <th>yes let</th>\n",
       "      <th>yes let continue</th>\n",
       "      <th>yes likely</th>\n",
       "      <th>yes limit</th>\n",
       "      <th>yes literally</th>\n",
       "      <th>yes making</th>\n",
       "      <th>yes matter</th>\n",
       "      <th>yes maybe</th>\n",
       "      <th>yes meant</th>\n",
       "      <th>yes medium</th>\n",
       "      <th>yes mental</th>\n",
       "      <th>yes mental illness</th>\n",
       "      <th>yes mention</th>\n",
       "      <th>yes met</th>\n",
       "      <th>yes mother</th>\n",
       "      <th>yes muslim</th>\n",
       "      <th>yes necessarily</th>\n",
       "      <th>yes necessarily real</th>\n",
       "      <th>yes necessarily real human</th>\n",
       "      <th>yes news</th>\n",
       "      <th>yes news deemed</th>\n",
       "      <th>yes news deemed law</th>\n",
       "      <th>yes non</th>\n",
       "      <th>yes obviously</th>\n",
       "      <th>yes ok</th>\n",
       "      <th>yes okay</th>\n",
       "      <th>yes old</th>\n",
       "      <th>yes opinion</th>\n",
       "      <th>yes oppresses</th>\n",
       "      <th>yes oppresses wouldn</th>\n",
       "      <th>yes oppresses wouldn eager</th>\n",
       "      <th>yes option</th>\n",
       "      <th>yes outside</th>\n",
       "      <th>yes partially</th>\n",
       "      <th>yes past</th>\n",
       "      <th>yes patriarchy</th>\n",
       "      <th>yes personally</th>\n",
       "      <th>yes politics</th>\n",
       "      <th>yes prefer</th>\n",
       "      <th>yes pretty</th>\n",
       "      <th>yes privilege</th>\n",
       "      <th>yes probably</th>\n",
       "      <th>yes probably stalemate</th>\n",
       "      <th>yes probably stalemate change</th>\n",
       "      <th>yes prominent</th>\n",
       "      <th>yes question</th>\n",
       "      <th>yes quite</th>\n",
       "      <th>yes race</th>\n",
       "      <th>yes race play</th>\n",
       "      <th>yes radical</th>\n",
       "      <th>yes rapist</th>\n",
       "      <th>yes rationale</th>\n",
       "      <th>yes react</th>\n",
       "      <th>yes react id</th>\n",
       "      <th>yes react id skeptical</th>\n",
       "      <th>yes read</th>\n",
       "      <th>yes real</th>\n",
       "      <th>yes realize</th>\n",
       "      <th>yes refer</th>\n",
       "      <th>yes refer toxic</th>\n",
       "      <th>yes refer toxic masculinity</th>\n",
       "      <th>yes regret</th>\n",
       "      <th>yes relationship</th>\n",
       "      <th>yes removal</th>\n",
       "      <th>yes removal idelaogical</th>\n",
       "      <th>yes removal idelaogical argument</th>\n",
       "      <th>yes ridiculous</th>\n",
       "      <th>yes risk</th>\n",
       "      <th>yes rocket</th>\n",
       "      <th>yes rocket science</th>\n",
       "      <th>yes role</th>\n",
       "      <th>yes role switched</th>\n",
       "      <th>yes role switched protest</th>\n",
       "      <th>yes saw</th>\n",
       "      <th>yes school</th>\n",
       "      <th>yes science</th>\n",
       "      <th>yes scream</th>\n",
       "      <th>yes scream bring</th>\n",
       "      <th>yes scream bring awareness</th>\n",
       "      <th>yes second</th>\n",
       "      <th>yes seen</th>\n",
       "      <th>yes seen article</th>\n",
       "      <th>yes self</th>\n",
       "      <th>yes self defense</th>\n",
       "      <th>yes sense</th>\n",
       "      <th>yes sexism</th>\n",
       "      <th>yes sexist</th>\n",
       "      <th>yes shitty</th>\n",
       "      <th>yes solid</th>\n",
       "      <th>yes sort</th>\n",
       "      <th>yes specifically</th>\n",
       "      <th>yes start</th>\n",
       "      <th>yes stop</th>\n",
       "      <th>yes strong</th>\n",
       "      <th>yes strongly</th>\n",
       "      <th>yes strongly anti</th>\n",
       "      <th>yes strongly anti doping</th>\n",
       "      <th>yes study</th>\n",
       "      <th>yes stuff</th>\n",
       "      <th>yes talk</th>\n",
       "      <th>yes talking</th>\n",
       "      <th>yes taught</th>\n",
       "      <th>yes teach</th>\n",
       "      <th>yes technically</th>\n",
       "      <th>yes tell</th>\n",
       "      <th>yes tend</th>\n",
       "      <th>yes terrible</th>\n",
       "      <th>yes thank</th>\n",
       "      <th>yes told</th>\n",
       "      <th>yes totally</th>\n",
       "      <th>yes trans</th>\n",
       "      <th>yes transphobic</th>\n",
       "      <th>yes true</th>\n",
       "      <th>yes try</th>\n",
       "      <th>yes type</th>\n",
       "      <th>yes understand</th>\n",
       "      <th>yes understanding</th>\n",
       "      <th>yes used</th>\n",
       "      <th>yes using</th>\n",
       "      <th>yes wanted</th>\n",
       "      <th>yes wasn</th>\n",
       "      <th>yes wearing</th>\n",
       "      <th>yes word</th>\n",
       "      <th>yes worse</th>\n",
       "      <th>yes wouldn</th>\n",
       "      <th>yes yes</th>\n",
       "      <th>yes yes enthusiastic</th>\n",
       "      <th>yes yes important</th>\n",
       "      <th>yes yes yes</th>\n",
       "      <th>yesallmen</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yesterday evening</th>\n",
       "      <th>yesterday posted</th>\n",
       "      <th>yesterday revealed</th>\n",
       "      <th>yi</th>\n",
       "      <th>yiannopoulos</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yikes</th>\n",
       "      <th>yin</th>\n",
       "      <th>yin yang</th>\n",
       "      <th>yinz</th>\n",
       "      <th>ymca</th>\n",
       "      <th>ymmv</th>\n",
       "      <th>yo</th>\n",
       "      <th>yo kid</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yoga pant</th>\n",
       "      <th>yoke</th>\n",
       "      <th>york</th>\n",
       "      <th>york article</th>\n",
       "      <th>york city</th>\n",
       "      <th>york state</th>\n",
       "      <th>york state law</th>\n",
       "      <th>york university</th>\n",
       "      <th>yorker</th>\n",
       "      <th>yorkshire</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>young able</th>\n",
       "      <th>young able inform</th>\n",
       "      <th>young absolutely</th>\n",
       "      <th>young academic</th>\n",
       "      <th>young actress</th>\n",
       "      <th>young adult</th>\n",
       "      <th>young adult strongly</th>\n",
       "      <th>young adult strongly associated</th>\n",
       "      <th>young adult study</th>\n",
       "      <th>young adult study parental</th>\n",
       "      <th>young age</th>\n",
       "      <th>young age knew</th>\n",
       "      <th>young age simply</th>\n",
       "      <th>young aged</th>\n",
       "      <th>young aged old</th>\n",
       "      <th>young aged old complete</th>\n",
       "      <th>young american</th>\n",
       "      <th>young angry</th>\n",
       "      <th>young aren</th>\n",
       "      <th>young asian</th>\n",
       "      <th>young ask</th>\n",
       "      <th>young attractive</th>\n",
       "      <th>young baby</th>\n",
       "      <th>young barely</th>\n",
       "      <th>young beautiful</th>\n",
       "      <th>young black</th>\n",
       "      <th>young black commit</th>\n",
       "      <th>young bodily</th>\n",
       "      <th>young bodily integrity</th>\n",
       "      <th>young boy</th>\n",
       "      <th>young boy aren</th>\n",
       "      <th>young boy born</th>\n",
       "      <th>young boy born surgery</th>\n",
       "      <th>young caught</th>\n",
       "      <th>young childless</th>\n",
       "      <th>young coming</th>\n",
       "      <th>young consent</th>\n",
       "      <th>young conservative</th>\n",
       "      <th>young couple</th>\n",
       "      <th>young daughter</th>\n",
       "      <th>young day</th>\n",
       "      <th>young desperate</th>\n",
       "      <th>young doing</th>\n",
       "      <th>young face</th>\n",
       "      <th>young father</th>\n",
       "      <th>young fertile</th>\n",
       "      <th>young fertile attractive</th>\n",
       "      <th>young fertile attractive countless</th>\n",
       "      <th>young fertile attractive period</th>\n",
       "      <th>young got</th>\n",
       "      <th>young hate</th>\n",
       "      <th>young hot</th>\n",
       "      <th>young human</th>\n",
       "      <th>young infant</th>\n",
       "      <th>young interested</th>\n",
       "      <th>young kid</th>\n",
       "      <th>young kind</th>\n",
       "      <th>young lady</th>\n",
       "      <th>young lady narrating</th>\n",
       "      <th>young lady narrating video</th>\n",
       "      <th>young learn</th>\n",
       "      <th>young left</th>\n",
       "      <th>young likely</th>\n",
       "      <th>young live</th>\n",
       "      <th>young live home</th>\n",
       "      <th>young living</th>\n",
       "      <th>young living city</th>\n",
       "      <th>young living city earn</th>\n",
       "      <th>young looking</th>\n",
       "      <th>young majority</th>\n",
       "      <th>young majority college</th>\n",
       "      <th>young majority college student</th>\n",
       "      <th>young mere</th>\n",
       "      <th>young mind</th>\n",
       "      <th>young money</th>\n",
       "      <th>young mother</th>\n",
       "      <th>young moved</th>\n",
       "      <th>young murdered</th>\n",
       "      <th>young nazi</th>\n",
       "      <th>young nazi soldier</th>\n",
       "      <th>young nazi soldier fall</th>\n",
       "      <th>young negative</th>\n",
       "      <th>young old</th>\n",
       "      <th>young old emotion</th>\n",
       "      <th>young old emotion dont</th>\n",
       "      <th>young owning</th>\n",
       "      <th>young owning property</th>\n",
       "      <th>young parent</th>\n",
       "      <th>young particular</th>\n",
       "      <th>young perception</th>\n",
       "      <th>young perception attitude</th>\n",
       "      <th>young perception attitude behaviour</th>\n",
       "      <th>young perform</th>\n",
       "      <th>young phase</th>\n",
       "      <th>young phase figure</th>\n",
       "      <th>young phase figure angry</th>\n",
       "      <th>young politician</th>\n",
       "      <th>young pretty</th>\n",
       "      <th>young promised</th>\n",
       "      <th>young raped</th>\n",
       "      <th>young single childless earn</th>\n",
       "      <th>young skill</th>\n",
       "      <th>young son</th>\n",
       "      <th>young sort</th>\n",
       "      <th>young speak</th>\n",
       "      <th>young start</th>\n",
       "      <th>young student</th>\n",
       "      <th>young teen</th>\n",
       "      <th>young teenager</th>\n",
       "      <th>young today</th>\n",
       "      <th>young told</th>\n",
       "      <th>young used</th>\n",
       "      <th>young workforce</th>\n",
       "      <th>young worried</th>\n",
       "      <th>young young</th>\n",
       "      <th>younger</th>\n",
       "      <th>younger age</th>\n",
       "      <th>younger attractive</th>\n",
       "      <th>younger born</th>\n",
       "      <th>younger boy</th>\n",
       "      <th>younger brother</th>\n",
       "      <th>younger doing</th>\n",
       "      <th>younger generation</th>\n",
       "      <th>younger older</th>\n",
       "      <th>younger older younger</th>\n",
       "      <th>younger rapist</th>\n",
       "      <th>younger self</th>\n",
       "      <th>younger sister</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youngest son</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youre</th>\n",
       "      <th>youre older</th>\n",
       "      <th>youre older instead</th>\n",
       "      <th>youre older instead transitioning</th>\n",
       "      <th>youre sexist</th>\n",
       "      <th>youse</th>\n",
       "      <th>yout</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth age</th>\n",
       "      <th>youth age domestic</th>\n",
       "      <th>youth age domestic violence</th>\n",
       "      <th>youth beauty</th>\n",
       "      <th>youth boy</th>\n",
       "      <th>youth club</th>\n",
       "      <th>youth mad</th>\n",
       "      <th>youth suicide</th>\n",
       "      <th>youth suicide average</th>\n",
       "      <th>youth suicide average okay</th>\n",
       "      <th>youthful</th>\n",
       "      <th>youtoo</th>\n",
       "      <th>youtoo successful</th>\n",
       "      <th>youtoo successful using</th>\n",
       "      <th>youtoo successful using pussy</th>\n",
       "      <th>youtried</th>\n",
       "      <th>youtried jpg</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube antifeminist</th>\n",
       "      <th>youtube audio</th>\n",
       "      <th>youtube channel</th>\n",
       "      <th>youtube channel called</th>\n",
       "      <th>youtube channel extracredits</th>\n",
       "      <th>youtube channel focused</th>\n",
       "      <th>youtube channel kenyan</th>\n",
       "      <th>youtube channel kenyan cosmetic</th>\n",
       "      <th>youtube channel similar</th>\n",
       "      <th>youtube check</th>\n",
       "      <th>youtube got</th>\n",
       "      <th>youtube great</th>\n",
       "      <th>youtube great analysis</th>\n",
       "      <th>youtube great analysis masculinity</th>\n",
       "      <th>youtube link</th>\n",
       "      <th>youtube probably</th>\n",
       "      <th>youtube search</th>\n",
       "      <th>youtube shooting</th>\n",
       "      <th>youtube spotlight</th>\n",
       "      <th>youtube star</th>\n",
       "      <th>youtube subscriber</th>\n",
       "      <th>youtube talk</th>\n",
       "      <th>youtube try</th>\n",
       "      <th>youtube tv</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>youtuber</th>\n",
       "      <th>youtubers</th>\n",
       "      <th>youtubes</th>\n",
       "      <th>youve</th>\n",
       "      <th>youve got</th>\n",
       "      <th>ypt</th>\n",
       "      <th>ypu</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>yt</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yung</th>\n",
       "      <th>yup</th>\n",
       "      <th>zahidi</th>\n",
       "      <th>zarya</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealand report</th>\n",
       "      <th>zealand report human</th>\n",
       "      <th>zealand submission</th>\n",
       "      <th>zealot</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeitgeist</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero dawn</th>\n",
       "      <th>zero dollar</th>\n",
       "      <th>zero effect</th>\n",
       "      <th>zero evidence</th>\n",
       "      <th>zero experience</th>\n",
       "      <th>zero precedent</th>\n",
       "      <th>zero proof</th>\n",
       "      <th>zero repercussion</th>\n",
       "      <th>zero restriction</th>\n",
       "      <th>zero self</th>\n",
       "      <th>zero sense</th>\n",
       "      <th>zero sum</th>\n",
       "      <th>zero sum game</th>\n",
       "      <th>zero sum situation</th>\n",
       "      <th>zero sympathy</th>\n",
       "      <th>zero tolerance</th>\n",
       "      <th>zero tolerance policy</th>\n",
       "      <th>zero value</th>\n",
       "      <th>zeroing</th>\n",
       "      <th>zimbardo</th>\n",
       "      <th>zionism</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zizek</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zoe quinn</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie apocalypse</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoning</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zur</th>\n",
       "      <th>zygote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.013801</td>\n",
       "      <td>-0.141119</td>\n",
       "      <td>0.135741</td>\n",
       "      <td>-0.066027</td>\n",
       "      <td>0.503888</td>\n",
       "      <td>0.03459</td>\n",
       "      <td>-0.05515</td>\n",
       "      <td>0.543464</td>\n",
       "      <td>0.069633</td>\n",
       "      <td>-0.142672</td>\n",
       "      <td>-0.536743</td>\n",
       "      <td>0.689786</td>\n",
       "      <td>0.265999</td>\n",
       "      <td>-0.095073</td>\n",
       "      <td>0.17266</td>\n",
       "      <td>-0.048525</td>\n",
       "      <td>0.258237</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>0.043457</td>\n",
       "      <td>-0.128661</td>\n",
       "      <td>0.088476</td>\n",
       "      <td>0.088476</td>\n",
       "      <td>-0.086814</td>\n",
       "      <td>-0.058487</td>\n",
       "      <td>0.121984</td>\n",
       "      <td>0.032844</td>\n",
       "      <td>-0.012841</td>\n",
       "      <td>-0.103017</td>\n",
       "      <td>0.084248</td>\n",
       "      <td>-0.036005</td>\n",
       "      <td>-0.118764</td>\n",
       "      <td>0.282517</td>\n",
       "      <td>0.16833</td>\n",
       "      <td>0.00311</td>\n",
       "      <td>-1.252711</td>\n",
       "      <td>-0.093389</td>\n",
       "      <td>-0.111448</td>\n",
       "      <td>-0.012955</td>\n",
       "      <td>-0.084094</td>\n",
       "      <td>0.030585</td>\n",
       "      <td>0.056264</td>\n",
       "      <td>-0.292663</td>\n",
       "      <td>-0.009066</td>\n",
       "      <td>-0.088057</td>\n",
       "      <td>0.016901</td>\n",
       "      <td>-0.011886</td>\n",
       "      <td>-0.010777</td>\n",
       "      <td>-0.111861</td>\n",
       "      <td>-0.111861</td>\n",
       "      <td>-0.111861</td>\n",
       "      <td>0.104289</td>\n",
       "      <td>0.198193</td>\n",
       "      <td>0.198193</td>\n",
       "      <td>0.026947</td>\n",
       "      <td>0.076652</td>\n",
       "      <td>0.190171</td>\n",
       "      <td>0.373146</td>\n",
       "      <td>-0.090035</td>\n",
       "      <td>0.130749</td>\n",
       "      <td>0.036709</td>\n",
       "      <td>0.073873</td>\n",
       "      <td>0.139434</td>\n",
       "      <td>0.042872</td>\n",
       "      <td>-0.110928</td>\n",
       "      <td>-0.188247</td>\n",
       "      <td>-0.054043</td>\n",
       "      <td>0.031674</td>\n",
       "      <td>-0.054201</td>\n",
       "      <td>-0.092198</td>\n",
       "      <td>0.158451</td>\n",
       "      <td>0.033684</td>\n",
       "      <td>-0.011121</td>\n",
       "      <td>0.090753</td>\n",
       "      <td>0.073145</td>\n",
       "      <td>-1.405962</td>\n",
       "      <td>-0.147112</td>\n",
       "      <td>-0.429875</td>\n",
       "      <td>-0.101707</td>\n",
       "      <td>-0.093644</td>\n",
       "      <td>0.154937</td>\n",
       "      <td>-0.067173</td>\n",
       "      <td>0.102377</td>\n",
       "      <td>0.217105</td>\n",
       "      <td>-0.040465</td>\n",
       "      <td>-0.194401</td>\n",
       "      <td>-0.117583</td>\n",
       "      <td>0.111432</td>\n",
       "      <td>-0.05733</td>\n",
       "      <td>0.263231</td>\n",
       "      <td>0.015192</td>\n",
       "      <td>-0.055667</td>\n",
       "      <td>-0.112371</td>\n",
       "      <td>0.279423</td>\n",
       "      <td>-0.239471</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.062226</td>\n",
       "      <td>-0.096364</td>\n",
       "      <td>-0.296076</td>\n",
       "      <td>0.099042</td>\n",
       "      <td>0.108727</td>\n",
       "      <td>0.071469</td>\n",
       "      <td>-0.053579</td>\n",
       "      <td>0.087489</td>\n",
       "      <td>-0.026312</td>\n",
       "      <td>0.012859</td>\n",
       "      <td>0.082486</td>\n",
       "      <td>-0.044007</td>\n",
       "      <td>0.215793</td>\n",
       "      <td>-0.263083</td>\n",
       "      <td>-0.046557</td>\n",
       "      <td>-0.122718</td>\n",
       "      <td>-0.102466</td>\n",
       "      <td>-0.102466</td>\n",
       "      <td>-0.094636</td>\n",
       "      <td>-0.112471</td>\n",
       "      <td>0.124342</td>\n",
       "      <td>-0.015199</td>\n",
       "      <td>0.07734</td>\n",
       "      <td>0.090896</td>\n",
       "      <td>0.219333</td>\n",
       "      <td>-0.163747</td>\n",
       "      <td>-0.00362</td>\n",
       "      <td>0.025096</td>\n",
       "      <td>-0.023298</td>\n",
       "      <td>-0.073324</td>\n",
       "      <td>0.14148</td>\n",
       "      <td>-0.028741</td>\n",
       "      <td>-0.04261</td>\n",
       "      <td>-0.12696</td>\n",
       "      <td>-0.022103</td>\n",
       "      <td>-0.022103</td>\n",
       "      <td>-0.022103</td>\n",
       "      <td>-0.062365</td>\n",
       "      <td>0.09851</td>\n",
       "      <td>-0.108937</td>\n",
       "      <td>0.057056</td>\n",
       "      <td>0.092018</td>\n",
       "      <td>0.257785</td>\n",
       "      <td>-0.021962</td>\n",
       "      <td>0.06291</td>\n",
       "      <td>0.028815</td>\n",
       "      <td>0.071489</td>\n",
       "      <td>-0.008874</td>\n",
       "      <td>0.019217</td>\n",
       "      <td>0.113206</td>\n",
       "      <td>-0.28389</td>\n",
       "      <td>-0.094827</td>\n",
       "      <td>0.178487</td>\n",
       "      <td>-0.123384</td>\n",
       "      <td>-0.156738</td>\n",
       "      <td>0.040769</td>\n",
       "      <td>-0.057435</td>\n",
       "      <td>0.113731</td>\n",
       "      <td>0.024201</td>\n",
       "      <td>0.179434</td>\n",
       "      <td>0.021308</td>\n",
       "      <td>0.118899</td>\n",
       "      <td>-0.08601</td>\n",
       "      <td>-0.16344</td>\n",
       "      <td>-0.150398</td>\n",
       "      <td>0.037491</td>\n",
       "      <td>-0.080761</td>\n",
       "      <td>0.011464</td>\n",
       "      <td>-0.049858</td>\n",
       "      <td>-0.049323</td>\n",
       "      <td>-0.121607</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>0.10455</td>\n",
       "      <td>-0.276675</td>\n",
       "      <td>-0.224059</td>\n",
       "      <td>-0.108227</td>\n",
       "      <td>-0.092431</td>\n",
       "      <td>-0.092431</td>\n",
       "      <td>-0.092431</td>\n",
       "      <td>-0.02558</td>\n",
       "      <td>-0.015917</td>\n",
       "      <td>-0.156055</td>\n",
       "      <td>-0.028652</td>\n",
       "      <td>0.405327</td>\n",
       "      <td>-0.192786</td>\n",
       "      <td>-0.053531</td>\n",
       "      <td>-0.053531</td>\n",
       "      <td>0.039243</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.130862</td>\n",
       "      <td>-0.127839</td>\n",
       "      <td>0.107106</td>\n",
       "      <td>-0.15868</td>\n",
       "      <td>0.01047</td>\n",
       "      <td>-0.056014</td>\n",
       "      <td>-0.238836</td>\n",
       "      <td>-0.627668</td>\n",
       "      <td>-0.00488</td>\n",
       "      <td>-0.041386</td>\n",
       "      <td>-0.153965</td>\n",
       "      <td>-0.028448</td>\n",
       "      <td>-0.157582</td>\n",
       "      <td>-0.094852</td>\n",
       "      <td>0.202064</td>\n",
       "      <td>-0.125202</td>\n",
       "      <td>-0.02277</td>\n",
       "      <td>0.263009</td>\n",
       "      <td>-0.1412</td>\n",
       "      <td>0.054921</td>\n",
       "      <td>-0.152945</td>\n",
       "      <td>-0.160367</td>\n",
       "      <td>-0.584844</td>\n",
       "      <td>-0.446439</td>\n",
       "      <td>0.175182</td>\n",
       "      <td>-0.092472</td>\n",
       "      <td>0.487803</td>\n",
       "      <td>-0.061039</td>\n",
       "      <td>-0.649933</td>\n",
       "      <td>-0.07844</td>\n",
       "      <td>-0.488996</td>\n",
       "      <td>-0.467679</td>\n",
       "      <td>-0.047036</td>\n",
       "      <td>-0.054417</td>\n",
       "      <td>-0.38204</td>\n",
       "      <td>-0.130737</td>\n",
       "      <td>-0.208354</td>\n",
       "      <td>-0.133586</td>\n",
       "      <td>0.067749</td>\n",
       "      <td>0.177807</td>\n",
       "      <td>0.517093</td>\n",
       "      <td>-0.028604</td>\n",
       "      <td>-0.112302</td>\n",
       "      <td>-0.448517</td>\n",
       "      <td>-0.054087</td>\n",
       "      <td>0.20955</td>\n",
       "      <td>0.136744</td>\n",
       "      <td>-0.091129</td>\n",
       "      <td>-1.701626</td>\n",
       "      <td>0.235557</td>\n",
       "      <td>0.302203</td>\n",
       "      <td>-0.103447</td>\n",
       "      <td>0.199577</td>\n",
       "      <td>-0.020802</td>\n",
       "      <td>-0.101514</td>\n",
       "      <td>-0.200917</td>\n",
       "      <td>-0.141872</td>\n",
       "      <td>-0.021318</td>\n",
       "      <td>0.117159</td>\n",
       "      <td>0.032993</td>\n",
       "      <td>-0.017005</td>\n",
       "      <td>0.010621</td>\n",
       "      <td>-0.086935</td>\n",
       "      <td>-0.097789</td>\n",
       "      <td>-0.087312</td>\n",
       "      <td>-0.040179</td>\n",
       "      <td>-0.148746</td>\n",
       "      <td>0.126125</td>\n",
       "      <td>0.119545</td>\n",
       "      <td>-0.038586</td>\n",
       "      <td>-0.05041</td>\n",
       "      <td>-0.039911</td>\n",
       "      <td>0.235451</td>\n",
       "      <td>0.085875</td>\n",
       "      <td>0.069165</td>\n",
       "      <td>-0.263898</td>\n",
       "      <td>-0.414661</td>\n",
       "      <td>-0.065961</td>\n",
       "      <td>0.069526</td>\n",
       "      <td>0.527997</td>\n",
       "      <td>-0.187985</td>\n",
       "      <td>-0.118801</td>\n",
       "      <td>-0.005344</td>\n",
       "      <td>0.490073</td>\n",
       "      <td>0.07506</td>\n",
       "      <td>-0.090715</td>\n",
       "      <td>-0.222432</td>\n",
       "      <td>-0.1412</td>\n",
       "      <td>-0.325048</td>\n",
       "      <td>-0.069383</td>\n",
       "      <td>-0.123516</td>\n",
       "      <td>-0.045292</td>\n",
       "      <td>-0.084152</td>\n",
       "      <td>0.029231</td>\n",
       "      <td>0.244494</td>\n",
       "      <td>-0.088717</td>\n",
       "      <td>-0.084976</td>\n",
       "      <td>0.050597</td>\n",
       "      <td>-0.02377</td>\n",
       "      <td>-0.266758</td>\n",
       "      <td>0.038504</td>\n",
       "      <td>0.079417</td>\n",
       "      <td>-0.174064</td>\n",
       "      <td>0.118842</td>\n",
       "      <td>0.118842</td>\n",
       "      <td>-0.098758</td>\n",
       "      <td>-0.107849</td>\n",
       "      <td>-0.004814</td>\n",
       "      <td>-0.130708</td>\n",
       "      <td>-0.148454</td>\n",
       "      <td>0.022296</td>\n",
       "      <td>0.130513</td>\n",
       "      <td>0.045675</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>-0.05608</td>\n",
       "      <td>-0.077782</td>\n",
       "      <td>-0.077782</td>\n",
       "      <td>-0.03396</td>\n",
       "      <td>-0.006677</td>\n",
       "      <td>0.044879</td>\n",
       "      <td>-0.179602</td>\n",
       "      <td>-0.096941</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.066566</td>\n",
       "      <td>-0.250945</td>\n",
       "      <td>-0.03251</td>\n",
       "      <td>0.035896</td>\n",
       "      <td>-0.103768</td>\n",
       "      <td>-0.028266</td>\n",
       "      <td>-0.081879</td>\n",
       "      <td>0.102373</td>\n",
       "      <td>-0.233645</td>\n",
       "      <td>0.253002</td>\n",
       "      <td>-0.125007</td>\n",
       "      <td>0.053407</td>\n",
       "      <td>-0.455485</td>\n",
       "      <td>-0.037271</td>\n",
       "      <td>-0.037271</td>\n",
       "      <td>0.071326</td>\n",
       "      <td>0.071326</td>\n",
       "      <td>-0.516419</td>\n",
       "      <td>-0.108829</td>\n",
       "      <td>0.10215</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.11996</td>\n",
       "      <td>0.054351</td>\n",
       "      <td>0.054351</td>\n",
       "      <td>-0.105413</td>\n",
       "      <td>0.076546</td>\n",
       "      <td>0.173011</td>\n",
       "      <td>-0.196716</td>\n",
       "      <td>-0.059646</td>\n",
       "      <td>-0.015787</td>\n",
       "      <td>-0.015787</td>\n",
       "      <td>0.230308</td>\n",
       "      <td>-0.004226</td>\n",
       "      <td>0.038161</td>\n",
       "      <td>0.038161</td>\n",
       "      <td>0.038161</td>\n",
       "      <td>0.03963</td>\n",
       "      <td>0.188217</td>\n",
       "      <td>-0.03307</td>\n",
       "      <td>-0.13574</td>\n",
       "      <td>-0.123277</td>\n",
       "      <td>0.098853</td>\n",
       "      <td>-1.123938</td>\n",
       "      <td>-0.188604</td>\n",
       "      <td>0.43442</td>\n",
       "      <td>-0.152571</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>-0.056054</td>\n",
       "      <td>-0.047699</td>\n",
       "      <td>0.101436</td>\n",
       "      <td>-0.064735</td>\n",
       "      <td>0.016623</td>\n",
       "      <td>0.075124</td>\n",
       "      <td>0.047811</td>\n",
       "      <td>0.072275</td>\n",
       "      <td>0.106259</td>\n",
       "      <td>-0.061802</td>\n",
       "      <td>0.340723</td>\n",
       "      <td>-0.077443</td>\n",
       "      <td>-0.03494</td>\n",
       "      <td>0.096941</td>\n",
       "      <td>-0.04679</td>\n",
       "      <td>-0.206905</td>\n",
       "      <td>0.075695</td>\n",
       "      <td>0.133644</td>\n",
       "      <td>-0.170964</td>\n",
       "      <td>-0.029918</td>\n",
       "      <td>-0.05191</td>\n",
       "      <td>0.099455</td>\n",
       "      <td>-0.02733</td>\n",
       "      <td>0.070453</td>\n",
       "      <td>-0.092718</td>\n",
       "      <td>-0.007252</td>\n",
       "      <td>0.015302</td>\n",
       "      <td>-0.056786</td>\n",
       "      <td>0.105451</td>\n",
       "      <td>-0.080909</td>\n",
       "      <td>0.112019</td>\n",
       "      <td>-0.119381</td>\n",
       "      <td>0.073792</td>\n",
       "      <td>0.027526</td>\n",
       "      <td>-0.202448</td>\n",
       "      <td>0.03279</td>\n",
       "      <td>0.018175</td>\n",
       "      <td>-0.025973</td>\n",
       "      <td>-0.025973</td>\n",
       "      <td>-0.171835</td>\n",
       "      <td>-0.217267</td>\n",
       "      <td>0.167015</td>\n",
       "      <td>0.202228</td>\n",
       "      <td>-0.226405</td>\n",
       "      <td>0.053906</td>\n",
       "      <td>-0.003149</td>\n",
       "      <td>0.09117</td>\n",
       "      <td>-0.158952</td>\n",
       "      <td>-0.273237</td>\n",
       "      <td>-0.224685</td>\n",
       "      <td>0.041681</td>\n",
       "      <td>-0.159544</td>\n",
       "      <td>-0.067409</td>\n",
       "      <td>-0.210803</td>\n",
       "      <td>-0.027083</td>\n",
       "      <td>0.057484</td>\n",
       "      <td>0.169128</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>-0.112776</td>\n",
       "      <td>-0.196997</td>\n",
       "      <td>0.043071</td>\n",
       "      <td>-0.100907</td>\n",
       "      <td>0.016179</td>\n",
       "      <td>0.189607</td>\n",
       "      <td>0.116955</td>\n",
       "      <td>-0.016483</td>\n",
       "      <td>-0.005374</td>\n",
       "      <td>-0.184704</td>\n",
       "      <td>-0.093434</td>\n",
       "      <td>-0.093434</td>\n",
       "      <td>-0.093434</td>\n",
       "      <td>0.094764</td>\n",
       "      <td>-0.079519</td>\n",
       "      <td>0.240681</td>\n",
       "      <td>0.080963</td>\n",
       "      <td>0.069049</td>\n",
       "      <td>0.205445</td>\n",
       "      <td>-0.484692</td>\n",
       "      <td>-0.0716</td>\n",
       "      <td>-0.071705</td>\n",
       "      <td>-0.21844</td>\n",
       "      <td>0.148333</td>\n",
       "      <td>0.175444</td>\n",
       "      <td>-0.132484</td>\n",
       "      <td>0.545039</td>\n",
       "      <td>-0.061477</td>\n",
       "      <td>0.197428</td>\n",
       "      <td>0.262061</td>\n",
       "      <td>0.156871</td>\n",
       "      <td>-0.025762</td>\n",
       "      <td>-0.122377</td>\n",
       "      <td>-0.122377</td>\n",
       "      <td>1.826338</td>\n",
       "      <td>-0.007622</td>\n",
       "      <td>0.025725</td>\n",
       "      <td>-0.041447</td>\n",
       "      <td>0.141115</td>\n",
       "      <td>0.169675</td>\n",
       "      <td>0.438967</td>\n",
       "      <td>0.067149</td>\n",
       "      <td>-0.011826</td>\n",
       "      <td>-0.08262</td>\n",
       "      <td>0.032735</td>\n",
       "      <td>-0.024385</td>\n",
       "      <td>-0.01042</td>\n",
       "      <td>-0.003797</td>\n",
       "      <td>-0.004761</td>\n",
       "      <td>-0.080495</td>\n",
       "      <td>0.013936</td>\n",
       "      <td>0.054567</td>\n",
       "      <td>0.047868</td>\n",
       "      <td>-0.046693</td>\n",
       "      <td>0.040731</td>\n",
       "      <td>-0.08616</td>\n",
       "      <td>0.4668</td>\n",
       "      <td>0.08351</td>\n",
       "      <td>0.120661</td>\n",
       "      <td>-0.297348</td>\n",
       "      <td>-0.042037</td>\n",
       "      <td>-0.051076</td>\n",
       "      <td>0.056927</td>\n",
       "      <td>-0.030117</td>\n",
       "      <td>0.09585</td>\n",
       "      <td>-0.171241</td>\n",
       "      <td>0.241729</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>-0.083961</td>\n",
       "      <td>-0.048765</td>\n",
       "      <td>-0.024382</td>\n",
       "      <td>0.022319</td>\n",
       "      <td>0.094539</td>\n",
       "      <td>0.094539</td>\n",
       "      <td>-0.002156</td>\n",
       "      <td>-0.057168</td>\n",
       "      <td>-0.227791</td>\n",
       "      <td>0.005482</td>\n",
       "      <td>0.13769</td>\n",
       "      <td>0.030075</td>\n",
       "      <td>-0.07476</td>\n",
       "      <td>-0.064297</td>\n",
       "      <td>-0.109869</td>\n",
       "      <td>-0.097301</td>\n",
       "      <td>-0.101066</td>\n",
       "      <td>-0.015474</td>\n",
       "      <td>0.290786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095013</td>\n",
       "      <td>-0.178606</td>\n",
       "      <td>-0.020043</td>\n",
       "      <td>-0.131688</td>\n",
       "      <td>0.112962</td>\n",
       "      <td>-0.204788</td>\n",
       "      <td>0.050109</td>\n",
       "      <td>0.028321</td>\n",
       "      <td>0.159058</td>\n",
       "      <td>-0.089152</td>\n",
       "      <td>-0.011177</td>\n",
       "      <td>-0.193152</td>\n",
       "      <td>-0.108236</td>\n",
       "      <td>-0.112438</td>\n",
       "      <td>-0.039882</td>\n",
       "      <td>-0.125846</td>\n",
       "      <td>0.218854</td>\n",
       "      <td>0.034804</td>\n",
       "      <td>-0.148204</td>\n",
       "      <td>-0.057877</td>\n",
       "      <td>-0.03885</td>\n",
       "      <td>0.021082</td>\n",
       "      <td>0.055308</td>\n",
       "      <td>0.095366</td>\n",
       "      <td>0.110642</td>\n",
       "      <td>-0.157254</td>\n",
       "      <td>0.130372</td>\n",
       "      <td>-0.165282</td>\n",
       "      <td>0.15822</td>\n",
       "      <td>0.16993</td>\n",
       "      <td>-0.04146</td>\n",
       "      <td>-0.012433</td>\n",
       "      <td>-0.223001</td>\n",
       "      <td>-0.128088</td>\n",
       "      <td>0.138956</td>\n",
       "      <td>-0.102106</td>\n",
       "      <td>0.01063</td>\n",
       "      <td>0.099535</td>\n",
       "      <td>0.115148</td>\n",
       "      <td>0.118905</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.018354</td>\n",
       "      <td>0.207855</td>\n",
       "      <td>0.08103</td>\n",
       "      <td>0.026225</td>\n",
       "      <td>-0.267899</td>\n",
       "      <td>-0.047946</td>\n",
       "      <td>0.110324</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>-0.102733</td>\n",
       "      <td>-0.171455</td>\n",
       "      <td>0.092736</td>\n",
       "      <td>0.092407</td>\n",
       "      <td>-0.025842</td>\n",
       "      <td>0.053629</td>\n",
       "      <td>-0.014642</td>\n",
       "      <td>-0.025345</td>\n",
       "      <td>0.045047</td>\n",
       "      <td>0.045047</td>\n",
       "      <td>0.073592</td>\n",
       "      <td>-0.055425</td>\n",
       "      <td>0.135004</td>\n",
       "      <td>0.135004</td>\n",
       "      <td>0.075854</td>\n",
       "      <td>-0.250672</td>\n",
       "      <td>0.092427</td>\n",
       "      <td>0.114059</td>\n",
       "      <td>0.167384</td>\n",
       "      <td>-0.053314</td>\n",
       "      <td>0.057647</td>\n",
       "      <td>-0.022241</td>\n",
       "      <td>-0.2184</td>\n",
       "      <td>0.067159</td>\n",
       "      <td>-0.094351</td>\n",
       "      <td>-0.116624</td>\n",
       "      <td>0.209809</td>\n",
       "      <td>0.110495</td>\n",
       "      <td>0.064209</td>\n",
       "      <td>0.102088</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>0.042122</td>\n",
       "      <td>-0.050209</td>\n",
       "      <td>0.043078</td>\n",
       "      <td>-0.08549</td>\n",
       "      <td>0.111026</td>\n",
       "      <td>0.111026</td>\n",
       "      <td>-0.014297</td>\n",
       "      <td>-0.009242</td>\n",
       "      <td>-0.235332</td>\n",
       "      <td>-0.076386</td>\n",
       "      <td>-0.18567</td>\n",
       "      <td>0.148122</td>\n",
       "      <td>0.024592</td>\n",
       "      <td>-0.576347</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>-0.22904</td>\n",
       "      <td>0.122635</td>\n",
       "      <td>0.078277</td>\n",
       "      <td>0.044588</td>\n",
       "      <td>-0.05107</td>\n",
       "      <td>-0.305125</td>\n",
       "      <td>-0.126613</td>\n",
       "      <td>0.087113</td>\n",
       "      <td>-0.020491</td>\n",
       "      <td>0.050234</td>\n",
       "      <td>-0.061356</td>\n",
       "      <td>-0.118123</td>\n",
       "      <td>0.04928</td>\n",
       "      <td>0.073472</td>\n",
       "      <td>-0.108349</td>\n",
       "      <td>-0.158302</td>\n",
       "      <td>0.465058</td>\n",
       "      <td>0.255471</td>\n",
       "      <td>0.284249</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.538596</td>\n",
       "      <td>-0.006555</td>\n",
       "      <td>0.057228</td>\n",
       "      <td>-0.272398</td>\n",
       "      <td>0.075147</td>\n",
       "      <td>-0.04602</td>\n",
       "      <td>-0.174645</td>\n",
       "      <td>0.288637</td>\n",
       "      <td>-0.183893</td>\n",
       "      <td>0.022524</td>\n",
       "      <td>0.022524</td>\n",
       "      <td>0.022524</td>\n",
       "      <td>0.08794</td>\n",
       "      <td>-1.28104</td>\n",
       "      <td>0.287855</td>\n",
       "      <td>0.066836</td>\n",
       "      <td>0.434999</td>\n",
       "      <td>-0.162659</td>\n",
       "      <td>0.084062</td>\n",
       "      <td>-0.045462</td>\n",
       "      <td>-0.014438</td>\n",
       "      <td>0.128645</td>\n",
       "      <td>-0.076572</td>\n",
       "      <td>0.194161</td>\n",
       "      <td>-0.11038</td>\n",
       "      <td>0.163122</td>\n",
       "      <td>-0.350755</td>\n",
       "      <td>-0.060632</td>\n",
       "      <td>0.460476</td>\n",
       "      <td>-0.040169</td>\n",
       "      <td>0.011349</td>\n",
       "      <td>0.04887</td>\n",
       "      <td>-0.453809</td>\n",
       "      <td>-0.233438</td>\n",
       "      <td>0.092694</td>\n",
       "      <td>0.227634</td>\n",
       "      <td>-0.114278</td>\n",
       "      <td>-0.12977</td>\n",
       "      <td>-0.30349</td>\n",
       "      <td>0.083598</td>\n",
       "      <td>0.012982</td>\n",
       "      <td>-0.42547</td>\n",
       "      <td>0.408151</td>\n",
       "      <td>0.616217</td>\n",
       "      <td>0.108565</td>\n",
       "      <td>-0.117188</td>\n",
       "      <td>0.444636</td>\n",
       "      <td>-0.169942</td>\n",
       "      <td>0.223058</td>\n",
       "      <td>-0.018665</td>\n",
       "      <td>0.086153</td>\n",
       "      <td>0.252178</td>\n",
       "      <td>0.034577</td>\n",
       "      <td>0.034577</td>\n",
       "      <td>-0.073295</td>\n",
       "      <td>0.328955</td>\n",
       "      <td>-0.289744</td>\n",
       "      <td>-0.067731</td>\n",
       "      <td>0.168038</td>\n",
       "      <td>0.051313</td>\n",
       "      <td>-0.153004</td>\n",
       "      <td>0.147223</td>\n",
       "      <td>0.10179</td>\n",
       "      <td>-0.282742</td>\n",
       "      <td>-0.108143</td>\n",
       "      <td>-0.171238</td>\n",
       "      <td>-0.215366</td>\n",
       "      <td>-0.447929</td>\n",
       "      <td>0.032773</td>\n",
       "      <td>-0.003452</td>\n",
       "      <td>0.055703</td>\n",
       "      <td>-0.136509</td>\n",
       "      <td>0.210015</td>\n",
       "      <td>-0.068223</td>\n",
       "      <td>0.161286</td>\n",
       "      <td>0.132516</td>\n",
       "      <td>-0.780321</td>\n",
       "      <td>0.254701</td>\n",
       "      <td>0.034795</td>\n",
       "      <td>-0.25084</td>\n",
       "      <td>0.064849</td>\n",
       "      <td>-0.055504</td>\n",
       "      <td>0.144463</td>\n",
       "      <td>0.300259</td>\n",
       "      <td>-0.11417</td>\n",
       "      <td>0.036296</td>\n",
       "      <td>-0.043757</td>\n",
       "      <td>-0.069088</td>\n",
       "      <td>0.25588</td>\n",
       "      <td>0.051562</td>\n",
       "      <td>-0.154409</td>\n",
       "      <td>0.048949</td>\n",
       "      <td>-0.091855</td>\n",
       "      <td>-0.181041</td>\n",
       "      <td>-0.038945</td>\n",
       "      <td>-0.038945</td>\n",
       "      <td>-0.092689</td>\n",
       "      <td>-0.228304</td>\n",
       "      <td>0.354478</td>\n",
       "      <td>-0.281828</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.113579</td>\n",
       "      <td>0.083164</td>\n",
       "      <td>0.248739</td>\n",
       "      <td>-0.110646</td>\n",
       "      <td>0.083658</td>\n",
       "      <td>-0.069804</td>\n",
       "      <td>-0.110099</td>\n",
       "      <td>-0.491825</td>\n",
       "      <td>0.005978</td>\n",
       "      <td>-0.174379</td>\n",
       "      <td>-0.037779</td>\n",
       "      <td>-0.058543</td>\n",
       "      <td>-0.058543</td>\n",
       "      <td>-0.199385</td>\n",
       "      <td>-0.199385</td>\n",
       "      <td>0.051915</td>\n",
       "      <td>-0.227849</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.208895</td>\n",
       "      <td>-0.112152</td>\n",
       "      <td>0.214461</td>\n",
       "      <td>0.103231</td>\n",
       "      <td>-0.200116</td>\n",
       "      <td>-0.072409</td>\n",
       "      <td>-0.074031</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>-0.071135</td>\n",
       "      <td>0.084541</td>\n",
       "      <td>-0.209878</td>\n",
       "      <td>0.164617</td>\n",
       "      <td>-0.033514</td>\n",
       "      <td>-0.287021</td>\n",
       "      <td>0.005589</td>\n",
       "      <td>-0.09111</td>\n",
       "      <td>-0.301151</td>\n",
       "      <td>0.318535</td>\n",
       "      <td>-0.231952</td>\n",
       "      <td>-0.067317</td>\n",
       "      <td>0.408692</td>\n",
       "      <td>-0.030199</td>\n",
       "      <td>-0.015255</td>\n",
       "      <td>-0.024318</td>\n",
       "      <td>-0.040241</td>\n",
       "      <td>-0.070589</td>\n",
       "      <td>-0.007172</td>\n",
       "      <td>-0.205295</td>\n",
       "      <td>-0.172488</td>\n",
       "      <td>0.155748</td>\n",
       "      <td>0.182368</td>\n",
       "      <td>-0.047256</td>\n",
       "      <td>-0.06569</td>\n",
       "      <td>-0.022818</td>\n",
       "      <td>-0.022818</td>\n",
       "      <td>-0.022818</td>\n",
       "      <td>-0.116528</td>\n",
       "      <td>-0.142054</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.209722</td>\n",
       "      <td>-0.043799</td>\n",
       "      <td>0.054646</td>\n",
       "      <td>0.072902</td>\n",
       "      <td>-0.146315</td>\n",
       "      <td>0.101579</td>\n",
       "      <td>-0.210988</td>\n",
       "      <td>-0.225467</td>\n",
       "      <td>0.048542</td>\n",
       "      <td>-0.060921</td>\n",
       "      <td>-0.115123</td>\n",
       "      <td>-0.109527</td>\n",
       "      <td>-0.057073</td>\n",
       "      <td>0.055363</td>\n",
       "      <td>-0.168579</td>\n",
       "      <td>-0.019975</td>\n",
       "      <td>0.061489</td>\n",
       "      <td>-0.383365</td>\n",
       "      <td>-0.064578</td>\n",
       "      <td>-0.292523</td>\n",
       "      <td>-0.017446</td>\n",
       "      <td>-0.0736</td>\n",
       "      <td>-0.150328</td>\n",
       "      <td>-0.071659</td>\n",
       "      <td>-0.009263</td>\n",
       "      <td>0.033544</td>\n",
       "      <td>0.053993</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.022344</td>\n",
       "      <td>-0.026576</td>\n",
       "      <td>0.263466</td>\n",
       "      <td>0.051497</td>\n",
       "      <td>0.034476</td>\n",
       "      <td>0.01792</td>\n",
       "      <td>-0.240831</td>\n",
       "      <td>0.030891</td>\n",
       "      <td>-0.063325</td>\n",
       "      <td>-0.050623</td>\n",
       "      <td>0.119808</td>\n",
       "      <td>0.270559</td>\n",
       "      <td>-0.594569</td>\n",
       "      <td>0.409892</td>\n",
       "      <td>-0.601565</td>\n",
       "      <td>-0.303788</td>\n",
       "      <td>-0.059649</td>\n",
       "      <td>0.016029</td>\n",
       "      <td>0.538064</td>\n",
       "      <td>0.060874</td>\n",
       "      <td>-0.00774</td>\n",
       "      <td>0.3061</td>\n",
       "      <td>0.088079</td>\n",
       "      <td>0.674292</td>\n",
       "      <td>0.089199</td>\n",
       "      <td>0.050286</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.087452</td>\n",
       "      <td>-0.291747</td>\n",
       "      <td>0.014095</td>\n",
       "      <td>0.169029</td>\n",
       "      <td>0.221815</td>\n",
       "      <td>0.007509</td>\n",
       "      <td>0.085171</td>\n",
       "      <td>0.414743</td>\n",
       "      <td>-0.085468</td>\n",
       "      <td>0.131778</td>\n",
       "      <td>-0.025757</td>\n",
       "      <td>-0.011157</td>\n",
       "      <td>-0.011157</td>\n",
       "      <td>0.098908</td>\n",
       "      <td>-0.328829</td>\n",
       "      <td>-0.012308</td>\n",
       "      <td>-0.49162</td>\n",
       "      <td>0.079339</td>\n",
       "      <td>0.265701</td>\n",
       "      <td>-0.212197</td>\n",
       "      <td>0.130132</td>\n",
       "      <td>0.057788</td>\n",
       "      <td>0.011496</td>\n",
       "      <td>-0.230263</td>\n",
       "      <td>-0.078902</td>\n",
       "      <td>-0.107062</td>\n",
       "      <td>-0.107062</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>0.129086</td>\n",
       "      <td>-0.002299</td>\n",
       "      <td>0.118558</td>\n",
       "      <td>-0.039862</td>\n",
       "      <td>-0.122609</td>\n",
       "      <td>0.060381</td>\n",
       "      <td>-0.222637</td>\n",
       "      <td>0.047406</td>\n",
       "      <td>0.274765</td>\n",
       "      <td>0.044267</td>\n",
       "      <td>0.377402</td>\n",
       "      <td>0.122007</td>\n",
       "      <td>-0.036438</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>0.02459</td>\n",
       "      <td>-0.07778</td>\n",
       "      <td>0.107969</td>\n",
       "      <td>-0.080698</td>\n",
       "      <td>0.099514</td>\n",
       "      <td>0.283346</td>\n",
       "      <td>0.082713</td>\n",
       "      <td>-0.123062</td>\n",
       "      <td>0.034147</td>\n",
       "      <td>-0.137778</td>\n",
       "      <td>0.084149</td>\n",
       "      <td>-0.037821</td>\n",
       "      <td>-0.535847</td>\n",
       "      <td>-0.187449</td>\n",
       "      <td>-0.705774</td>\n",
       "      <td>-0.019537</td>\n",
       "      <td>0.057259</td>\n",
       "      <td>0.049069</td>\n",
       "      <td>0.013937</td>\n",
       "      <td>0.038972</td>\n",
       "      <td>0.039232</td>\n",
       "      <td>-0.290525</td>\n",
       "      <td>0.11691</td>\n",
       "      <td>0.118818</td>\n",
       "      <td>0.030412</td>\n",
       "      <td>0.186821</td>\n",
       "      <td>-0.297302</td>\n",
       "      <td>0.038355</td>\n",
       "      <td>0.082695</td>\n",
       "      <td>-0.162426</td>\n",
       "      <td>-0.007776</td>\n",
       "      <td>0.090388</td>\n",
       "      <td>-0.465424</td>\n",
       "      <td>-0.070227</td>\n",
       "      <td>-0.081568</td>\n",
       "      <td>0.030719</td>\n",
       "      <td>-0.508734</td>\n",
       "      <td>0.109745</td>\n",
       "      <td>-0.423109</td>\n",
       "      <td>-0.028381</td>\n",
       "      <td>0.369053</td>\n",
       "      <td>0.042671</td>\n",
       "      <td>-0.037004</td>\n",
       "      <td>-0.16331</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>0.181057</td>\n",
       "      <td>-0.459958</td>\n",
       "      <td>0.385913</td>\n",
       "      <td>-0.020394</td>\n",
       "      <td>0.17848</td>\n",
       "      <td>-0.043952</td>\n",
       "      <td>0.024024</td>\n",
       "      <td>0.296268</td>\n",
       "      <td>0.033891</td>\n",
       "      <td>0.364426</td>\n",
       "      <td>-0.005798</td>\n",
       "      <td>-0.060089</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.035463</td>\n",
       "      <td>0.076756</td>\n",
       "      <td>-0.58515</td>\n",
       "      <td>-0.507242</td>\n",
       "      <td>0.437706</td>\n",
       "      <td>-0.147546</td>\n",
       "      <td>0.196251</td>\n",
       "      <td>0.110459</td>\n",
       "      <td>0.147317</td>\n",
       "      <td>-0.296508</td>\n",
       "      <td>0.406385</td>\n",
       "      <td>0.160566</td>\n",
       "      <td>0.283069</td>\n",
       "      <td>0.368022</td>\n",
       "      <td>-0.149424</td>\n",
       "      <td>-0.041164</td>\n",
       "      <td>-0.146211</td>\n",
       "      <td>0.040896</td>\n",
       "      <td>0.069903</td>\n",
       "      <td>0.148292</td>\n",
       "      <td>-0.07844</td>\n",
       "      <td>0.189116</td>\n",
       "      <td>0.171171</td>\n",
       "      <td>0.00235</td>\n",
       "      <td>0.114916</td>\n",
       "      <td>0.023637</td>\n",
       "      <td>0.329235</td>\n",
       "      <td>0.218473</td>\n",
       "      <td>-0.154122</td>\n",
       "      <td>1.005889</td>\n",
       "      <td>-0.020682</td>\n",
       "      <td>-0.545062</td>\n",
       "      <td>-0.190473</td>\n",
       "      <td>0.210488</td>\n",
       "      <td>0.251006</td>\n",
       "      <td>-0.048343</td>\n",
       "      <td>-0.014167</td>\n",
       "      <td>-0.014167</td>\n",
       "      <td>-0.014167</td>\n",
       "      <td>0.038456</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.221035</td>\n",
       "      <td>0.044666</td>\n",
       "      <td>0.078818</td>\n",
       "      <td>0.152585</td>\n",
       "      <td>0.060094</td>\n",
       "      <td>0.07693</td>\n",
       "      <td>-0.101928</td>\n",
       "      <td>-0.226935</td>\n",
       "      <td>0.475147</td>\n",
       "      <td>-0.027663</td>\n",
       "      <td>0.236656</td>\n",
       "      <td>0.077343</td>\n",
       "      <td>0.260145</td>\n",
       "      <td>0.105844</td>\n",
       "      <td>-0.280075</td>\n",
       "      <td>-0.291292</td>\n",
       "      <td>0.195554</td>\n",
       "      <td>-0.150114</td>\n",
       "      <td>0.982024</td>\n",
       "      <td>0.090484</td>\n",
       "      <td>0.127725</td>\n",
       "      <td>-0.030725</td>\n",
       "      <td>0.102878</td>\n",
       "      <td>0.020038</td>\n",
       "      <td>0.091779</td>\n",
       "      <td>0.227828</td>\n",
       "      <td>0.462465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  125000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aa  aa policy   aaaaand      aabg       aap     aauw       ab  \\\n",
       "0 -0.013801 -0.141119   0.135741 -0.066027  0.503888  0.03459 -0.05515   \n",
       "\n",
       "      aback   abandon  abandoned  abandoning  abandonment     abbey  \\\n",
       "0  0.543464  0.069633 -0.142672  -0.536743    0.689786     0.265999   \n",
       "\n",
       "   abbreviated  abbreviation       abc  abc news     abctv  \\\n",
       "0 -0.095073     0.17266      -0.048525  0.258237  0.043457   \n",
       "\n",
       "   abctv scottmorrisonmp  abdicate  abdicate responsibility  abdication  \\\n",
       "0  0.043457              -0.128661  0.088476                 0.088476     \n",
       "\n",
       "    abdomen  abdominal  abducted  abduction  aberrant  aberration     abhor  \\\n",
       "0 -0.086814 -0.058487   0.121984  0.032844  -0.012841 -0.103017    0.084248   \n",
       "\n",
       "   abhorrent     abide  abide rule  abiding  abigail   ability  \\\n",
       "0 -0.036005  -0.118764  0.282517    0.16833  0.00311 -1.252711   \n",
       "\n",
       "   ability ability  ability associated  ability associated chess  \\\n",
       "0 -0.093389        -0.111448           -0.012955                   \n",
       "\n",
       "   ability associated chess assumption  ability based  ability birth  \\\n",
       "0 -0.084094                             0.030585       0.056264        \n",
       "\n",
       "   ability career  ability change  ability choose  ability coder  \\\n",
       "0 -0.292663       -0.009066       -0.088057        0.016901        \n",
       "\n",
       "   ability commit  ability communicate  ability compete  ability consent  \\\n",
       "0 -0.011886       -0.010777            -0.111861        -0.111861          \n",
       "\n",
       "   ability control  ability decision  ability earn  ability experience  \\\n",
       "0 -0.111861         0.104289          0.198193      0.198193             \n",
       "\n",
       "   ability influence  ability job  ability lead  ability obtain  ability pay  \\\n",
       "0  0.026947           0.076652     0.190171      0.373146       -0.090035      \n",
       "\n",
       "   ability perceive  ability power  ability prevent  ability provide  \\\n",
       "0  0.130749          0.036709       0.073873         0.139434          \n",
       "\n",
       "   ability reproduce  ability responsibility  ability speak  ability survive  \\\n",
       "0  0.042872          -0.110928               -0.188247      -0.054043          \n",
       "\n",
       "   ability understand  ability wanted  ability willingness  \\\n",
       "0  0.031674           -0.054201       -0.092198              \n",
       "\n",
       "   ability willingness negotiate  ability willingness negotiate salary  \\\n",
       "0  0.158451                       0.033684                               \n",
       "\n",
       "       abit  abit bbc    abject  abject poverty   ablated      able  \\\n",
       "0 -0.011121  0.090753  0.073145 -1.405962       -0.147112 -0.429875   \n",
       "\n",
       "   able abandon  able abandon parental  able abandon parental obligation  \\\n",
       "0 -0.101707     -0.093644               0.154937                           \n",
       "\n",
       "   able able  able abortion  able accept  able accept trash  \\\n",
       "0 -0.067173   0.102377       0.217105    -0.040465            \n",
       "\n",
       "   able accept trash quality  able access  able accrue  able accrue wealth  \\\n",
       "0 -0.194401                  -0.117583     0.111432    -0.05733              \n",
       "\n",
       "   able accrue wealth unlike  able achieve  able acknowledge  able act  \\\n",
       "0  0.263231                   0.015192     -0.055667         -0.112371   \n",
       "\n",
       "   able address  able afford  able afford able  able afford able day  \\\n",
       "0  0.279423     -0.239471     0.010729          0.062226               \n",
       "\n",
       "   able afford private  able agree  able analyze  able analyze situation  \\\n",
       "0 -0.096364            -0.296076    0.099042      0.108727                 \n",
       "\n",
       "   able angry  able angry allowed  able angry allowed arent  able answer  \\\n",
       "0  0.071469   -0.053579            0.087489                 -0.026312      \n",
       "\n",
       "   able answer question  able appeal  able apply  able appreciate  \\\n",
       "0  0.012859              0.082486    -0.044007    0.215793          \n",
       "\n",
       "   able argument  able ask  able assault  able assist  able assist husband  \\\n",
       "0 -0.263083      -0.046557 -0.122718     -0.102466    -0.102466              \n",
       "\n",
       "   able assist husband trade  able avoid  able aware  able away  able baby  \\\n",
       "0 -0.094636                  -0.112471    0.124342   -0.015199   0.07734     \n",
       "\n",
       "   able beat  able block  able bodied  able bodied ci  able bodied disease  \\\n",
       "0  0.090896   0.219333   -0.163747    -0.00362         0.025096              \n",
       "\n",
       "   able bodied disease free  able bodied neurotypical  able body  able break  \\\n",
       "0 -0.023298                 -0.073324                  0.14148   -0.028741     \n",
       "\n",
       "   able break agreement  able breastfeed  able breathe  able bring  \\\n",
       "0 -0.04261              -0.12696         -0.022103     -0.022103     \n",
       "\n",
       "   able build  able buy  able buy house  able care  able catch  \\\n",
       "0 -0.022103   -0.062365  0.09851        -0.108937   0.057056     \n",
       "\n",
       "   able catch damn  able catch damn diagnostic  able change  \\\n",
       "0  0.092018         0.257785                   -0.021962      \n",
       "\n",
       "   able change maybe  able change mind  able choice  able choose  \\\n",
       "0  0.06291            0.028815          0.071489    -0.008874      \n",
       "\n",
       "   able choose father  able claim  able clear  able close  able comfortable  \\\n",
       "0  0.019217            0.113206   -0.28389    -0.094827    0.178487           \n",
       "\n",
       "   able comfortable home  able comfortable home guarantee  able company  \\\n",
       "0 -0.123384              -0.156738                         0.040769       \n",
       "\n",
       "   able compete  able compete ci  able compete sport  able compete sport line  \\\n",
       "0 -0.057435      0.113731         0.024201            0.179434                  \n",
       "\n",
       "   able complain  able concede  able concede status  \\\n",
       "0  0.021308       0.118899     -0.08601               \n",
       "\n",
       "   able concede status father  able connect  able consent  able consent tell  \\\n",
       "0 -0.16344                    -0.150398      0.037491     -0.080761            \n",
       "\n",
       "   able consider  able continue  able contribute  able control  able convince  \\\n",
       "0  0.011464      -0.049858      -0.049323        -0.121607     -0.0057          \n",
       "\n",
       "   able convince reddit  able cope  able court  able court order  able create  \\\n",
       "0  0.10455              -0.276675  -0.224059   -0.108227         -0.092431      \n",
       "\n",
       "   able cum  able data  able date  able day  able day recover  \\\n",
       "0 -0.092431 -0.092431  -0.02558   -0.015917 -0.156055           \n",
       "\n",
       "   able day recover clinic  able deal  able decide  able decision  \\\n",
       "0 -0.028652                 0.405327  -0.192786    -0.053531        \n",
       "\n",
       "   able defend  able definitely  able destroy  able determine  able directly  \\\n",
       "0 -0.053531     0.039243         0.003667      0.130862       -0.127839        \n",
       "\n",
       "   able discern  able discriminate  able discus  able discussion  \\\n",
       "0  0.107106     -0.15868            0.01047     -0.056014          \n",
       "\n",
       "   able distinguish  able dress  able empathize  able evidence  \\\n",
       "0 -0.238836         -0.627668   -0.00488        -0.041386        \n",
       "\n",
       "   able experience  able explain  able explain biological  \\\n",
       "0 -0.153965        -0.028448     -0.157582                  \n",
       "\n",
       "   able explain biological basis  able express  able fight  able financial  \\\n",
       "0 -0.094852                       0.202064     -0.125202   -0.02277          \n",
       "\n",
       "   able force  able forward  able fully  able gain  able game  able handle  \\\n",
       "0  0.263009   -0.1412        0.054921   -0.152945  -0.160367  -0.584844      \n",
       "\n",
       "   able happens  able hold  able identify  able incorporate  able infer  \\\n",
       "0 -0.446439      0.175182  -0.092472       0.487803         -0.061039     \n",
       "\n",
       "   able infer kind  able infer kind iat  able inform  able interpret  \\\n",
       "0 -0.649933        -0.07844             -0.488996    -0.467679         \n",
       "\n",
       "   able job  able laid  able learn  able leave  able legally  able leverage  \\\n",
       "0 -0.047036 -0.054417  -0.38204    -0.130737   -0.208354     -0.133586        \n",
       "\n",
       "   able lift  able lift pound  able live   able ma  able ma line  \\\n",
       "0  0.067749   0.177807         0.517093  -0.028604 -0.112302       \n",
       "\n",
       "   able maintain  able manipulate  able marry  able meet  able million  \\\n",
       "0 -0.448517      -0.054087         0.20955     0.136744  -0.091129       \n",
       "\n",
       "   able navigate  able note  able note moderator  able offer  able opt  \\\n",
       "0 -1.701626       0.235557   0.302203            -0.103447    0.199577   \n",
       "\n",
       "   able participate  able past  able pay  able perform  able pick  able play  \\\n",
       "0 -0.020802         -0.101514  -0.200917 -0.141872     -0.021318   0.117159    \n",
       "\n",
       "   able power  able pregnant  able protect  able prove  able provide  \\\n",
       "0  0.032993   -0.017005       0.010621     -0.086935   -0.097789       \n",
       "\n",
       "   able pull  able read  able recognize  able record  able reduce  \\\n",
       "0 -0.087312  -0.040179  -0.148746        0.126125     0.119545      \n",
       "\n",
       "   able reduced  able reduced free  able reduced free home  able refute  \\\n",
       "0 -0.038586     -0.05041           -0.039911                0.235451      \n",
       "\n",
       "   able relate  able remain  able remove  able run  able sell  able separate  \\\n",
       "0  0.085875     0.069165    -0.263898    -0.414661 -0.065961   0.069526        \n",
       "\n",
       "   able separate conversation  able separate conversation fgm  able set  \\\n",
       "0  0.527997                   -0.187985                       -0.118801   \n",
       "\n",
       "   able sexist  able share  able shut  able sign  able similar  able single  \\\n",
       "0 -0.005344     0.490073    0.07506   -0.090715  -0.222432     -0.1412        \n",
       "\n",
       "   able solve  able speak  able speak clearly  \\\n",
       "0 -0.325048   -0.069383   -0.123516             \n",
       "\n",
       "   able speak clearly unambiguously  able spend  able spend hour  able spot  \\\n",
       "0 -0.045292                         -0.084152    0.029231         0.244494    \n",
       "\n",
       "   able stand  able stay  able stop  able straight  able strong  able study  \\\n",
       "0 -0.088717   -0.084976   0.050597  -0.02377       -0.266758     0.038504     \n",
       "\n",
       "   able subpoena  able subpoena law  able survive  able talk  \\\n",
       "0  0.079417      -0.174064           0.118842      0.118842    \n",
       "\n",
       "   able talk feeling  able talk feeling scared  able teach  able tell  \\\n",
       "0 -0.098758          -0.107849                 -0.004814   -0.130708    \n",
       "\n",
       "   able toilet  able trust  able try  able turn  able twist  able understand  \\\n",
       "0 -0.148454     0.022296    0.130513  0.045675  -0.1779     -0.05608           \n",
       "\n",
       "   able understand concept  able understand concept emotional  able vote  \\\n",
       "0 -0.077782                -0.077782                          -0.03396     \n",
       "\n",
       "   able vote learn  able walk  able watch  able wear  able willing  able win  \\\n",
       "0 -0.006677         0.044879  -0.179602   -0.096941  -0.002813      0.066566   \n",
       "\n",
       "   able withdraw  able withdraw consent  able withdraw consent morning  \\\n",
       "0 -0.250945      -0.03251                0.035896                        \n",
       "\n",
       "   able word     abled   ableism  ableism calling  ableism calling believed  \\\n",
       "0 -0.103768  -0.028266 -0.081879  0.102373        -0.233645                   \n",
       "\n",
       "   ableism calling believed kid  ableism homophobia   ableist  ableist change  \\\n",
       "0  0.253002                     -0.125007            0.053407 -0.455485         \n",
       "\n",
       "   ableist change dime  ableist change dime challenging  ableist language  \\\n",
       "0 -0.037271            -0.037271                         0.071326           \n",
       "\n",
       "   ableist preference  ableist remark  ableist slur  ableist slur cognitive  \\\n",
       "0  0.071326           -0.516419       -0.108829      0.10215                  \n",
       "\n",
       "   ableist slur cognitive developmental  abnormal  abnormal behavior  \\\n",
       "0  0.766                                 0.11996   0.054351            \n",
       "\n",
       "   abnormality  abnormality hilarious  abnormality hilarious banana  \\\n",
       "0  0.054351    -0.105413               0.076546                       \n",
       "\n",
       "   abnormality hilarious banana useless  abnormally    aboard  aboard plane  \\\n",
       "0  0.173011                             -0.196716   -0.059646 -0.015787       \n",
       "\n",
       "    abolish  abolish prison  abolished  abolishing  abolishment  abolition  \\\n",
       "0 -0.015787  0.230308       -0.004226   0.038161    0.038161     0.038161    \n",
       "\n",
       "   abolitionism  abolitionist  abominable  abomination  abomination important  \\\n",
       "0  0.03963       0.188217     -0.03307    -0.13574     -0.123277                \n",
       "\n",
       "   abomination important moral  abomination important moral consideration  \\\n",
       "0  0.098853                    -1.123938                                    \n",
       "\n",
       "   aboriginal    abort  abort adoption  abort baby  abort body  abort fetus  \\\n",
       "0 -0.188604    0.43442 -0.152571        0.005715   -0.056054   -0.047699      \n",
       "\n",
       "   abort force   aborted  aborting  abortion  abortion able  \\\n",
       "0  0.101436    -0.064735  0.016623  0.075124  0.047811        \n",
       "\n",
       "   abortion abortion  abortion adoption  abortion agree  abortion anti  \\\n",
       "0  0.072275           0.106259          -0.061802        0.340723        \n",
       "\n",
       "   abortion appointment  abortion aren  abortion arguing  abortion argument  \\\n",
       "0 -0.077443             -0.03494        0.096941         -0.04679             \n",
       "\n",
       "   abortion asked  abortion attacked  abortion available  \\\n",
       "0 -0.206905        0.075695           0.133644             \n",
       "\n",
       "   abortion available assault  abortion available assault aside  \\\n",
       "0 -0.170964                   -0.029918                           \n",
       "\n",
       "   abortion avoid  abortion avoid supporting  \\\n",
       "0 -0.05191         0.099455                    \n",
       "\n",
       "   abortion avoid supporting maternity  abortion away  abortion away gay  \\\n",
       "0 -0.02733                              0.070453      -0.092718            \n",
       "\n",
       "   abortion away gay marriage  abortion baby  abortion basically  abortion bc  \\\n",
       "0 -0.007252                    0.015302      -0.056786            0.105451      \n",
       "\n",
       "   abortion begin  abortion best  abortion birth  abortion birth control  \\\n",
       "0 -0.080909        0.112019      -0.119381        0.073792                 \n",
       "\n",
       "   abortion birth control purpose  abortion bodily  abortion bodily autonomy  \\\n",
       "0  0.027526                       -0.202448         0.03279                    \n",
       "\n",
       "   abortion body  abortion body choice  abortion called  abortion came  \\\n",
       "0  0.018175      -0.025973             -0.025973        -0.171835        \n",
       "\n",
       "   abortion carry  abortion carrying  abortion carrying term  \\\n",
       "0 -0.217267        0.167015           0.202228                 \n",
       "\n",
       "   abortion carrying term analogous  abortion choice  abortion choice body  \\\n",
       "0 -0.226405                          0.053906        -0.003149               \n",
       "\n",
       "   abortion chooses  abortion chooses half  abortion chooses half pregnancy  \\\n",
       "0  0.09117          -0.158952              -0.273237                          \n",
       "\n",
       "   abortion clinic  abortion common  abortion completely  abortion consent  \\\n",
       "0 -0.224685         0.041681        -0.159544            -0.067409           \n",
       "\n",
       "   abortion consider  abortion contraceptive  abortion control  abortion cost  \\\n",
       "0 -0.210803          -0.027083                0.057484          0.169128        \n",
       "\n",
       "   abortion country  abortion cut  abortion debate  abortion defend  \\\n",
       "0  0.0462           -0.112776     -0.196997         0.043071          \n",
       "\n",
       "   abortion despite  abortion discovered  abortion discovered expecting  \\\n",
       "0 -0.100907          0.016179             0.189607                        \n",
       "\n",
       "   abortion discovered expecting son  abortion easy  abortion fetus  \\\n",
       "0  0.116955                          -0.016483      -0.005374         \n",
       "\n",
       "   abortion financial  abortion fine  abortion form  abortion free  \\\n",
       "0 -0.184704           -0.093434      -0.093434      -0.093434        \n",
       "\n",
       "   abortion freely  abortion fucked  abortion gendered  abortion happen  \\\n",
       "0  0.094764        -0.079519         0.240681           0.080963          \n",
       "\n",
       "   abortion illegal  abortion late  abortion law  abortion legal  \\\n",
       "0  0.069049          0.205445      -0.484692     -0.0716           \n",
       "\n",
       "   abortion medical  abortion morally  abortion murder  abortion necessarily  \\\n",
       "0 -0.071705         -0.21844           0.148333         0.175444               \n",
       "\n",
       "   abortion okay  abortion opinion  abortion option  abortion parent  \\\n",
       "0 -0.132484       0.545039         -0.061477         0.197428          \n",
       "\n",
       "   abortion pay  abortion performed  abortion poor  abortion position  \\\n",
       "0  0.262061      0.156871           -0.025762      -0.122377            \n",
       "\n",
       "   abortion possible  abortion pregnancy  abortion pregnant  abortion pretty  \\\n",
       "0 -0.122377           1.826338           -0.007622           0.025725          \n",
       "\n",
       "   abortion pro  abortion pro choice  abortion reproductive  \\\n",
       "0 -0.041447      0.141115             0.169675                \n",
       "\n",
       "   abortion restriction  abortion service  abortion simply  abortion wanted  \\\n",
       "0  0.438967              0.067149         -0.011826        -0.08262           \n",
       "\n",
       "   abortion week  abovementioned  abraham  abrahamic  abrahamic religion  \\\n",
       "0  0.032735      -0.024385       -0.01042 -0.003797  -0.004761             \n",
       "\n",
       "   abrahamic religion christianity  abrasion  abrasive  abridging  \\\n",
       "0 -0.080495                         0.013936  0.054567  0.047868    \n",
       "\n",
       "   abridging freedom  abridging freedom speech   abroad  abrupt  abruptly  \\\n",
       "0 -0.046693           0.040731                 -0.08616  0.4668  0.08351    \n",
       "\n",
       "    absence  absence court  absence court order  absence evidence    absent  \\\n",
       "0  0.120661 -0.297348      -0.042037            -0.051076          0.056927   \n",
       "\n",
       "   absent evidence  absent father  absentee  absolute  absolute abundance  \\\n",
       "0 -0.030117         0.09585       -0.171241  0.241729  0.020236             \n",
       "\n",
       "   absolute bare  absolute bare minimum  absolute best  absolute bitch  \\\n",
       "0 -0.083961      -0.048765              -0.024382       0.022319         \n",
       "\n",
       "   absolute confidence  absolute garbage  absolute hell  absolute hiv  \\\n",
       "0  0.094539             0.094539         -0.002156      -0.057168       \n",
       "\n",
       "   absolute hiv number  absolute hiv number methodological  absolute nonsense  \\\n",
       "0 -0.227791             0.005482                            0.13769             \n",
       "\n",
       "   absolute opposite  absolute opposite love  absolute ops  \\\n",
       "0  0.030075          -0.07476                -0.064297       \n",
       "\n",
       "   absolute ops business  absolute ops business ask  absolute outrage  \\\n",
       "0 -0.109869              -0.097301                  -0.101066           \n",
       "\n",
       "   absolute piece  absolute piece shit    ...     yes consider  \\\n",
       "0 -0.015474        0.290786               ...     0.095013       \n",
       "\n",
       "   yes consider using  yes consider using gain  yes considered  yes correct  \\\n",
       "0 -0.178606           -0.020043                -0.131688        0.112962      \n",
       "\n",
       "   yes course  yes created  yes crime  yes cross  yes culture  yes current  \\\n",
       "0 -0.204788    0.050109     0.028321   0.159058  -0.089152    -0.011177      \n",
       "\n",
       "   yes currently  yes currently biology  yes currently biology relevance  \\\n",
       "0 -0.193152      -0.108236              -0.112438                          \n",
       "\n",
       "    yes dad  yes dangerous   yes day  yes dead  yes decided  yes decision  \\\n",
       "0 -0.039882 -0.125846       0.218854  0.034804 -0.148204    -0.057877       \n",
       "\n",
       "   yes decision body  yes decision impact  yes decision impact choice  \\\n",
       "0 -0.03885            0.021082             0.055308                     \n",
       "\n",
       "   yes default  yes definitely  yes definitely site  yes definitely site wide  \\\n",
       "0  0.095366     0.110642       -0.157254             0.130372                   \n",
       "\n",
       "   yes definition  yes depends  yes difference  yes different  yes directly  \\\n",
       "0 -0.165282        0.15822      0.16993        -0.04146       -0.012433       \n",
       "\n",
       "   yes disagree  yes distinction  yes distinction important  \\\n",
       "0 -0.223001     -0.128088         0.138956                    \n",
       "\n",
       "   yes distinction important denying  yes doing  yes domination  \\\n",
       "0 -0.102106                           0.01063    0.099535         \n",
       "\n",
       "   yes endorsing  yes enthusiastic  yes especially  yes exactly  yes example  \\\n",
       "0  0.115148       0.118905          0.047721        0.018354     0.018354      \n",
       "\n",
       "   yes exception  yes experience  yes explain  yes fair  yes false  \\\n",
       "0  0.018354       0.018354        0.207855     0.08103   0.026225    \n",
       "\n",
       "   yes false accusation  yes feeling  yes feminine  yes fine   yes fix  \\\n",
       "0 -0.267899             -0.047946     0.110324      0.001897 -0.102733   \n",
       "\n",
       "   yes fluid  yes fluid nb  yes fluid nb keen  yes focus  yes free  \\\n",
       "0 -0.171455   0.092736      0.092407          -0.025842   0.053629   \n",
       "\n",
       "   yes free informed  yes free informed consent  yes freely  \\\n",
       "0 -0.014642          -0.025345                   0.045047     \n",
       "\n",
       "   yes freely enthusiastically  yes freely enthusiastically given  yes friend  \\\n",
       "0  0.045047                     0.073592                          -0.055425     \n",
       "\n",
       "   yes fucking  yes gave   yes gay  yes generally   yes got  yes guess  \\\n",
       "0  0.135004     0.135004  0.075854 -0.250672       0.092427  0.114059    \n",
       "\n",
       "   yes happen  yes happened  yes hard  yes hate  yes hear  yes heard  \\\n",
       "0  0.167384   -0.053314      0.057647 -0.022241 -0.2184    0.067159    \n",
       "\n",
       "   yes hold  yes home  yes home longer  yes home longer allowed  \\\n",
       "0 -0.094351 -0.116624  0.209809         0.110495                  \n",
       "\n",
       "   yes homemaker  yes huge  yes human  yes idea  yes important  yes including  \\\n",
       "0  0.064209       0.102088  0.012837   0.042122 -0.050209       0.043078        \n",
       "\n",
       "   yes inequality  yes interesting  yes involved  yes involved ending  \\\n",
       "0 -0.08549         0.111026         0.111026     -0.014297              \n",
       "\n",
       "   yes involved ending innocent   yes job  yes join  yes kid  yes kind  \\\n",
       "0 -0.009242                     -0.235332 -0.076386 -0.18567  0.148122   \n",
       "\n",
       "    yes law  yes lawyer  yes lead  yes let  yes let continue  yes likely  \\\n",
       "0  0.024592 -0.576347    0.008605 -0.22904  0.122635          0.078277     \n",
       "\n",
       "   yes limit  yes literally  yes making  yes matter  yes maybe  yes meant  \\\n",
       "0  0.044588  -0.05107       -0.305125   -0.126613    0.087113  -0.020491    \n",
       "\n",
       "   yes medium  yes mental  yes mental illness  yes mention   yes met  \\\n",
       "0  0.050234   -0.061356   -0.118123            0.04928      0.073472   \n",
       "\n",
       "   yes mother  yes muslim  yes necessarily  yes necessarily real  \\\n",
       "0 -0.108349   -0.158302    0.465058         0.255471               \n",
       "\n",
       "   yes necessarily real human  yes news  yes news deemed  yes news deemed law  \\\n",
       "0  0.284249                    0.148515  0.538596        -0.006555              \n",
       "\n",
       "    yes non  yes obviously    yes ok  yes okay   yes old  yes opinion  \\\n",
       "0  0.057228 -0.272398       0.075147 -0.04602  -0.174645  0.288637      \n",
       "\n",
       "   yes oppresses  yes oppresses wouldn  yes oppresses wouldn eager  \\\n",
       "0 -0.183893       0.022524              0.022524                     \n",
       "\n",
       "   yes option  yes outside  yes partially  yes past  yes patriarchy  \\\n",
       "0  0.022524    0.08794     -1.28104        0.287855  0.066836         \n",
       "\n",
       "   yes personally  yes politics  yes prefer  yes pretty  yes privilege  \\\n",
       "0  0.434999       -0.162659      0.084062   -0.045462   -0.014438        \n",
       "\n",
       "   yes probably  yes probably stalemate  yes probably stalemate change  \\\n",
       "0  0.128645     -0.076572                0.194161                        \n",
       "\n",
       "   yes prominent  yes question  yes quite  yes race  yes race play  \\\n",
       "0 -0.11038        0.163122     -0.350755  -0.060632  0.460476        \n",
       "\n",
       "   yes radical  yes rapist  yes rationale  yes react  yes react id  \\\n",
       "0 -0.040169     0.011349    0.04887       -0.453809  -0.233438       \n",
       "\n",
       "   yes react id skeptical  yes read  yes real  yes realize  yes refer  \\\n",
       "0  0.092694                0.227634 -0.114278 -0.12977     -0.30349     \n",
       "\n",
       "   yes refer toxic  yes refer toxic masculinity  yes regret  yes relationship  \\\n",
       "0  0.083598         0.012982                    -0.42547     0.408151           \n",
       "\n",
       "   yes removal  yes removal idelaogical  yes removal idelaogical argument  \\\n",
       "0  0.616217     0.108565                -0.117188                           \n",
       "\n",
       "   yes ridiculous  yes risk  yes rocket  yes rocket science  yes role  \\\n",
       "0  0.444636       -0.169942  0.223058   -0.018665            0.086153   \n",
       "\n",
       "   yes role switched  yes role switched protest   yes saw  yes school  \\\n",
       "0  0.252178           0.034577                   0.034577 -0.073295     \n",
       "\n",
       "   yes science  yes scream  yes scream bring  yes scream bring awareness  \\\n",
       "0  0.328955    -0.289744   -0.067731          0.168038                     \n",
       "\n",
       "   yes second  yes seen  yes seen article  yes self  yes self defense  \\\n",
       "0  0.051313   -0.153004  0.147223          0.10179  -0.282742           \n",
       "\n",
       "   yes sense  yes sexism  yes sexist  yes shitty  yes solid  yes sort  \\\n",
       "0 -0.108143  -0.171238   -0.215366   -0.447929    0.032773  -0.003452   \n",
       "\n",
       "   yes specifically  yes start  yes stop  yes strong  yes strongly  \\\n",
       "0  0.055703         -0.136509   0.210015 -0.068223    0.161286       \n",
       "\n",
       "   yes strongly anti  yes strongly anti doping  yes study  yes stuff  \\\n",
       "0  0.132516          -0.780321                  0.254701   0.034795    \n",
       "\n",
       "   yes talk  yes talking  yes taught  yes teach  yes technically  yes tell  \\\n",
       "0 -0.25084   0.064849    -0.055504    0.144463   0.300259        -0.11417    \n",
       "\n",
       "   yes tend  yes terrible  yes thank  yes told  yes totally  yes trans  \\\n",
       "0  0.036296 -0.043757     -0.069088   0.25588   0.051562    -0.154409    \n",
       "\n",
       "   yes transphobic  yes true   yes try  yes type  yes understand  \\\n",
       "0  0.048949        -0.091855 -0.181041 -0.038945 -0.038945         \n",
       "\n",
       "   yes understanding  yes used  yes using  yes wanted  yes wasn  yes wearing  \\\n",
       "0 -0.092689          -0.228304  0.354478  -0.281828    0.007001  0.113579      \n",
       "\n",
       "   yes word  yes worse  yes wouldn   yes yes  yes yes enthusiastic  \\\n",
       "0  0.083164  0.248739  -0.110646    0.083658 -0.069804               \n",
       "\n",
       "   yes yes important  yes yes yes  yesallmen  yesterday  yesterday evening  \\\n",
       "0 -0.110099          -0.491825     0.005978  -0.174379  -0.037779            \n",
       "\n",
       "   yesterday posted  yesterday revealed        yi  yiannopoulos     yield  \\\n",
       "0 -0.058543         -0.058543           -0.199385 -0.199385      0.051915   \n",
       "\n",
       "    yielded     yikes       yin  yin yang      yinz      ymca      ymmv  \\\n",
       "0 -0.227849  0.028345  0.208895 -0.112152  0.214461  0.103231 -0.200116   \n",
       "\n",
       "         yo    yo kid      yoga  yoga pant      yoke      york  york article  \\\n",
       "0 -0.072409 -0.074031  0.017978 -0.071135   0.084541 -0.209878  0.164617       \n",
       "\n",
       "   york city  york state  york state law  york university    yorker  \\\n",
       "0 -0.033514  -0.287021    0.005589       -0.09111         -0.301151   \n",
       "\n",
       "   yorkshire      youd     youll     young  young able  young able inform  \\\n",
       "0  0.318535  -0.231952 -0.067317  0.408692 -0.030199   -0.015255            \n",
       "\n",
       "   young absolutely  young academic  young actress  young adult  \\\n",
       "0 -0.024318         -0.040241       -0.070589      -0.007172      \n",
       "\n",
       "   young adult strongly  young adult strongly associated  young adult study  \\\n",
       "0 -0.205295             -0.172488                         0.155748            \n",
       "\n",
       "   young adult study parental  young age  young age knew  young age simply  \\\n",
       "0  0.182368                   -0.047256  -0.06569        -0.022818           \n",
       "\n",
       "   young aged  young aged old  young aged old complete  young american  \\\n",
       "0 -0.022818   -0.022818       -0.116528                -0.142054         \n",
       "\n",
       "   young angry  young aren  young asian  young ask  young attractive  \\\n",
       "0  0.038707     0.209722   -0.043799     0.054646   0.072902           \n",
       "\n",
       "   young baby  young barely  young beautiful  young black  young black commit  \\\n",
       "0 -0.146315    0.101579     -0.210988        -0.225467     0.048542             \n",
       "\n",
       "   young bodily  young bodily integrity  young boy  young boy aren  \\\n",
       "0 -0.060921     -0.115123               -0.109527  -0.057073         \n",
       "\n",
       "   young boy born  young boy born surgery  young caught  young childless  \\\n",
       "0  0.055363       -0.168579               -0.019975      0.061489          \n",
       "\n",
       "   young coming  young consent  young conservative  young couple  \\\n",
       "0 -0.383365     -0.064578      -0.292523           -0.017446       \n",
       "\n",
       "   young daughter  young day  young desperate  young doing  young face  \\\n",
       "0 -0.0736         -0.150328  -0.071659        -0.009263     0.033544     \n",
       "\n",
       "   young father  young fertile  young fertile attractive  \\\n",
       "0  0.053993      0.029788       0.022344                   \n",
       "\n",
       "   young fertile attractive countless  young fertile attractive period  \\\n",
       "0 -0.026576                            0.263466                          \n",
       "\n",
       "   young got  young hate  young hot  young human  young infant  \\\n",
       "0  0.051497   0.034476    0.01792   -0.240831     0.030891       \n",
       "\n",
       "   young interested  young kid  young kind  young lady  young lady narrating  \\\n",
       "0 -0.063325         -0.050623   0.119808    0.270559   -0.594569               \n",
       "\n",
       "   young lady narrating video  young learn  young left  young likely  \\\n",
       "0  0.409892                   -0.601565    -0.303788   -0.059649       \n",
       "\n",
       "   young live  young live home  young living  young living city  \\\n",
       "0  0.016029    0.538064         0.060874     -0.00774             \n",
       "\n",
       "   young living city earn  young looking  young majority  \\\n",
       "0  0.3061                  0.088079       0.674292         \n",
       "\n",
       "   young majority college  young majority college student  young mere  \\\n",
       "0  0.089199                0.050286                        0.034548     \n",
       "\n",
       "   young mind  young money  young mother  young moved  young murdered  \\\n",
       "0  0.087452   -0.291747     0.014095      0.169029     0.221815         \n",
       "\n",
       "   young nazi  young nazi soldier  young nazi soldier fall  young negative  \\\n",
       "0  0.007509    0.085171            0.414743                -0.085468         \n",
       "\n",
       "   young old  young old emotion  young old emotion dont  young owning  \\\n",
       "0  0.131778  -0.025757          -0.011157               -0.011157       \n",
       "\n",
       "   young owning property  young parent  young particular  young perception  \\\n",
       "0  0.098908              -0.328829     -0.012308         -0.49162            \n",
       "\n",
       "   young perception attitude  young perception attitude behaviour  \\\n",
       "0  0.079339                   0.265701                              \n",
       "\n",
       "   young perform  young phase  young phase figure  young phase figure angry  \\\n",
       "0 -0.212197       0.130132     0.057788            0.011496                   \n",
       "\n",
       "   young politician  young pretty  young promised  young raped  \\\n",
       "0 -0.230263         -0.078902     -0.107062       -0.107062      \n",
       "\n",
       "   young single childless earn  young skill  young son  young sort  \\\n",
       "0  0.033344                     0.129086    -0.002299   0.118558     \n",
       "\n",
       "   young speak  young start  young student  young teen  young teenager  \\\n",
       "0 -0.039862    -0.122609     0.060381      -0.222637    0.047406         \n",
       "\n",
       "   young today  young told  young used  young workforce  young worried  \\\n",
       "0  0.274765     0.044267    0.377402    0.122007        -0.036438        \n",
       "\n",
       "   young young  younger  younger age  younger attractive  younger born  \\\n",
       "0  0.175751     0.02459 -0.07778      0.107969           -0.080698       \n",
       "\n",
       "   younger boy  younger brother  younger doing  younger generation  \\\n",
       "0  0.099514     0.283346         0.082713      -0.123062             \n",
       "\n",
       "   younger older  younger older younger  younger rapist  younger self  \\\n",
       "0  0.034147      -0.137778               0.084149       -0.037821       \n",
       "\n",
       "   younger sister  youngest  youngest son  youngster     youre  youre older  \\\n",
       "0 -0.535847       -0.187449 -0.705774     -0.019537   0.057259  0.049069      \n",
       "\n",
       "   youre older instead  youre older instead transitioning  youre sexist  \\\n",
       "0  0.013937             0.038972                           0.039232       \n",
       "\n",
       "      youse     yout     youth  youth age  youth age domestic  \\\n",
       "0 -0.290525  0.11691  0.118818  0.030412   0.186821             \n",
       "\n",
       "   youth age domestic violence  youth beauty  youth boy  youth club  \\\n",
       "0 -0.297302                     0.038355      0.082695  -0.162426     \n",
       "\n",
       "   youth mad  youth suicide  youth suicide average  \\\n",
       "0 -0.007776   0.090388      -0.465424                \n",
       "\n",
       "   youth suicide average okay  youthful    youtoo  youtoo successful  \\\n",
       "0 -0.070227                   -0.081568  0.030719 -0.508734            \n",
       "\n",
       "   youtoo successful using  youtoo successful using pussy  youtried  \\\n",
       "0  0.109745                -0.423109                      -0.028381   \n",
       "\n",
       "   youtried jpg   youtube  youtube antifeminist  youtube audio  \\\n",
       "0  0.369053      0.042671 -0.037004             -0.16331         \n",
       "\n",
       "   youtube channel  youtube channel called  youtube channel extracredits  \\\n",
       "0  0.004206         0.181057               -0.459958                       \n",
       "\n",
       "   youtube channel focused  youtube channel kenyan  \\\n",
       "0  0.385913                -0.020394                 \n",
       "\n",
       "   youtube channel kenyan cosmetic  youtube channel similar  youtube check  \\\n",
       "0  0.17848                         -0.043952                 0.024024        \n",
       "\n",
       "   youtube got  youtube great  youtube great analysis  \\\n",
       "0  0.296268     0.033891       0.364426                 \n",
       "\n",
       "   youtube great analysis masculinity  youtube link  youtube probably  \\\n",
       "0 -0.005798                           -0.060089      0.008443           \n",
       "\n",
       "   youtube search  youtube shooting  youtube spotlight  youtube star  \\\n",
       "0  0.035463        0.076756         -0.58515           -0.507242       \n",
       "\n",
       "   youtube subscriber  youtube talk  youtube try  youtube tv  youtube video  \\\n",
       "0  0.437706           -0.147546      0.196251     0.110459    0.147317        \n",
       "\n",
       "   youtuber  youtubers  youtubes     youve  youve got       ypt       ypu  \\\n",
       "0 -0.296508  0.406385   0.160566  0.283069  0.368022  -0.149424 -0.041164   \n",
       "\n",
       "         yr    yr old        yt      yuck      yum     yummy      yung  \\\n",
       "0 -0.146211  0.040896  0.069903  0.148292 -0.07844  0.189116  0.171171   \n",
       "\n",
       "       yup    zahidi     zarya   zealand  zealand report  \\\n",
       "0  0.00235  0.114916  0.023637  0.329235  0.218473         \n",
       "\n",
       "   zealand report human  zealand submission    zealot   zealous  zeitgeist  \\\n",
       "0 -0.154122              1.005889           -0.020682 -0.545062 -0.190473    \n",
       "\n",
       "      zelda      zero  zero dawn  zero dollar  zero effect  zero evidence  \\\n",
       "0  0.210488  0.251006 -0.048343  -0.014167    -0.014167    -0.014167        \n",
       "\n",
       "   zero experience  zero precedent  zero proof  zero repercussion  \\\n",
       "0  0.038456         0.09866         0.221035    0.044666            \n",
       "\n",
       "   zero restriction  zero self  zero sense  zero sum  zero sum game  \\\n",
       "0  0.078818          0.152585   0.060094    0.07693  -0.101928        \n",
       "\n",
       "   zero sum situation  zero sympathy  zero tolerance  zero tolerance policy  \\\n",
       "0 -0.226935            0.475147      -0.027663        0.236656                \n",
       "\n",
       "   zero value   zeroing  zimbardo   zionism    zipper     zizek       zoe  \\\n",
       "0  0.077343    0.260145  0.105844 -0.280075 -0.291292  0.195554 -0.150114   \n",
       "\n",
       "   zoe quinn    zombie  zombie apocalypse      zone    zoning       zoo  \\\n",
       "0  0.982024   0.090484  0.127725          -0.030725  0.102878  0.020038   \n",
       "\n",
       "   zuckerberg       zur    zygote  \n",
       "0  0.091779    0.227828  0.462465  \n",
       "\n",
       "[1 rows x 125000 columns]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "member dominant                      5.570271\n",
       "bread scientist react violently      4.393380\n",
       "argument moving                      4.075883\n",
       "message author                       3.676436\n",
       "sexually submissive                  3.567055\n",
       "brave serve                          3.275661\n",
       "mobile atm                           3.235209\n",
       "manifesting                          3.220126\n",
       "self righteousness simply            2.914714\n",
       "atlassian accepted majority coder    2.782258\n",
       "beg scrap                            2.760610\n",
       "slut slut                            2.736139\n",
       "expendable                           2.723877\n",
       "month son                            2.716348\n",
       "bias court                           2.708516\n",
       "dtype: float64"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most positively correlated with content being from MensRights\n",
    "# some of these make NO sense, might have been good to do more cleaning\n",
    "coef_df.sum(axis=0).sort_values(ascending=False)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pre determined                  -5.681340\n",
       "study seminar                   -4.972181\n",
       "talking federation autonomous   -4.799818\n",
       "eff                             -3.908666\n",
       "anti sjw                        -3.638601\n",
       "desperate attempt               -3.596841\n",
       "shitty yeah                     -3.415897\n",
       "pick apart                      -3.373890\n",
       "okay sexualize                  -3.134356\n",
       "exposed message                 -3.017257\n",
       "brainwashing hell drug          -2.897521\n",
       "duo                             -2.883798\n",
       "distinguish                     -2.873333\n",
       "non binary particularly         -2.842388\n",
       "appearance body                 -2.806616\n",
       "dtype: float64"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most positively correlated with content being from AskFeminists\n",
    "# these make a little more sense than the MensRights ones...slightly.\n",
    "coef_df.sum(axis=0).sort_values(ascending=True)[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log = logreg.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11591,)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11591,)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 4317\n",
      "False Positives: 1474\n",
      "False Negatives: 1302\n",
      "True Positives: 4498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred_log)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_log).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up df for comparison\n",
    "y_test_preds = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds['pred'] = pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_MensRights</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57021</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46763</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37992</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11353</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_MensRights  pred\n",
       "57021  1                     0   \n",
       "46763  1                     1   \n",
       "26190  0                     0   \n",
       "37992  1                     1   \n",
       "11353  0                     0   "
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_MensRights</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57021</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53228</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18964</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49924</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41747</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_MensRights  pred\n",
       "57021  1                     0   \n",
       "53228  1                     0   \n",
       "18964  0                     1   \n",
       "49924  1                     0   \n",
       "41747  1                     0   "
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows where predictions don't match\n",
    "y_test_preds[y_test_preds['subreddit_MensRights'] != y_test_preds['pred']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                   19820                                                                                                                                                                                                                                                   \n",
       "text                    The first clip legitimately doesn't work, and none from the second are JP so I've still not seen proof of your origional claim that JP ever said that. If you send me a functioning clip then I'll change my opinion but it just says video unavailable.\n",
       "type                    comment                                                                                                                                                                                                                                                 \n",
       "removed                 0                                                                                                                                                                                                                                                       \n",
       "deleted                 0                                                                                                                                                                                                                                                       \n",
       "clean_text_stop         the first clip legitimately doesn t work  and none from the second are jp so i ve still not seen proof of your origional claim that jp ever said that  if you send me a functioning clip then i ll change my opinion but it just says video unavailable \n",
       "lems                    the first clip legitimately doesn work and none from the second are jp so i still not seen proof of your origional claim that jp ever said that if you send me a functioning clip then i change my opinion but it just say video unavailable            \n",
       "subreddit_MensRights    1                                                                                                                                                                                                                                                       \n",
       "Name: 57021, dtype: object"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a post misclassified as AskFeminists\n",
    "#this comment is just responding to another user who posted something about JP (Jordan Peterson)\n",
    "# but there's no real context that would help identify which thread this belongs to\n",
    "femvmen.loc[57021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                   19170                                                                     \n",
       "text                    Dude, I can't argue with you when you are so deep in the dogma. Good luck.\n",
       "type                    comment                                                                   \n",
       "removed                 0                                                                         \n",
       "deleted                 0                                                                         \n",
       "clean_text_stop         dude  i can t argue with you when you are so deep in the dogma  good luck \n",
       "lems                    dude i can argue with you when you are so deep in the dogma good luck     \n",
       "subreddit_MensRights    0                                                                         \n",
       "Name: 18964, dtype: object"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a post misclassified as MensRights\n",
    "# not much to go off here for classification, it could really be from either.\n",
    "femvmen.loc[18964]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried all of the below models before selecting the Logistic Regression for further analysis of the coefficients - none of these had as strong a test score as the one on the Logistic Regression. I spent more time with Random Forest, and I could have tried to tune the parameters for Naive Bayes and SVC to improve the scores but didn't have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train_tf2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8503796048658442"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_train_tf2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5014235182469157"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_test_tf2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel= 'rbf', C = 100, gamma = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.05, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9543180053489777"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4925373134328358"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking sample for faster gridsearching\n",
    "sample20000 = femvmen.sample(n=20000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5018\n",
       "0    0.4982\n",
       "Name: subreddit_MensRights, dtype: float64"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample20000['subreddit_MensRights'].value_counts(normalize=True) # about same proportions as full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample20000['lems']\n",
    "y = sample20000['subreddit_MensRights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = .30, \n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 125000)"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just trying out basic random forest without tuning parameters or setting max depth\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9857142857142858"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6108333333333333"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearching for best parameters - this is the first one I did\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6128571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_leaf_nodes': 20000,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -2,\n",
       " 'oob_score': 'False',\n",
       " 'warm_start': 'True'}"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [5, 10, 1],\n",
    "    'max_depth': [10000, 90000, 20000],\n",
    "    'oob_score': ['True', 'False'],\n",
    "    'warm_start': ['True'],\n",
    "    'n_jobs': [-2]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid = params, cv = 3)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6589285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 880,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -2,\n",
       " 'oob_score': 'False',\n",
       " 'warm_start': 'True'}"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second gridsearch, I repeated this until I narrowed down the best max depth range\n",
    "params = {\n",
    "    'n_estimators': [10],\n",
    "    'max_depth': range(875, 900, 1),\n",
    "    'oob_score': ['False'],\n",
    "    'warm_start': ['True'],\n",
    "    'n_jobs': [-2]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid = params, cv = 3)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting X and y variables to test random forest on full train/test sets\n",
    "X = femvmen['lems']\n",
    "y = femvmen['subreddit_MensRights']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = .25,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 880, n_estimators = 1000, n_jobs = -2, oob_score = False,\n",
    "                 warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=880, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-2,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9846776791055077"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7199254606943198"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be too easy to say that my best model's inability to crack 77% accuracy means that \"we're really not that different after all\", and if I were to spend more time on this project there are other options I would explore to try and improve my model:\n",
    "\n",
    "1. I'd pull a completely unrelated subreddit to test as a control against the other two subreddits to make sure I was tuning the best model possible.\n",
    "\n",
    "2. Given how important context is for the comments, I would either try to aggregate comments on a post and analyze them together, or set a minimum wordcount on comments to be used in the analysis to give the model a better chance of distinguishing them.\n",
    "\n",
    "3. I could scrape another years' worth of data to add to the model.\n",
    "\n",
    "4. I could do some more cleaning and EDA that might help consolidate slang/similar terms that weren't captured by the lemmatizer, and identify stronger trends in the subreddits that could be leveraged for better prediction. It'd also be interesting to see what difference it might make to fit the TFIDF on just one subreddit first, then transform both subreddits and analyze them together.\n",
    "\n",
    "5. If any of the above helped minimize overfit on the Logistic Regression or Random Forest, I'd give Naive Bayes Multinomial and Support Vector Classifier another shot (and spend more time tuning them).\n",
    "\n",
    "The really interesting questions this project has generated would require a deeper analysis: what's the overlap in people who post on MensRights and AskFeminists? (How many of those are trolls, whose content is removed?) What common themes exist between removed posts (can we write an algorithm for trolling content?) How does sentiment analysis compare across the two subreddits, is there a discernable difference? Is it possible to measure 'extreme' attitudes in a subreddit, and if so can it be mapped over time, against real events happening in the world that might trigger anger on both or one side? Is it possible to follow the posts of single users and see how they change in sentiment and neutrality over time (and does the sub they post on the most make a difference?) Maybe I'll come back to it when I've solidified my modeling skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
