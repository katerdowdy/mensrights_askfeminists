{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: 'AskFeminists' vs. 'MensRights'\n",
    "## Part C: Vectorizing & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**Vectorizing**](#vector)\n",
    "\n",
    "Before combining the two subreddit dataframes into one for modeling, I tested out different vectorizing options on both to compare the two subreddits. I tried out CountVectorizer and TFIDF on the AskFeminists content setting ngrams = (3-5) to get the top phrases from both: TFIDF was less repetitive than the CountVectorizer so I used TFIDF for vectorizing throughout. \n",
    "\n",
    "I tried vectorizing on ngrams 3-5, 1-2, and just single words, and created two custom lists of stopwords: one with all English stopwords and words in common between the top 100 words (without stopwords) from each subreddit; the second list of stopwords was created after fitting TFIDF on both subreddits (with stopwords = the first custom list) and taking the common words from the top 100 lists of words for each subreddit again. I tested models with both sets of stopwords and ultimately they didn't improve the model over using stopwords = 'english.')\n",
    "\n",
    "[**Modeling**](#model)\n",
    "\n",
    "I primarily tested two models on the lemmatized text: Logistic Regression and Random Forest Classifier. Starting with Logistic Regression, I tried several different parameters for vectorizing with TFIDF and the best parameters were 125,000 max features, ngrams = 1-4, and stopwords = 'English'.\n",
    "\n",
    "The best test score I had ended up being on Logistic Regression (76% accuracy compared to baseline of 50%). This model was overfit on the training data (85% accuracy), and setting lower max_features closed the gap to a 3% difference between training/testing scores but also lowered the test scores. \n",
    "\n",
    "The best training score I had was with RandomForestClassifier, n_estimators = 100 and max_depth = 880, which gave me 98% on training data (but just 72% on the test data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "men = pd.read_csv('./data/men_clean_lem')\n",
    "fem = pd.read_csv('./data/fem_clean_lem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing datasets, removing nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men['lems'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "men.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem['lems'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fem.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31221, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28955, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating smaller version for mensrights, same # rows as submissions for askfeminists\n",
    "men = men.sample(29000, replace = False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29000, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "men.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing: Separate Analysis <a name=\"vector\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a CountVectorizer\n",
    "vect = CountVectorizer(ngram_range=(3,5), max_features = 10000, stop_words = 'english')\n",
    "# Instantiate TFIDF\n",
    "tfidf = TfidfVectorizer(ngram_range = (3, 5), max_features = 10000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing fem for test\n",
    "fem_vect = vect.fit_transform(fem['lems'])\n",
    "# tfidfing fem for test\n",
    "fem_tfidf = tfidf.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words\n",
    "fem_vect_df = pd.DataFrame(fem_vect.toarray(), columns=vect.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reflect feminist perspective                  160\n",
       "feminist reflect feminist                     155\n",
       "feminist reflect feminist perspective         153\n",
       "come feminist reflect                         89 \n",
       "come feminist reflect feminist                89 \n",
       "come feminist reflect feminist perspective    89 \n",
       "level comment thread                          81 \n",
       "difference men woman                          80 \n",
       "feminist perspective comment                  74 \n",
       "doesn make sense                              74 \n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts\n",
    "vect_counts = fem_vect_df.sum(axis=0)\n",
    "vect_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doesn make sense                41.643637\n",
       "false rape accusation           36.430295\n",
       "difference men woman            32.162463\n",
       "traditional gender role         28.743795\n",
       "just don think                  27.087392\n",
       "reflect feminist perspective    26.147715\n",
       "don really know                 25.665504\n",
       "innocent proven guilty          24.234714\n",
       "rape sexual assault             24.215631\n",
       "gt don think                    23.785815\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at tfidf value counts\n",
    "tfidf_counts = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF seems to have done a better job of ignoring similar phrases than count vectorizer (ex. 'reflect feminist perspective') and the results are more interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing Top Phrases (TFIDF, ngrams 3-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "men_tfidf = tfidf.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false rape accusation     62.306164\n",
       "men right movement        60.595215\n",
       "men right issue           54.943756\n",
       "international men day     50.388507\n",
       "men right activist        39.744089\n",
       "gender pay gap            33.950820\n",
       "innocent proven guilty    33.152104\n",
       "pay child support         27.741176\n",
       "year old boy              22.977497\n",
       "doesn make sense          22.798229\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling top 10 phrases for men TFIDF with English stopwords, 3 are the same as fem\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing single words and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1, 2), max_features = 10000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "fem_tfidf = tfidf.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf = tfidf.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf.get_feature_names())\n",
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wife ha             976.118614\n",
       "men family          729.171984\n",
       "fellow              677.549175\n",
       "thing               612.694714\n",
       "divorce             607.185712\n",
       "partner violence    605.775949\n",
       "lifestyle           550.282282\n",
       "june                537.683578\n",
       "voting              513.169382\n",
       "greatest            450.885487\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling top 10 words/phrases for fem TFIDF with English stopwords\n",
    "tfidf_counts_fem = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       868.976790\n",
       "men         826.650364\n",
       "wa          579.965131\n",
       "just        491.954841\n",
       "like        476.984193\n",
       "don         462.072782\n",
       "people      423.281936\n",
       "gt          407.147621\n",
       "right       403.265087\n",
       "feminist    391.284568\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pulling top 10 words/phrases for men TFIDF with English stopwords\n",
    "# interesting that feminist is one of the top 10 words for mensrights\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparing single words & bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top ngrams 1-2\n",
    "top_100_fem_ngrams_stop = tfidf_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_ngrams_stop = tfidf_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar ngrams in both\n",
    "fem_men_ngrams = set(top_100_men_ngrams_stop.index) & set(top_100_fem_ngrams_stop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'thing'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# not a lot of similarity in smaller ngrams\n",
    "fem_men_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't a lot of similarity when comparing ngrams 1-2 between the two subs - AskFeminists has more common 2-word phrases (with stopwords removed) which suggest more similar contextualization of common words such as 'men' and 'women'. MensRights has very few 2-word phrases that are common, so the top 100 words/short phrases come with less context than the list for AskFeminists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing top words (ngrams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf1 = TfidfVectorizer(max_features = 30000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "fem_tfidf = tfidf1.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28955, 30000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf = tfidf1.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workshop     1059.252157\n",
       "manigault    803.501338 \n",
       "fanedit      752.867745 \n",
       "doesnt       665.165769 \n",
       "tomboy       655.855623 \n",
       "paypal       645.690254 \n",
       "lawyered     582.257494 \n",
       "jordon       572.694283 \n",
       "weak         530.538017 \n",
       "hardwick     474.213095 \n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for askfeminists\n",
    "tfidf_counts_fem = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       932.960891\n",
       "men         894.450155\n",
       "wa          598.002861\n",
       "just        512.151036\n",
       "don         496.218393\n",
       "like        493.284077\n",
       "people      443.586868\n",
       "right       435.860838\n",
       "feminist    409.632239\n",
       "gt          408.652878\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for mensrights\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top ngrams 1-2\n",
    "top_100_fem_words_stop = tfidf_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_words_stop = tfidf_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar words in both\n",
    "fem_men_words = set(top_100_men_words_stop.index) & set(top_100_fem_words_stop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fem_men_words) # 77 of the words are in the top 100 for both! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_men_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Stopwords with Count Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect1 = CountVectorizer(max_features = 30000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to vect\n",
    "fem_vect = vect1.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_vect = vect1.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_vect_df = pd.DataFrame(fem_vect.toarray(), columns=vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_vect_df = pd.DataFrame(men_vect.toarray(), columns=vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workshop     22124\n",
       "manigault    14360\n",
       "paypal       12063\n",
       "doesnt       11036\n",
       "tomboy       10884\n",
       "lawyered     10296\n",
       "fanedit      10242\n",
       "jordon       9659 \n",
       "weak         9085 \n",
       "hardwick     7853 \n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for askfeminists\n",
    "vect_counts_fem = fem_vect_df.sum(axis=0)\n",
    "vect_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "woman       17622\n",
       "men         16400\n",
       "wa          10868\n",
       "just        7402 \n",
       "like        6929 \n",
       "people      6385 \n",
       "don         6367 \n",
       "gt          6236 \n",
       "right       5382 \n",
       "feminist    5265 \n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for mensrights\n",
    "vect_counts_men = men_vect_df.sum(axis=0)\n",
    "vect_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top words\n",
    "top_100_fem_words_stop = vect_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_words_stop = vect_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar words in both\n",
    "fem_men_words = set(top_100_men_words_stop.index) & set(top_100_fem_words_stop.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fem_men_words) # about the same as tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Custom Stopwords, Second Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brian's code, setting stop words equal to intersection terms in top 100 using count vectorizer\n",
    "stop_words_1 = text.ENGLISH_STOP_WORDS.union(['actually',\n",
    " 'agree',\n",
    " 'bad',\n",
    " 'believe',\n",
    " 'better',\n",
    " 'case',\n",
    " 'child',\n",
    " 'come',\n",
    " 'comment',\n",
    " 'did',\n",
    " 'didn',\n",
    " 'doe',\n",
    " 'doesn',\n",
    " 'don',\n",
    " 'equality',\n",
    " 'fact',\n",
    " 'feel',\n",
    " 'female',\n",
    " 'feminism',\n",
    " 'feminist',\n",
    " 'gender',\n",
    " 'girl',\n",
    " 'going',\n",
    " 'good',\n",
    " 'group',\n",
    " 'gt',\n",
    " 'guy',\n",
    " 'ha',\n",
    " 'having',\n",
    " 'help',\n",
    " 'isn',\n",
    " 'issue',\n",
    " 'just',\n",
    " 'know',\n",
    " 'life',\n",
    " 'like',\n",
    " 'look',\n",
    " 'lot',\n",
    " 'make',\n",
    " 'male',\n",
    " 'man',\n",
    " 'mean',\n",
    " 'men',\n",
    " 'need',\n",
    " 'people',\n",
    " 'person',\n",
    " 'point',\n",
    " 'post',\n",
    " 'problem',\n",
    " 'rape',\n",
    " 'read',\n",
    " 'really',\n",
    " 'reason',\n",
    " 'right',\n",
    " 'said',\n",
    " 'say',\n",
    " 'saying',\n",
    " 'sex',\n",
    " 'sexual',\n",
    " 'society',\n",
    " 'sure',\n",
    " 'thanks',\n",
    " 'thing',\n",
    " 'think',\n",
    " 'thought',\n",
    " 'time',\n",
    " 'use',\n",
    " 'victim',\n",
    " 'wa',\n",
    " 'want',\n",
    " 'way',\n",
    " 'white',\n",
    " 'woman',\n",
    " 'word',\n",
    " 'work',\n",
    " 'wrong',\n",
    " 'yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf2 = TfidfVectorizer(max_features = 30000, stop_words = stop_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting to tfidf\n",
    "fem_tfidf = tfidf2.fit_transform(fem['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf = tfidf2.fit_transform(men['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for tfidf words\n",
    "fem_tfidf_df = pd.DataFrame(fem_tfidf.toarray(), columns=tfidf2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "men_tfidf_df = pd.DataFrame(men_tfidf.toarray(), columns=tfidf2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relationshipi    378.489849\n",
       "herald           223.135285\n",
       "dialed           219.112282\n",
       "unoriginal       204.333295\n",
       "anatomist        204.207552\n",
       "partyi           195.403570\n",
       "eradicate        191.807080\n",
       "trustworthy      182.818588\n",
       "unbroken         178.130618\n",
       "applies          177.842142\n",
       "dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for askfeminists, after removing most common words\n",
    "tfidf_counts_fem = fem_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_fem.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year       257.012902\n",
       "day        203.796288\n",
       "archive    202.565864\n",
       "boy        193.050981\n",
       "law        172.890177\n",
       "sub        170.635181\n",
       "article    166.782898\n",
       "got        164.916142\n",
       "shit       163.609884\n",
       "support    161.837291\n",
       "dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top words for mensrights, after removing most common words\n",
    "tfidf_counts_men = men_tfidf_df.sum(axis=0)\n",
    "tfidf_counts_men.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding top words\n",
    "top_100_fem_words_stop1 = tfidf_counts_fem.sort_values(ascending=False)[0:100]\n",
    "top_100_men_words_stop1 = tfidf_counts_men.sort_values(ascending=False)[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding similar words in both\n",
    "fem_men_words1 = set(top_100_men_words_stop1.index) & set(top_100_fem_words_stop1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fem_men_words1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fem_men_words1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Custom Stopwords (2nd layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# added common words from last version to common words from previous version\n",
    "stop_words_2 = text.ENGLISH_STOP_WORDS.union(['agree',\n",
    " 'aren',\n",
    " 'argument',\n",
    " 'article',\n",
    " 'assault',\n",
    " 'best',\n",
    " 'boy',\n",
    " 'care',\n",
    " 'change',\n",
    " 'claim',\n",
    " 'consent',\n",
    " 'crime',\n",
    " 'day',\n",
    " 'different',\n",
    " 'doing',\n",
    " 'equal',\n",
    " 'evidence',\n",
    " 'exactly',\n",
    " 'example',\n",
    " 'far',\n",
    " 'friend',\n",
    " 'getting',\n",
    " 'got',\n",
    " 'hard',\n",
    " 'hate',\n",
    " 'idea',\n",
    " 'job',\n",
    " 'kind',\n",
    " 'law',\n",
    " 'let',\n",
    " 'making',\n",
    " 'masculinity',\n",
    " 'matter',\n",
    " 'maybe',\n",
    " 'movement',\n",
    " 'opinion',\n",
    " 'place',\n",
    " 'power',\n",
    " 'pretty',\n",
    " 'probably',\n",
    " 'question',\n",
    " 'read',\n",
    " 'real',\n",
    " 'seen',\n",
    " 'sound',\n",
    " 'stop',\n",
    " 'study',\n",
    " 'sub',\n",
    " 'talk',\n",
    " 'talking',\n",
    " 'tell',\n",
    " 'thanks',\n",
    " 'toxic',\n",
    " 'true',\n",
    " 'try',\n",
    " 'understand',\n",
    " 'used',\n",
    " 'violence',\n",
    " 'word',\n",
    " 'wouldn',\n",
    " 'yeah',\n",
    " 'yes',\n",
    "'actually',\n",
    " 'agree',\n",
    " 'bad',\n",
    " 'believe',\n",
    " 'better',\n",
    " 'case',\n",
    " 'child',\n",
    " 'come',\n",
    " 'comment',\n",
    " 'did',\n",
    " 'didn',\n",
    " 'doe',\n",
    " 'doesn',\n",
    " 'don',\n",
    " 'equality',\n",
    " 'fact',\n",
    " 'feel',\n",
    " 'female',\n",
    " 'feminism',\n",
    " 'feminist',\n",
    " 'gender',\n",
    " 'girl',\n",
    " 'going',\n",
    " 'good',\n",
    " 'gt',\n",
    " 'guy',\n",
    " 'ha',\n",
    " 'isn',\n",
    " 'issue',\n",
    " 'just',\n",
    " 'know',\n",
    " 'life',\n",
    " 'like',\n",
    " 'look',\n",
    " 'lot',\n",
    " 'make',\n",
    " 'male',\n",
    " 'man',\n",
    " 'mean',\n",
    " 'men',\n",
    " 'nan',\n",
    " 'need',\n",
    " 'people',\n",
    " 'person',\n",
    " 'point',\n",
    " 'post',\n",
    " 'problem',\n",
    " 'rape',\n",
    " 'read',\n",
    " 'really',\n",
    " 'reason',\n",
    " 'right',\n",
    " 'said',\n",
    " 'say',\n",
    " 'saying',\n",
    " 'sex',\n",
    " 'sexual',\n",
    " 'sure',\n",
    " 'thanks',\n",
    " 'thing',\n",
    " 'think',\n",
    " 'time',\n",
    " 'use',\n",
    " 'wa',\n",
    " 'want',\n",
    " 'way',\n",
    " 'white',\n",
    " 'woman',\n",
    " 'word',\n",
    " 'work',\n",
    " 'wrong',\n",
    " 'yes', 'aren',\n",
    " 'argument',\n",
    " 'assault',\n",
    " 'best',\n",
    " 'boy',\n",
    " 'care',\n",
    " 'change',\n",
    " 'claim',\n",
    " 'consent',\n",
    " 'day',\n",
    " 'different',\n",
    " 'doing',\n",
    " 'equal',\n",
    " 'evidence',\n",
    " 'exactly',\n",
    " 'far',\n",
    " 'getting',\n",
    " 'got',\n",
    " 'group',\n",
    " 'hate',\n",
    " 'having',\n",
    " 'help',\n",
    " 'idea',\n",
    " 'job',\n",
    " 'kind',\n",
    " 'law',\n",
    " 'le',\n",
    " 'let',\n",
    " 'making',\n",
    " 'matter',\n",
    " 'maybe',\n",
    " 'movement',\n",
    " 'ok',\n",
    " 'place',\n",
    " 'power',\n",
    " 'pretty',\n",
    " 'probably',\n",
    " 'question',\n",
    " 'real',\n",
    " 'society',\n",
    " 'sound',\n",
    " 'stop',\n",
    " 'sub',\n",
    " 'support',\n",
    " 'talk',\n",
    " 'talking',\n",
    " 'tell',\n",
    " 'thought',\n",
    " 'true',\n",
    " 'try',\n",
    " 'trying',\n",
    " 'understand',\n",
    " 'used',\n",
    " 'victim',\n",
    " 'violence',\n",
    " 'world',\n",
    " 'wouldn',\n",
    " 'yeah',\n",
    " 'year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining into one DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen = pd.concat([fem, men], axis=0, join='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57955, 8)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femvmen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'text', 'type', 'subreddit', 'removed', 'deleted',\n",
       "       'clean_text_stop', 'lems'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femvmen.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen = pd.get_dummies(femvmen, columns=['subreddit'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen.drop(columns = \"Unnamed: 0\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen = pd.DataFrame(femvmen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "femvmen.to_csv('./data/femvmen_lem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'text', 'type', 'removed', 'deleted', 'clean_text_stop',\n",
       "       'lems', 'subreddit_MensRights'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "femvmen.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling <a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Vars/TrainTestSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = femvmen['lems']\n",
    "y = femvmen['subreddit_MensRights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500388\n",
       "0    0.499612\n",
       "Name: subreddit_MensRights, dtype: float64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True) # about 50% mensrights posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True, \n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.500388\n",
       "0    0.499612\n",
       "Name: subreddit_MensRights, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up multiple vectorizers with english stopwords, ngrams 1-4, different features\n",
    "tfidf = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        'english')\n",
    "tfidf2 = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf3 = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        stop_words_2)\n",
    "tfidf4 = TfidfVectorizer(ngram_range = (1, 4), max_features = 100000, stop_words = \n",
    "                        'english')\n",
    "tfidf5 = TfidfVectorizer(ngram_range = (1, 4), max_features = 100000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf6 = TfidfVectorizer(ngram_range = (1, 4), max_features = 100000, stop_words = \n",
    "                        stop_words_2)\n",
    "tfidf7 = TfidfVectorizer(ngram_range = (1, 4), max_features = 50000, stop_words = \n",
    "                        'english')\n",
    "tfidf8 = TfidfVectorizer(ngram_range = (1, 4), max_features = 50000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf9 = TfidfVectorizer(ngram_range = (1, 4), max_features = 50000, stop_words = \n",
    "                        stop_words_2)\n",
    "tfidf10 = TfidfVectorizer(ngram_range = (1, 4), max_features = 5000, stop_words = \n",
    "                        'english')\n",
    "tfidf11 = TfidfVectorizer(ngram_range = (1, 4), max_features = 5000, stop_words = \n",
    "                        stop_words_1)\n",
    "tfidf12 = TfidfVectorizer(ngram_range = (1, 4), max_features = 5000, stop_words = \n",
    "                        stop_words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit transform train sets\n",
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_train_tf2 = tfidf2.fit_transform(X_train)\n",
    "X_train_tf3 = tfidf3.fit_transform(X_train)\n",
    "X_train_tf4 = tfidf4.fit_transform(X_train)\n",
    "X_train_tf5 = tfidf5.fit_transform(X_train)\n",
    "X_train_tf6 = tfidf6.fit_transform(X_train)\n",
    "X_train_tf7 = tfidf7.fit_transform(X_train)\n",
    "X_train_tf8 = tfidf8.fit_transform(X_train)\n",
    "X_train_tf9 = tfidf9.fit_transform(X_train)\n",
    "X_train_tf10 = tfidf10.fit_transform(X_train)\n",
    "X_train_tf11 = tfidf11.fit_transform(X_train)\n",
    "X_train_tf12 = tfidf12.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test set\n",
    "X_test_tf = tfidf.transform(X_test)\n",
    "X_test_tf2 = tfidf2.transform(X_test)\n",
    "X_test_tf3 = tfidf3.transform(X_test)\n",
    "X_test_tf4 = tfidf4.transform(X_test)\n",
    "X_test_tf5 = tfidf5.transform(X_test)\n",
    "X_test_tf6 = tfidf6.transform(X_test)\n",
    "X_test_tf7 = tfidf7.transform(X_test)\n",
    "X_test_tf8 = tfidf8.transform(X_test)\n",
    "X_test_tf9 = tfidf9.transform(X_test)\n",
    "X_test_tf10 = tfidf10.transform(X_test)\n",
    "X_test_tf11 = tfidf11.transform(X_test)\n",
    "X_test_tf12 = tfidf12.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best versions of vectorized data for logistic regression are with stopwords = 'english' and max features = 125000. With very few features (10K or less) the gap between train and test scores closes to 3% but the test score continues to drop with fewer than 100K features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# version 1\n",
    "X_train_tf = tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.853420757484255"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7560175998619618"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8392718488482444"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2\n",
    "logreg.fit(X_train_tf2, y_train)\n",
    "logreg.score(X_train_tf2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7379863687343629"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf2, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8339228711931671"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "logreg.fit(X_train_tf3, y_train)\n",
    "logreg.score(X_train_tf3, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7366059874040204"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf3, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8494737296178069"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4\n",
    "logreg.fit(X_train_tf4, y_train)\n",
    "logreg.score(X_train_tf4, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7559313260288154"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf4, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8349365887326374"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 5\n",
    "logreg.fit(X_train_tf5, y_train)\n",
    "logreg.score(X_train_tf5, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.739711845397291"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf5, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829868001035286"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 6\n",
    "logreg.fit(X_train_tf6, y_train)\n",
    "logreg.score(X_train_tf6, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7355707014062635"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf6, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8363385385212665"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 7\n",
    "logreg.fit(X_train_tf7, y_train)\n",
    "logreg.score(X_train_tf7, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7547234923647658"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf7, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8239582434647571"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 8\n",
    "logreg.fit(X_train_tf8, y_train)\n",
    "logreg.score(X_train_tf8, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.739280476231559"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf8, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8189974980588387"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 9\n",
    "logreg.fit(X_train_tf9, y_train)\n",
    "logreg.score(X_train_tf9, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.736519713570874"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf9, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7811664222241395"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 10\n",
    "logreg.fit(X_train_tf10, y_train)\n",
    "logreg.score(X_train_tf10, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7394530238978518"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf10, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7710292468294366"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 11\n",
    "logreg.fit(X_train_tf11, y_train)\n",
    "logreg.score(X_train_tf11, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7278923302562332"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf11, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7657449745492192"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 12\n",
    "logreg.fit(X_train_tf12, y_train)\n",
    "logreg.score(X_train_tf12, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7192649469415926"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tf12, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Influential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting on best model\n",
    "logreg.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = logreg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(coefficients, columns = tfidf2.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aap</th>\n",
       "      <th>aap circumcision</th>\n",
       "      <th>aauw</th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>abc</th>\n",
       "      <th>abc news</th>\n",
       "      <th>abctv</th>\n",
       "      <th>abctv scottmorrisonmp</th>\n",
       "      <th>abd</th>\n",
       "      <th>abdicate</th>\n",
       "      <th>abdicate responsibility</th>\n",
       "      <th>abdication</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>abducted</th>\n",
       "      <th>abduction</th>\n",
       "      <th>abelist</th>\n",
       "      <th>aberrant</th>\n",
       "      <th>aberration</th>\n",
       "      <th>abhor</th>\n",
       "      <th>abhor culture</th>\n",
       "      <th>abhorrent</th>\n",
       "      <th>abide</th>\n",
       "      <th>abiding</th>\n",
       "      <th>abiding citizen</th>\n",
       "      <th>abigail</th>\n",
       "      <th>ability</th>\n",
       "      <th>ability based</th>\n",
       "      <th>ability birth</th>\n",
       "      <th>ability change</th>\n",
       "      <th>ability choose</th>\n",
       "      <th>ability commit</th>\n",
       "      <th>ability compete</th>\n",
       "      <th>ability consent</th>\n",
       "      <th>ability consent reasonable</th>\n",
       "      <th>ability control</th>\n",
       "      <th>ability create</th>\n",
       "      <th>ability decision</th>\n",
       "      <th>ability dress</th>\n",
       "      <th>ability dress certain</th>\n",
       "      <th>ability earn</th>\n",
       "      <th>ability enjoy</th>\n",
       "      <th>ability experience</th>\n",
       "      <th>ability influence</th>\n",
       "      <th>ability informed</th>\n",
       "      <th>ability job</th>\n",
       "      <th>ability le</th>\n",
       "      <th>ability obtain</th>\n",
       "      <th>ability pay</th>\n",
       "      <th>ability power</th>\n",
       "      <th>ability prevent</th>\n",
       "      <th>ability provide</th>\n",
       "      <th>ability provide care</th>\n",
       "      <th>ability refrain</th>\n",
       "      <th>ability refrain personal</th>\n",
       "      <th>ability refrain personal accountability</th>\n",
       "      <th>ability reproduce</th>\n",
       "      <th>ability responsibility</th>\n",
       "      <th>ability survive</th>\n",
       "      <th>ability understand</th>\n",
       "      <th>ability willingness</th>\n",
       "      <th>abit</th>\n",
       "      <th>abject</th>\n",
       "      <th>able</th>\n",
       "      <th>able abandon</th>\n",
       "      <th>able able</th>\n",
       "      <th>able abortion</th>\n",
       "      <th>able access</th>\n",
       "      <th>able accrue</th>\n",
       "      <th>able accrue wealth</th>\n",
       "      <th>able achieve</th>\n",
       "      <th>able afford</th>\n",
       "      <th>able angry</th>\n",
       "      <th>able answer</th>\n",
       "      <th>able answer question</th>\n",
       "      <th>able argue</th>\n",
       "      <th>able argument</th>\n",
       "      <th>able ask</th>\n",
       "      <th>able away</th>\n",
       "      <th>able bodied</th>\n",
       "      <th>able bodied ci</th>\n",
       "      <th>able bring</th>\n",
       "      <th>able buy</th>\n",
       "      <th>able care</th>\n",
       "      <th>able change</th>\n",
       "      <th>able choose</th>\n",
       "      <th>able claim</th>\n",
       "      <th>able compete</th>\n",
       "      <th>able consent</th>\n",
       "      <th>able consider</th>\n",
       "      <th>able continue</th>\n",
       "      <th>able contribute</th>\n",
       "      <th>able control</th>\n",
       "      <th>able convince</th>\n",
       "      <th>able create</th>\n",
       "      <th>able date</th>\n",
       "      <th>able day</th>\n",
       "      <th>able deal</th>\n",
       "      <th>able decide</th>\n",
       "      <th>able decision</th>\n",
       "      <th>able defend</th>\n",
       "      <th>able discriminate</th>\n",
       "      <th>able discus</th>\n",
       "      <th>able distinguish</th>\n",
       "      <th>able empathize</th>\n",
       "      <th>able evidence</th>\n",
       "      <th>able experience</th>\n",
       "      <th>able explain</th>\n",
       "      <th>able explain biological</th>\n",
       "      <th>able explain biological basis</th>\n",
       "      <th>able express</th>\n",
       "      <th>able fight</th>\n",
       "      <th>able financial</th>\n",
       "      <th>able fix</th>\n",
       "      <th>able force</th>\n",
       "      <th>able forward</th>\n",
       "      <th>able fully</th>\n",
       "      <th>able gain</th>\n",
       "      <th>able handle</th>\n",
       "      <th>able hold</th>\n",
       "      <th>able identify</th>\n",
       "      <th>able incorporate</th>\n",
       "      <th>able job</th>\n",
       "      <th>able learn</th>\n",
       "      <th>able leave</th>\n",
       "      <th>able legally</th>\n",
       "      <th>able lift</th>\n",
       "      <th>able live</th>\n",
       "      <th>able maintain</th>\n",
       "      <th>able manipulate</th>\n",
       "      <th>able meet</th>\n",
       "      <th>able navigate</th>\n",
       "      <th>able opt</th>\n",
       "      <th>able participate</th>\n",
       "      <th>able perform</th>\n",
       "      <th>able pick</th>\n",
       "      <th>able play</th>\n",
       "      <th>able pregnant</th>\n",
       "      <th>able protect</th>\n",
       "      <th>able prove</th>\n",
       "      <th>able provide</th>\n",
       "      <th>able pull</th>\n",
       "      <th>able raise</th>\n",
       "      <th>able recognize</th>\n",
       "      <th>able record</th>\n",
       "      <th>able refute</th>\n",
       "      <th>able relate</th>\n",
       "      <th>able remove</th>\n",
       "      <th>able separate</th>\n",
       "      <th>able separate conversation</th>\n",
       "      <th>able separate conversation fgm</th>\n",
       "      <th>able set</th>\n",
       "      <th>able sexist</th>\n",
       "      <th>able share</th>\n",
       "      <th>able shut</th>\n",
       "      <th>able sign</th>\n",
       "      <th>able solve</th>\n",
       "      <th>able sort</th>\n",
       "      <th>able speak</th>\n",
       "      <th>able speak clearly</th>\n",
       "      <th>able speak clearly unambiguously</th>\n",
       "      <th>able spend</th>\n",
       "      <th>able stand</th>\n",
       "      <th>able start</th>\n",
       "      <th>able stay</th>\n",
       "      <th>able study</th>\n",
       "      <th>able support</th>\n",
       "      <th>able survive</th>\n",
       "      <th>able talk</th>\n",
       "      <th>able tell</th>\n",
       "      <th>able train</th>\n",
       "      <th>able trust</th>\n",
       "      <th>able turn</th>\n",
       "      <th>able understand</th>\n",
       "      <th>able vote</th>\n",
       "      <th>able watch</th>\n",
       "      <th>able wear</th>\n",
       "      <th>abled</th>\n",
       "      <th>ableism</th>\n",
       "      <th>ableist</th>\n",
       "      <th>ableist slur</th>\n",
       "      <th>abnormal</th>\n",
       "      <th>abnormal behavior</th>\n",
       "      <th>abnormality</th>\n",
       "      <th>aboard</th>\n",
       "      <th>abolish</th>\n",
       "      <th>abolish prison</th>\n",
       "      <th>abolished</th>\n",
       "      <th>abolishing</th>\n",
       "      <th>abolition</th>\n",
       "      <th>abolition capitalism</th>\n",
       "      <th>abolitionism</th>\n",
       "      <th>abolitionist</th>\n",
       "      <th>abominable</th>\n",
       "      <th>abomination</th>\n",
       "      <th>aboriginal</th>\n",
       "      <th>abort</th>\n",
       "      <th>abort baby</th>\n",
       "      <th>abort body</th>\n",
       "      <th>abort fetus</th>\n",
       "      <th>abort force</th>\n",
       "      <th>aborted</th>\n",
       "      <th>aborted fetus</th>\n",
       "      <th>aborting</th>\n",
       "      <th>aborting baby</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abortion able</th>\n",
       "      <th>abortion abortion</th>\n",
       "      <th>abortion access</th>\n",
       "      <th>abortion adoption</th>\n",
       "      <th>abortion aren</th>\n",
       "      <th>abortion argument</th>\n",
       "      <th>abortion available</th>\n",
       "      <th>abortion baby</th>\n",
       "      <th>abortion based</th>\n",
       "      <th>abortion bc</th>\n",
       "      <th>abortion begin</th>\n",
       "      <th>abortion birth</th>\n",
       "      <th>abortion birth control</th>\n",
       "      <th>abortion bodily</th>\n",
       "      <th>abortion bodily autonomy</th>\n",
       "      <th>abortion body</th>\n",
       "      <th>abortion body choice</th>\n",
       "      <th>abortion choice</th>\n",
       "      <th>abortion clinic</th>\n",
       "      <th>abortion completely</th>\n",
       "      <th>abortion consent</th>\n",
       "      <th>abortion consider</th>\n",
       "      <th>abortion cost</th>\n",
       "      <th>abortion country</th>\n",
       "      <th>abortion debate</th>\n",
       "      <th>abortion easy</th>\n",
       "      <th>abortion effective</th>\n",
       "      <th>abortion fine</th>\n",
       "      <th>abortion form</th>\n",
       "      <th>abortion free</th>\n",
       "      <th>abortion freely</th>\n",
       "      <th>abortion gendered</th>\n",
       "      <th>abortion generally</th>\n",
       "      <th>abortion happening</th>\n",
       "      <th>abortion illegal</th>\n",
       "      <th>abortion killing</th>\n",
       "      <th>abortion late</th>\n",
       "      <th>abortion law</th>\n",
       "      <th>abortion le</th>\n",
       "      <th>abortion legal</th>\n",
       "      <th>abortion let</th>\n",
       "      <th>abortion literally</th>\n",
       "      <th>abortion making</th>\n",
       "      <th>abortion medical</th>\n",
       "      <th>abortion month</th>\n",
       "      <th>abortion mother</th>\n",
       "      <th>abortion murder</th>\n",
       "      <th>abortion necessarily</th>\n",
       "      <th>abortion okay</th>\n",
       "      <th>abortion option</th>\n",
       "      <th>abortion parent</th>\n",
       "      <th>abortion pay</th>\n",
       "      <th>abortion poor</th>\n",
       "      <th>abortion position</th>\n",
       "      <th>abortion pregnancy</th>\n",
       "      <th>abortion pregnant</th>\n",
       "      <th>abortion pretty</th>\n",
       "      <th>abortion prevent</th>\n",
       "      <th>abortion pro</th>\n",
       "      <th>abortion pro choice</th>\n",
       "      <th>abortion rare</th>\n",
       "      <th>abortion reproductive</th>\n",
       "      <th>abortion service</th>\n",
       "      <th>abortion shouldn</th>\n",
       "      <th>abortion simply</th>\n",
       "      <th>abortion support</th>\n",
       "      <th>abortion topic</th>\n",
       "      <th>abortion unjust</th>\n",
       "      <th>abortion unjust killing</th>\n",
       "      <th>abortion wanted</th>\n",
       "      <th>abortion week</th>\n",
       "      <th>abortion world</th>\n",
       "      <th>abovementioned</th>\n",
       "      <th>abraham</th>\n",
       "      <th>abrahamic</th>\n",
       "      <th>abrahamic religion</th>\n",
       "      <th>abrams</th>\n",
       "      <th>abrasion</th>\n",
       "      <th>abrasive</th>\n",
       "      <th>abridging</th>\n",
       "      <th>abridging freedom</th>\n",
       "      <th>abridging freedom speech</th>\n",
       "      <th>abroad</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absence</th>\n",
       "      <th>absence compelling</th>\n",
       "      <th>absence compelling evidence</th>\n",
       "      <th>absence court</th>\n",
       "      <th>absence court order</th>\n",
       "      <th>absence evidence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absent evidence</th>\n",
       "      <th>absent father</th>\n",
       "      <th>absentee</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolute best</th>\n",
       "      <th>absolute bitch</th>\n",
       "      <th>absolute critical</th>\n",
       "      <th>absolute critical reflection</th>\n",
       "      <th>absolute critical reflection mediated</th>\n",
       "      <th>absolute garbage</th>\n",
       "      <th>absolute hell</th>\n",
       "      <th>absolute hiv</th>\n",
       "      <th>absolute hiv number</th>\n",
       "      <th>absolute hiv number methodological</th>\n",
       "      <th>absolute lack</th>\n",
       "      <th>absolute power</th>\n",
       "      <th>absolute statement</th>\n",
       "      <th>absolute totally</th>\n",
       "      <th>absolute totally free</th>\n",
       "      <th>absolute totally free oppression</th>\n",
       "      <th>absolute truth</th>\n",
       "      <th>absolute worst</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absolutely absurd</th>\n",
       "      <th>absolutely arguing</th>\n",
       "      <th>absolutely awful</th>\n",
       "      <th>absolutely ban</th>\n",
       "      <th>absolutely based</th>\n",
       "      <th>absolutely body</th>\n",
       "      <th>absolutely called</th>\n",
       "      <th>absolutely change</th>\n",
       "      <th>absolutely choice</th>\n",
       "      <th>absolutely choose</th>\n",
       "      <th>absolutely clear</th>\n",
       "      <th>absolutely correct</th>\n",
       "      <th>absolutely deserve</th>\n",
       "      <th>absolutely disgusting</th>\n",
       "      <th>absolutely doubt</th>\n",
       "      <th>absolutely equal</th>\n",
       "      <th>absolutely equal oppression</th>\n",
       "      <th>absolutely equal oppression free</th>\n",
       "      <th>absolutely essential</th>\n",
       "      <th>absolutely evidence</th>\n",
       "      <th>absolutely fine</th>\n",
       "      <th>absolutely fucking</th>\n",
       "      <th>absolutely harassment</th>\n",
       "      <th>absolutely hate</th>\n",
       "      <th>absolutely horrible</th>\n",
       "      <th>absolutely idea</th>\n",
       "      <th>absolutely important</th>\n",
       "      <th>absolutely involves</th>\n",
       "      <th>absolutely love</th>\n",
       "      <th>absolutely matter</th>\n",
       "      <th>absolutely necessary</th>\n",
       "      <th>absolutely option</th>\n",
       "      <th>absolutely possible</th>\n",
       "      <th>absolutely proof</th>\n",
       "      <th>absolutely proof gendered</th>\n",
       "      <th>absolutely proof gendered difference</th>\n",
       "      <th>absolutely question</th>\n",
       "      <th>absolutely relevant</th>\n",
       "      <th>absolutely ridiculous</th>\n",
       "      <th>absolutely sense</th>\n",
       "      <th>absolutely shit</th>\n",
       "      <th>absolutely sign</th>\n",
       "      <th>absolutely stand</th>\n",
       "      <th>absolutely support</th>\n",
       "      <th>absolutely talk</th>\n",
       "      <th>absolutely terrible</th>\n",
       "      <th>absolutely true</th>\n",
       "      <th>absolutely understand</th>\n",
       "      <th>absolutely useless</th>\n",
       "      <th>absolutely whatsoever</th>\n",
       "      <th>absolutely wonderful</th>\n",
       "      <th>absolutely wonderful helping</th>\n",
       "      <th>absolutely wonderful helping primary</th>\n",
       "      <th>absolutely zero</th>\n",
       "      <th>absolution</th>\n",
       "      <th>absolutist</th>\n",
       "      <th>absolutley</th>\n",
       "      <th>absolutly</th>\n",
       "      <th>absolve</th>\n",
       "      <th>absolved</th>\n",
       "      <th>absolving</th>\n",
       "      <th>absorb</th>\n",
       "      <th>absorbed</th>\n",
       "      <th>absorbs</th>\n",
       "      <th>absorption</th>\n",
       "      <th>abstain</th>\n",
       "      <th>abstaining</th>\n",
       "      <th>abstinence</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstract concept</th>\n",
       "      <th>abstract paper</th>\n",
       "      <th>abstraction</th>\n",
       "      <th>abstractly</th>\n",
       "      <th>absurd</th>\n",
       "      <th>absurd claim</th>\n",
       "      <th>absurd statement</th>\n",
       "      <th>absurdity</th>\n",
       "      <th>absurdly</th>\n",
       "      <th>absurdum</th>\n",
       "      <th>abundance</th>\n",
       "      <th>abundant</th>\n",
       "      <th>abundantly</th>\n",
       "      <th>abundantly clear</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abuse abuse</th>\n",
       "      <th>abuse abused</th>\n",
       "      <th>abuse abuser</th>\n",
       "      <th>abuse according</th>\n",
       "      <th>abuse accusation</th>\n",
       "      <th>abuse allegation</th>\n",
       "      <th>abuse amp</th>\n",
       "      <th>abuse assault</th>\n",
       "      <th>abuse authority</th>\n",
       "      <th>abuse based</th>\n",
       "      <th>abuse believed</th>\n",
       "      <th>abuse boy</th>\n",
       "      <th>abuse bullying</th>\n",
       "      <th>abuse called</th>\n",
       "      <th>abuse came</th>\n",
       "      <th>abuse cause</th>\n",
       "      <th>abuse childhood</th>\n",
       "      <th>abuse claim</th>\n",
       "      <th>abuse convicted</th>\n",
       "      <th>abuse country</th>\n",
       "      <th>abuse culture</th>\n",
       "      <th>abuse depression</th>\n",
       "      <th>abuse discrimination</th>\n",
       "      <th>abuse domestic</th>\n",
       "      <th>abuse domestic violence</th>\n",
       "      <th>abuse dr</th>\n",
       "      <th>abuse end</th>\n",
       "      <th>abuse equal</th>\n",
       "      <th>abuse especially</th>\n",
       "      <th>abuse example</th>\n",
       "      <th>abuse exploitation</th>\n",
       "      <th>abuse family</th>\n",
       "      <th>abuse far</th>\n",
       "      <th>abuse form</th>\n",
       "      <th>abuse got</th>\n",
       "      <th>abuse hand</th>\n",
       "      <th>abuse happened</th>\n",
       "      <th>abuse harassment</th>\n",
       "      <th>abuse horrible</th>\n",
       "      <th>abuse husband</th>\n",
       "      <th>abuse inflicted</th>\n",
       "      <th>abuse intimate</th>\n",
       "      <th>abuse intimate partner</th>\n",
       "      <th>abuse kid</th>\n",
       "      <th>abuse kill</th>\n",
       "      <th>abuse kind</th>\n",
       "      <th>abuse law</th>\n",
       "      <th>abuse le</th>\n",
       "      <th>abuse lead</th>\n",
       "      <th>abuse let</th>\n",
       "      <th>abuse likely</th>\n",
       "      <th>abuse little</th>\n",
       "      <th>abuse maybe</th>\n",
       "      <th>abuse mental</th>\n",
       "      <th>abuse mental health</th>\n",
       "      <th>abuse mother</th>\n",
       "      <th>abuse neglect</th>\n",
       "      <th>abuse number</th>\n",
       "      <th>abuse offender</th>\n",
       "      <th>abuse ok</th>\n",
       "      <th>abuse okay</th>\n",
       "      <th>abuse online</th>\n",
       "      <th>abuse parent</th>\n",
       "      <th>abuse parent court</th>\n",
       "      <th>abuse partner</th>\n",
       "      <th>abuse perpetrator</th>\n",
       "      <th>abuse physical</th>\n",
       "      <th>abuse position</th>\n",
       "      <th>abuse power</th>\n",
       "      <th>abuse psychopathic</th>\n",
       "      <th>abuse psychopathic guidance</th>\n",
       "      <th>abuse psychopathic guidance stereotype</th>\n",
       "      <th>abuse question</th>\n",
       "      <th>abuse raped</th>\n",
       "      <th>abuse receive</th>\n",
       "      <th>abuse received</th>\n",
       "      <th>abuse report</th>\n",
       "      <th>abuse seen</th>\n",
       "      <th>abuse sharing</th>\n",
       "      <th>abuse sharing metoo</th>\n",
       "      <th>abuse sharing metoo platform</th>\n",
       "      <th>abuse shelter</th>\n",
       "      <th>abuse situation</th>\n",
       "      <th>abuse social</th>\n",
       "      <th>abuse source</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah relationship</th>\n",
       "      <th>yeah saw</th>\n",
       "      <th>yeah seen</th>\n",
       "      <th>yeah sense</th>\n",
       "      <th>yeah sexist</th>\n",
       "      <th>yeah shit</th>\n",
       "      <th>yeah shouldn</th>\n",
       "      <th>yeah sorry</th>\n",
       "      <th>yeah sort</th>\n",
       "      <th>yeah sound</th>\n",
       "      <th>yeah tell</th>\n",
       "      <th>yeah thats</th>\n",
       "      <th>yeah thinking</th>\n",
       "      <th>yeah totally</th>\n",
       "      <th>yeah toxic</th>\n",
       "      <th>yeah trans</th>\n",
       "      <th>yeah transphobia</th>\n",
       "      <th>yeah true</th>\n",
       "      <th>yeah understand</th>\n",
       "      <th>yeah used</th>\n",
       "      <th>yeah usually</th>\n",
       "      <th>yeah went</th>\n",
       "      <th>yeah won</th>\n",
       "      <th>yeah yeah</th>\n",
       "      <th>yeah year</th>\n",
       "      <th>year</th>\n",
       "      <th>year abuse</th>\n",
       "      <th>year age</th>\n",
       "      <th>year ago</th>\n",
       "      <th>year ago considered</th>\n",
       "      <th>year ago culture</th>\n",
       "      <th>year ago difference</th>\n",
       "      <th>year ago got</th>\n",
       "      <th>year ago maybe</th>\n",
       "      <th>year ago pay</th>\n",
       "      <th>year ago refused</th>\n",
       "      <th>year ago refused appear</th>\n",
       "      <th>year ago student</th>\n",
       "      <th>year ago talking</th>\n",
       "      <th>year ago today</th>\n",
       "      <th>year ago year</th>\n",
       "      <th>year ago year old</th>\n",
       "      <th>year amp</th>\n",
       "      <th>year apart</th>\n",
       "      <th>year assault</th>\n",
       "      <th>year average</th>\n",
       "      <th>year away</th>\n",
       "      <th>year baby</th>\n",
       "      <th>year bar</th>\n",
       "      <th>year based</th>\n",
       "      <th>year basically</th>\n",
       "      <th>year black</th>\n",
       "      <th>year book</th>\n",
       "      <th>year called</th>\n",
       "      <th>year care</th>\n",
       "      <th>year career</th>\n",
       "      <th>year change</th>\n",
       "      <th>year changed</th>\n",
       "      <th>year clean</th>\n",
       "      <th>year close</th>\n",
       "      <th>year college</th>\n",
       "      <th>year compared</th>\n",
       "      <th>year considering</th>\n",
       "      <th>year control</th>\n",
       "      <th>year control ex</th>\n",
       "      <th>year couldn</th>\n",
       "      <th>year count</th>\n",
       "      <th>year country</th>\n",
       "      <th>year crime</th>\n",
       "      <th>year culture</th>\n",
       "      <th>year culture cool</th>\n",
       "      <th>year current</th>\n",
       "      <th>year date</th>\n",
       "      <th>year dating</th>\n",
       "      <th>year day</th>\n",
       "      <th>year decade</th>\n",
       "      <th>year degree</th>\n",
       "      <th>year difference</th>\n",
       "      <th>year difficult</th>\n",
       "      <th>year doctrine</th>\n",
       "      <th>year doing</th>\n",
       "      <th>year earlier</th>\n",
       "      <th>year early</th>\n",
       "      <th>year education</th>\n",
       "      <th>year end</th>\n",
       "      <th>year especially</th>\n",
       "      <th>year eventually</th>\n",
       "      <th>year eventually got</th>\n",
       "      <th>year example</th>\n",
       "      <th>year expect</th>\n",
       "      <th>year experience</th>\n",
       "      <th>year failed</th>\n",
       "      <th>year fair</th>\n",
       "      <th>year fall</th>\n",
       "      <th>year false</th>\n",
       "      <th>year field</th>\n",
       "      <th>year fighting</th>\n",
       "      <th>year finally</th>\n",
       "      <th>year fine</th>\n",
       "      <th>year free</th>\n",
       "      <th>year friend</th>\n",
       "      <th>year fuck</th>\n",
       "      <th>year fucking</th>\n",
       "      <th>year future</th>\n",
       "      <th>year general</th>\n",
       "      <th>year genuinely</th>\n",
       "      <th>year getting</th>\n",
       "      <th>year got</th>\n",
       "      <th>year graduated</th>\n",
       "      <th>year great</th>\n",
       "      <th>year grew</th>\n",
       "      <th>year guilty</th>\n",
       "      <th>year half</th>\n",
       "      <th>year happened</th>\n",
       "      <th>year harassment</th>\n",
       "      <th>year hard</th>\n",
       "      <th>year haven</th>\n",
       "      <th>year high</th>\n",
       "      <th>year high school</th>\n",
       "      <th>year hit</th>\n",
       "      <th>year home</th>\n",
       "      <th>year hrt</th>\n",
       "      <th>year huge</th>\n",
       "      <th>year human</th>\n",
       "      <th>year husband</th>\n",
       "      <th>year inappropriate</th>\n",
       "      <th>year income</th>\n",
       "      <th>year instead</th>\n",
       "      <th>year international</th>\n",
       "      <th>year jail</th>\n",
       "      <th>year job</th>\n",
       "      <th>year judge</th>\n",
       "      <th>year kid</th>\n",
       "      <th>year killing</th>\n",
       "      <th>year later</th>\n",
       "      <th>year law</th>\n",
       "      <th>year le</th>\n",
       "      <th>year left</th>\n",
       "      <th>year let</th>\n",
       "      <th>year likely</th>\n",
       "      <th>year line</th>\n",
       "      <th>year live</th>\n",
       "      <th>year lived</th>\n",
       "      <th>year living</th>\n",
       "      <th>year long</th>\n",
       "      <th>year longer</th>\n",
       "      <th>year looked</th>\n",
       "      <th>year lost</th>\n",
       "      <th>year love</th>\n",
       "      <th>year marriage</th>\n",
       "      <th>year married</th>\n",
       "      <th>year massive</th>\n",
       "      <th>year max</th>\n",
       "      <th>year maximum</th>\n",
       "      <th>year maybe</th>\n",
       "      <th>year medical</th>\n",
       "      <th>year met</th>\n",
       "      <th>year metoo</th>\n",
       "      <th>year middle</th>\n",
       "      <th>year million</th>\n",
       "      <th>year minimum</th>\n",
       "      <th>year miserable</th>\n",
       "      <th>year mod</th>\n",
       "      <th>year month</th>\n",
       "      <th>year mrm</th>\n",
       "      <th>year new</th>\n",
       "      <th>year noticed</th>\n",
       "      <th>year number</th>\n",
       "      <th>year obviously</th>\n",
       "      <th>year old</th>\n",
       "      <th>year old able</th>\n",
       "      <th>year old attractive</th>\n",
       "      <th>year old baby</th>\n",
       "      <th>year old black</th>\n",
       "      <th>year old boy</th>\n",
       "      <th>year old chose</th>\n",
       "      <th>year old consent</th>\n",
       "      <th>year old considered</th>\n",
       "      <th>year old dating</th>\n",
       "      <th>year old daughter</th>\n",
       "      <th>year old florida</th>\n",
       "      <th>year old friend</th>\n",
       "      <th>year old getting</th>\n",
       "      <th>year old got</th>\n",
       "      <th>year old kid</th>\n",
       "      <th>year old le</th>\n",
       "      <th>year old live</th>\n",
       "      <th>year old living</th>\n",
       "      <th>year old love</th>\n",
       "      <th>year old mature</th>\n",
       "      <th>year old model</th>\n",
       "      <th>year old mom</th>\n",
       "      <th>year old nephew</th>\n",
       "      <th>year old pregnant</th>\n",
       "      <th>year old probably</th>\n",
       "      <th>year old raped</th>\n",
       "      <th>year old single</th>\n",
       "      <th>year old sister</th>\n",
       "      <th>year old son</th>\n",
       "      <th>year old started</th>\n",
       "      <th>year old student</th>\n",
       "      <th>year old teenager</th>\n",
       "      <th>year old told</th>\n",
       "      <th>year old used</th>\n",
       "      <th>year old working</th>\n",
       "      <th>year old year</th>\n",
       "      <th>year old year old</th>\n",
       "      <th>year older</th>\n",
       "      <th>year oppression</th>\n",
       "      <th>year order</th>\n",
       "      <th>year paid</th>\n",
       "      <th>year parent</th>\n",
       "      <th>year particularly</th>\n",
       "      <th>year passed</th>\n",
       "      <th>year past</th>\n",
       "      <th>year pay</th>\n",
       "      <th>year period</th>\n",
       "      <th>year pretty</th>\n",
       "      <th>year prison</th>\n",
       "      <th>year prison fine</th>\n",
       "      <th>year probably</th>\n",
       "      <th>year probation</th>\n",
       "      <th>year quite</th>\n",
       "      <th>year reach</th>\n",
       "      <th>year real</th>\n",
       "      <th>year realize</th>\n",
       "      <th>year receive</th>\n",
       "      <th>year received</th>\n",
       "      <th>year recently</th>\n",
       "      <th>year regarding</th>\n",
       "      <th>year relationship</th>\n",
       "      <th>year released</th>\n",
       "      <th>year remember</th>\n",
       "      <th>year retirement</th>\n",
       "      <th>year retirement difference</th>\n",
       "      <th>year retirement difference retirement</th>\n",
       "      <th>year retirement expectancy</th>\n",
       "      <th>year retirement expectancy pension</th>\n",
       "      <th>year road</th>\n",
       "      <th>year rodger</th>\n",
       "      <th>year row</th>\n",
       "      <th>year saving</th>\n",
       "      <th>year school</th>\n",
       "      <th>year second</th>\n",
       "      <th>year seen</th>\n",
       "      <th>year self</th>\n",
       "      <th>year senior</th>\n",
       "      <th>year sentence</th>\n",
       "      <th>year social</th>\n",
       "      <th>year specifically</th>\n",
       "      <th>year spend</th>\n",
       "      <th>year spent</th>\n",
       "      <th>year splc</th>\n",
       "      <th>year stand</th>\n",
       "      <th>year start</th>\n",
       "      <th>year start coming</th>\n",
       "      <th>year start coming stop</th>\n",
       "      <th>year started</th>\n",
       "      <th>year state</th>\n",
       "      <th>year step</th>\n",
       "      <th>year straight</th>\n",
       "      <th>year student</th>\n",
       "      <th>year study</th>\n",
       "      <th>year suck</th>\n",
       "      <th>year support</th>\n",
       "      <th>year taken</th>\n",
       "      <th>year taking</th>\n",
       "      <th>year talk</th>\n",
       "      <th>year talking</th>\n",
       "      <th>year teacher</th>\n",
       "      <th>year tell</th>\n",
       "      <th>year telling</th>\n",
       "      <th>year thinking</th>\n",
       "      <th>year thousand</th>\n",
       "      <th>year took</th>\n",
       "      <th>year training</th>\n",
       "      <th>year treatment</th>\n",
       "      <th>year tried</th>\n",
       "      <th>year true</th>\n",
       "      <th>year trying</th>\n",
       "      <th>year undergrad</th>\n",
       "      <th>year understand</th>\n",
       "      <th>year united</th>\n",
       "      <th>year university</th>\n",
       "      <th>year unless</th>\n",
       "      <th>year used</th>\n",
       "      <th>year using</th>\n",
       "      <th>year usually</th>\n",
       "      <th>year waiting</th>\n",
       "      <th>year wanted</th>\n",
       "      <th>year went</th>\n",
       "      <th>year wife</th>\n",
       "      <th>year won</th>\n",
       "      <th>year worked</th>\n",
       "      <th>year working</th>\n",
       "      <th>year world</th>\n",
       "      <th>year wouldn</th>\n",
       "      <th>year year</th>\n",
       "      <th>year year ago</th>\n",
       "      <th>year year old</th>\n",
       "      <th>year young</th>\n",
       "      <th>year younger</th>\n",
       "      <th>year zero</th>\n",
       "      <th>yearly</th>\n",
       "      <th>yearn</th>\n",
       "      <th>yearning</th>\n",
       "      <th>yeast</th>\n",
       "      <th>yeast infection</th>\n",
       "      <th>yee</th>\n",
       "      <th>yee pedophile</th>\n",
       "      <th>yeh</th>\n",
       "      <th>yell</th>\n",
       "      <th>yelled</th>\n",
       "      <th>yelling</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yellow red</th>\n",
       "      <th>yen</th>\n",
       "      <th>yep</th>\n",
       "      <th>yep absolutely</th>\n",
       "      <th>yep concept</th>\n",
       "      <th>yep concept little</th>\n",
       "      <th>yep concept little fuel</th>\n",
       "      <th>yep toxic</th>\n",
       "      <th>yer</th>\n",
       "      <th>yesallmen</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yesterday evening</th>\n",
       "      <th>yesterday posted</th>\n",
       "      <th>yesterday revealed</th>\n",
       "      <th>yi</th>\n",
       "      <th>yiannopoulis</th>\n",
       "      <th>yiannopoulos</th>\n",
       "      <th>yield</th>\n",
       "      <th>yielded</th>\n",
       "      <th>yikes</th>\n",
       "      <th>yin</th>\n",
       "      <th>yin yang</th>\n",
       "      <th>yinz</th>\n",
       "      <th>yo</th>\n",
       "      <th>yo kid</th>\n",
       "      <th>yoffe</th>\n",
       "      <th>yoga</th>\n",
       "      <th>yoga pant</th>\n",
       "      <th>yoke</th>\n",
       "      <th>york</th>\n",
       "      <th>york city</th>\n",
       "      <th>york city called</th>\n",
       "      <th>york city called wing</th>\n",
       "      <th>york city washington</th>\n",
       "      <th>york city washington dc</th>\n",
       "      <th>york state</th>\n",
       "      <th>york state excluded</th>\n",
       "      <th>york state excluded attending</th>\n",
       "      <th>york state law</th>\n",
       "      <th>york university</th>\n",
       "      <th>yorker</th>\n",
       "      <th>yorkshire</th>\n",
       "      <th>youd</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>young absolutely</th>\n",
       "      <th>young adult</th>\n",
       "      <th>young age</th>\n",
       "      <th>young american</th>\n",
       "      <th>young angry</th>\n",
       "      <th>young asian</th>\n",
       "      <th>young beautiful</th>\n",
       "      <th>young black</th>\n",
       "      <th>young boy</th>\n",
       "      <th>young brother</th>\n",
       "      <th>young caught</th>\n",
       "      <th>young childless</th>\n",
       "      <th>young coming</th>\n",
       "      <th>young consent</th>\n",
       "      <th>young conservative</th>\n",
       "      <th>young couple</th>\n",
       "      <th>young daughter</th>\n",
       "      <th>young day</th>\n",
       "      <th>young desperate</th>\n",
       "      <th>young face</th>\n",
       "      <th>young fertile</th>\n",
       "      <th>young fertile attractive</th>\n",
       "      <th>young fertile attractive countless</th>\n",
       "      <th>young fertile attractive period</th>\n",
       "      <th>young got</th>\n",
       "      <th>young hate</th>\n",
       "      <th>young hot</th>\n",
       "      <th>young human</th>\n",
       "      <th>young infant</th>\n",
       "      <th>young kid</th>\n",
       "      <th>young lady</th>\n",
       "      <th>young learn</th>\n",
       "      <th>young living</th>\n",
       "      <th>young looking</th>\n",
       "      <th>young mind</th>\n",
       "      <th>young murdered</th>\n",
       "      <th>young old</th>\n",
       "      <th>young remember</th>\n",
       "      <th>young son</th>\n",
       "      <th>young sort</th>\n",
       "      <th>young straight</th>\n",
       "      <th>young student</th>\n",
       "      <th>young teen</th>\n",
       "      <th>young teen boy</th>\n",
       "      <th>young teenager</th>\n",
       "      <th>young today</th>\n",
       "      <th>young told</th>\n",
       "      <th>young used</th>\n",
       "      <th>young violence</th>\n",
       "      <th>young world</th>\n",
       "      <th>young year</th>\n",
       "      <th>young year old</th>\n",
       "      <th>young young</th>\n",
       "      <th>younger</th>\n",
       "      <th>younger boy</th>\n",
       "      <th>younger brother</th>\n",
       "      <th>younger doing</th>\n",
       "      <th>younger generation</th>\n",
       "      <th>younger older</th>\n",
       "      <th>younger older younger</th>\n",
       "      <th>younger sister</th>\n",
       "      <th>younger student</th>\n",
       "      <th>younger year</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youngster</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth age</th>\n",
       "      <th>youth suicide</th>\n",
       "      <th>youtoo</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtube channel</th>\n",
       "      <th>youtube channel called</th>\n",
       "      <th>youtube spotlight</th>\n",
       "      <th>youtube video</th>\n",
       "      <th>youtube viewer</th>\n",
       "      <th>youtuber</th>\n",
       "      <th>youtubers</th>\n",
       "      <th>youtubes</th>\n",
       "      <th>youve</th>\n",
       "      <th>youve got</th>\n",
       "      <th>yovino</th>\n",
       "      <th>yoy</th>\n",
       "      <th>ypt</th>\n",
       "      <th>ypu</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr old</th>\n",
       "      <th>yt</th>\n",
       "      <th>yuck</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "      <th>zambia</th>\n",
       "      <th>zarya</th>\n",
       "      <th>ze</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zealand report</th>\n",
       "      <th>zealand report human</th>\n",
       "      <th>zealand support</th>\n",
       "      <th>zealand support submission</th>\n",
       "      <th>zealot</th>\n",
       "      <th>zealous</th>\n",
       "      <th>zeitgeist</th>\n",
       "      <th>zelda</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero actual</th>\n",
       "      <th>zero dawn</th>\n",
       "      <th>zero dollar</th>\n",
       "      <th>zero effect</th>\n",
       "      <th>zero evidence</th>\n",
       "      <th>zero experience</th>\n",
       "      <th>zero personal</th>\n",
       "      <th>zero personal experience</th>\n",
       "      <th>zero personal experience oppression</th>\n",
       "      <th>zero precedent</th>\n",
       "      <th>zero proof</th>\n",
       "      <th>zero responsibility</th>\n",
       "      <th>zero sense</th>\n",
       "      <th>zero sum</th>\n",
       "      <th>zero sum game</th>\n",
       "      <th>zero sum situation</th>\n",
       "      <th>zero sympathy</th>\n",
       "      <th>zero tolerance</th>\n",
       "      <th>zero tolerance policy</th>\n",
       "      <th>zero value</th>\n",
       "      <th>zeroing</th>\n",
       "      <th>zimbardo</th>\n",
       "      <th>zip</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zizek</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zoe quinn</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombie apocalypse</th>\n",
       "      <th>zone</th>\n",
       "      <th>zone problematic</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zygote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.003033</td>\n",
       "      <td>-0.243533</td>\n",
       "      <td>0.584949</td>\n",
       "      <td>0.027575</td>\n",
       "      <td>-0.031888</td>\n",
       "      <td>0.447751</td>\n",
       "      <td>0.143809</td>\n",
       "      <td>-0.002159</td>\n",
       "      <td>-0.351781</td>\n",
       "      <td>0.68035</td>\n",
       "      <td>0.244434</td>\n",
       "      <td>-0.271246</td>\n",
       "      <td>0.087812</td>\n",
       "      <td>-0.006912</td>\n",
       "      <td>0.092327</td>\n",
       "      <td>0.035839</td>\n",
       "      <td>0.339848</td>\n",
       "      <td>0.067532</td>\n",
       "      <td>0.03482</td>\n",
       "      <td>0.03482</td>\n",
       "      <td>-0.117718</td>\n",
       "      <td>0.142861</td>\n",
       "      <td>0.142861</td>\n",
       "      <td>-0.078466</td>\n",
       "      <td>-0.059329</td>\n",
       "      <td>-0.037682</td>\n",
       "      <td>0.090871</td>\n",
       "      <td>0.002733</td>\n",
       "      <td>-0.013394</td>\n",
       "      <td>-0.015316</td>\n",
       "      <td>0.038021</td>\n",
       "      <td>0.098304</td>\n",
       "      <td>0.019924</td>\n",
       "      <td>-0.199023</td>\n",
       "      <td>0.149842</td>\n",
       "      <td>0.148714</td>\n",
       "      <td>0.005406</td>\n",
       "      <td>-0.002942</td>\n",
       "      <td>-1.423937</td>\n",
       "      <td>-0.097382</td>\n",
       "      <td>-0.087434</td>\n",
       "      <td>-0.006748</td>\n",
       "      <td>-0.111227</td>\n",
       "      <td>-0.08009</td>\n",
       "      <td>-0.016359</td>\n",
       "      <td>0.05178</td>\n",
       "      <td>-0.220896</td>\n",
       "      <td>-0.010088</td>\n",
       "      <td>-0.058558</td>\n",
       "      <td>0.01246</td>\n",
       "      <td>-0.016994</td>\n",
       "      <td>-0.015997</td>\n",
       "      <td>-0.11345</td>\n",
       "      <td>-0.11345</td>\n",
       "      <td>-0.11345</td>\n",
       "      <td>0.098854</td>\n",
       "      <td>-0.005324</td>\n",
       "      <td>-0.042037</td>\n",
       "      <td>-0.054979</td>\n",
       "      <td>0.021139</td>\n",
       "      <td>0.194201</td>\n",
       "      <td>0.424501</td>\n",
       "      <td>-0.08296</td>\n",
       "      <td>0.044902</td>\n",
       "      <td>0.081207</td>\n",
       "      <td>-0.119315</td>\n",
       "      <td>-0.030559</td>\n",
       "      <td>0.028345</td>\n",
       "      <td>0.139491</td>\n",
       "      <td>0.170983</td>\n",
       "      <td>0.170983</td>\n",
       "      <td>0.170983</td>\n",
       "      <td>0.040814</td>\n",
       "      <td>-0.114449</td>\n",
       "      <td>0.033531</td>\n",
       "      <td>-0.131436</td>\n",
       "      <td>-0.107079</td>\n",
       "      <td>0.150548</td>\n",
       "      <td>0.100568</td>\n",
       "      <td>0.074039</td>\n",
       "      <td>-0.039929</td>\n",
       "      <td>0.105973</td>\n",
       "      <td>0.039938</td>\n",
       "      <td>-1.397111</td>\n",
       "      <td>-0.161323</td>\n",
       "      <td>-0.413053</td>\n",
       "      <td>-0.077746</td>\n",
       "      <td>-0.237168</td>\n",
       "      <td>-0.237168</td>\n",
       "      <td>-0.066151</td>\n",
       "      <td>0.21879</td>\n",
       "      <td>0.175632</td>\n",
       "      <td>-0.326183</td>\n",
       "      <td>-0.040549</td>\n",
       "      <td>0.037573</td>\n",
       "      <td>-0.002807</td>\n",
       "      <td>-0.031812</td>\n",
       "      <td>-0.045277</td>\n",
       "      <td>-0.218265</td>\n",
       "      <td>0.109496</td>\n",
       "      <td>-0.01798</td>\n",
       "      <td>0.009982</td>\n",
       "      <td>0.00401</td>\n",
       "      <td>0.034322</td>\n",
       "      <td>-0.049388</td>\n",
       "      <td>-0.061987</td>\n",
       "      <td>0.199782</td>\n",
       "      <td>-0.275799</td>\n",
       "      <td>-0.060911</td>\n",
       "      <td>0.197067</td>\n",
       "      <td>-0.138953</td>\n",
       "      <td>-0.095473</td>\n",
       "      <td>-0.027915</td>\n",
       "      <td>0.110358</td>\n",
       "      <td>0.10363</td>\n",
       "      <td>0.0397</td>\n",
       "      <td>0.050348</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>0.112641</td>\n",
       "      <td>0.08501</td>\n",
       "      <td>-0.021393</td>\n",
       "      <td>0.195895</td>\n",
       "      <td>-0.012996</td>\n",
       "      <td>-0.236062</td>\n",
       "      <td>-0.029908</td>\n",
       "      <td>-0.060161</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.030472</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.007121</td>\n",
       "      <td>-0.056807</td>\n",
       "      <td>-0.056807</td>\n",
       "      <td>0.259255</td>\n",
       "      <td>-0.010853</td>\n",
       "      <td>0.08631</td>\n",
       "      <td>-0.107596</td>\n",
       "      <td>-0.00326</td>\n",
       "      <td>0.026905</td>\n",
       "      <td>-0.030063</td>\n",
       "      <td>-0.011004</td>\n",
       "      <td>-0.156547</td>\n",
       "      <td>-0.051091</td>\n",
       "      <td>-0.131074</td>\n",
       "      <td>-0.034017</td>\n",
       "      <td>0.012918</td>\n",
       "      <td>0.224952</td>\n",
       "      <td>-0.030311</td>\n",
       "      <td>0.101266</td>\n",
       "      <td>0.033531</td>\n",
       "      <td>-0.035723</td>\n",
       "      <td>-0.025325</td>\n",
       "      <td>0.02585</td>\n",
       "      <td>0.102706</td>\n",
       "      <td>-0.009168</td>\n",
       "      <td>0.142517</td>\n",
       "      <td>-0.093112</td>\n",
       "      <td>-0.158737</td>\n",
       "      <td>-0.179177</td>\n",
       "      <td>-0.051309</td>\n",
       "      <td>-0.103808</td>\n",
       "      <td>-0.155513</td>\n",
       "      <td>0.256864</td>\n",
       "      <td>0.175216</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>0.125598</td>\n",
       "      <td>-0.125761</td>\n",
       "      <td>-0.10457</td>\n",
       "      <td>0.034271</td>\n",
       "      <td>-0.113076</td>\n",
       "      <td>0.008346</td>\n",
       "      <td>-0.065847</td>\n",
       "      <td>-0.085608</td>\n",
       "      <td>-0.149411</td>\n",
       "      <td>-0.122514</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>-0.135367</td>\n",
       "      <td>-0.340193</td>\n",
       "      <td>-0.22684</td>\n",
       "      <td>-0.101936</td>\n",
       "      <td>-0.101936</td>\n",
       "      <td>-0.101936</td>\n",
       "      <td>-0.001051</td>\n",
       "      <td>0.011018</td>\n",
       "      <td>-0.039199</td>\n",
       "      <td>-0.02846</td>\n",
       "      <td>0.387025</td>\n",
       "      <td>-0.107016</td>\n",
       "      <td>-0.054314</td>\n",
       "      <td>-0.054314</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.144977</td>\n",
       "      <td>-0.211097</td>\n",
       "      <td>-0.018556</td>\n",
       "      <td>0.009354</td>\n",
       "      <td>-0.095282</td>\n",
       "      <td>-0.186047</td>\n",
       "      <td>-0.524099</td>\n",
       "      <td>0.015689</td>\n",
       "      <td>-0.008753</td>\n",
       "      <td>-0.167259</td>\n",
       "      <td>0.144716</td>\n",
       "      <td>-0.272834</td>\n",
       "      <td>-0.087354</td>\n",
       "      <td>0.212212</td>\n",
       "      <td>0.004133</td>\n",
       "      <td>0.193009</td>\n",
       "      <td>0.058722</td>\n",
       "      <td>0.094241</td>\n",
       "      <td>0.120052</td>\n",
       "      <td>-0.277535</td>\n",
       "      <td>-0.655305</td>\n",
       "      <td>-0.465653</td>\n",
       "      <td>0.273255</td>\n",
       "      <td>0.010718</td>\n",
       "      <td>0.458838</td>\n",
       "      <td>-0.065199</td>\n",
       "      <td>-0.627907</td>\n",
       "      <td>-0.496817</td>\n",
       "      <td>-0.520416</td>\n",
       "      <td>0.11938</td>\n",
       "      <td>-0.298573</td>\n",
       "      <td>-0.137042</td>\n",
       "      <td>-0.09393</td>\n",
       "      <td>-0.166976</td>\n",
       "      <td>0.052455</td>\n",
       "      <td>0.085735</td>\n",
       "      <td>0.170614</td>\n",
       "      <td>0.261225</td>\n",
       "      <td>-0.269783</td>\n",
       "      <td>-0.054826</td>\n",
       "      <td>-0.121761</td>\n",
       "      <td>0.123499</td>\n",
       "      <td>-1.479742</td>\n",
       "      <td>0.099794</td>\n",
       "      <td>0.263282</td>\n",
       "      <td>0.244188</td>\n",
       "      <td>0.071321</td>\n",
       "      <td>-0.214189</td>\n",
       "      <td>-0.146056</td>\n",
       "      <td>-0.023488</td>\n",
       "      <td>-0.054664</td>\n",
       "      <td>0.024207</td>\n",
       "      <td>-0.18809</td>\n",
       "      <td>-0.093573</td>\n",
       "      <td>-0.04174</td>\n",
       "      <td>-0.155959</td>\n",
       "      <td>-0.143353</td>\n",
       "      <td>0.090762</td>\n",
       "      <td>-0.019061</td>\n",
       "      <td>-0.057622</td>\n",
       "      <td>0.166257</td>\n",
       "      <td>0.408203</td>\n",
       "      <td>0.020433</td>\n",
       "      <td>-0.15814</td>\n",
       "      <td>0.072865</td>\n",
       "      <td>0.470934</td>\n",
       "      <td>-0.166658</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.49776</td>\n",
       "      <td>0.10731</td>\n",
       "      <td>-0.266527</td>\n",
       "      <td>-0.399055</td>\n",
       "      <td>-0.097801</td>\n",
       "      <td>-0.081926</td>\n",
       "      <td>0.150281</td>\n",
       "      <td>-0.069697</td>\n",
       "      <td>0.138582</td>\n",
       "      <td>0.104782</td>\n",
       "      <td>0.120072</td>\n",
       "      <td>0.16378</td>\n",
       "      <td>-0.022614</td>\n",
       "      <td>-0.110706</td>\n",
       "      <td>-0.136473</td>\n",
       "      <td>-0.138934</td>\n",
       "      <td>-0.044775</td>\n",
       "      <td>0.099999</td>\n",
       "      <td>0.11268</td>\n",
       "      <td>-0.158365</td>\n",
       "      <td>-0.061198</td>\n",
       "      <td>-0.077319</td>\n",
       "      <td>-0.077319</td>\n",
       "      <td>0.06619</td>\n",
       "      <td>-0.023671</td>\n",
       "      <td>-0.124976</td>\n",
       "      <td>-0.138493</td>\n",
       "      <td>-0.090144</td>\n",
       "      <td>-0.108994</td>\n",
       "      <td>-0.003449</td>\n",
       "      <td>0.093834</td>\n",
       "      <td>-0.284981</td>\n",
       "      <td>-0.031728</td>\n",
       "      <td>0.136778</td>\n",
       "      <td>-0.230838</td>\n",
       "      <td>-0.229084</td>\n",
       "      <td>0.037179</td>\n",
       "      <td>0.091815</td>\n",
       "      <td>-0.210392</td>\n",
       "      <td>-0.227497</td>\n",
       "      <td>-0.227497</td>\n",
       "      <td>-0.227497</td>\n",
       "      <td>0.094825</td>\n",
       "      <td>0.043382</td>\n",
       "      <td>-0.536455</td>\n",
       "      <td>-0.043676</td>\n",
       "      <td>-0.043676</td>\n",
       "      <td>0.076108</td>\n",
       "      <td>0.076108</td>\n",
       "      <td>-0.559523</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.105596</td>\n",
       "      <td>0.10605</td>\n",
       "      <td>-0.118325</td>\n",
       "      <td>0.222178</td>\n",
       "      <td>0.998195</td>\n",
       "      <td>0.031494</td>\n",
       "      <td>0.082953</td>\n",
       "      <td>-0.162383</td>\n",
       "      <td>-0.039299</td>\n",
       "      <td>-0.039299</td>\n",
       "      <td>-0.060426</td>\n",
       "      <td>-0.01783</td>\n",
       "      <td>-0.01783</td>\n",
       "      <td>0.137868</td>\n",
       "      <td>-0.006951</td>\n",
       "      <td>0.039127</td>\n",
       "      <td>0.039127</td>\n",
       "      <td>0.039127</td>\n",
       "      <td>0.016847</td>\n",
       "      <td>0.19813</td>\n",
       "      <td>-0.002727</td>\n",
       "      <td>-0.134241</td>\n",
       "      <td>-0.124115</td>\n",
       "      <td>-0.234706</td>\n",
       "      <td>0.101434</td>\n",
       "      <td>-1.679096</td>\n",
       "      <td>-0.116716</td>\n",
       "      <td>0.267626</td>\n",
       "      <td>-0.068443</td>\n",
       "      <td>-0.123776</td>\n",
       "      <td>-0.022759</td>\n",
       "      <td>-0.005149</td>\n",
       "      <td>-0.068109</td>\n",
       "      <td>0.160371</td>\n",
       "      <td>0.392655</td>\n",
       "      <td>0.464881</td>\n",
       "      <td>-0.104759</td>\n",
       "      <td>-0.132916</td>\n",
       "      <td>-0.094957</td>\n",
       "      <td>-0.098495</td>\n",
       "      <td>-0.026177</td>\n",
       "      <td>-0.026177</td>\n",
       "      <td>-0.026177</td>\n",
       "      <td>0.039833</td>\n",
       "      <td>-0.144296</td>\n",
       "      <td>0.10202</td>\n",
       "      <td>-0.046132</td>\n",
       "      <td>-0.159564</td>\n",
       "      <td>0.084196</td>\n",
       "      <td>0.012997</td>\n",
       "      <td>0.161981</td>\n",
       "      <td>-0.153378</td>\n",
       "      <td>-0.031059</td>\n",
       "      <td>-0.072652</td>\n",
       "      <td>0.092178</td>\n",
       "      <td>0.063612</td>\n",
       "      <td>-0.029394</td>\n",
       "      <td>-0.029394</td>\n",
       "      <td>0.044419</td>\n",
       "      <td>-0.015316</td>\n",
       "      <td>-0.001124</td>\n",
       "      <td>-0.006852</td>\n",
       "      <td>-0.03837</td>\n",
       "      <td>0.081718</td>\n",
       "      <td>-0.274572</td>\n",
       "      <td>0.121526</td>\n",
       "      <td>-0.03151</td>\n",
       "      <td>0.08789</td>\n",
       "      <td>0.100911</td>\n",
       "      <td>-0.152726</td>\n",
       "      <td>0.067252</td>\n",
       "      <td>0.012061</td>\n",
       "      <td>-0.033388</td>\n",
       "      <td>-0.033388</td>\n",
       "      <td>-0.2878</td>\n",
       "      <td>0.27899</td>\n",
       "      <td>0.210158</td>\n",
       "      <td>-0.315858</td>\n",
       "      <td>-0.003517</td>\n",
       "      <td>-0.124605</td>\n",
       "      <td>0.051907</td>\n",
       "      <td>-0.063801</td>\n",
       "      <td>-0.144068</td>\n",
       "      <td>-0.212129</td>\n",
       "      <td>-0.111617</td>\n",
       "      <td>0.034917</td>\n",
       "      <td>-0.035362</td>\n",
       "      <td>-0.181126</td>\n",
       "      <td>0.064944</td>\n",
       "      <td>0.068985</td>\n",
       "      <td>-0.091769</td>\n",
       "      <td>-0.361114</td>\n",
       "      <td>0.044587</td>\n",
       "      <td>-0.176689</td>\n",
       "      <td>0.42389</td>\n",
       "      <td>0.02052</td>\n",
       "      <td>0.25461</td>\n",
       "      <td>0.0741</td>\n",
       "      <td>0.017153</td>\n",
       "      <td>-0.079558</td>\n",
       "      <td>0.009877</td>\n",
       "      <td>0.100538</td>\n",
       "      <td>-0.078111</td>\n",
       "      <td>0.020759</td>\n",
       "      <td>0.176666</td>\n",
       "      <td>0.159543</td>\n",
       "      <td>0.067211</td>\n",
       "      <td>-0.774588</td>\n",
       "      <td>-0.08708</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>-0.178037</td>\n",
       "      <td>-0.069889</td>\n",
       "      <td>-0.124833</td>\n",
       "      <td>-0.067405</td>\n",
       "      <td>0.048711</td>\n",
       "      <td>0.163258</td>\n",
       "      <td>-0.066281</td>\n",
       "      <td>0.184622</td>\n",
       "      <td>-0.169374</td>\n",
       "      <td>0.560434</td>\n",
       "      <td>-0.16392</td>\n",
       "      <td>0.218972</td>\n",
       "      <td>0.204333</td>\n",
       "      <td>0.052009</td>\n",
       "      <td>-0.150673</td>\n",
       "      <td>-0.100176</td>\n",
       "      <td>1.382885</td>\n",
       "      <td>-0.04918</td>\n",
       "      <td>0.059883</td>\n",
       "      <td>0.01296</td>\n",
       "      <td>0.077377</td>\n",
       "      <td>-0.06871</td>\n",
       "      <td>-0.380138</td>\n",
       "      <td>0.045499</td>\n",
       "      <td>0.051549</td>\n",
       "      <td>0.081579</td>\n",
       "      <td>0.056813</td>\n",
       "      <td>0.476011</td>\n",
       "      <td>0.166003</td>\n",
       "      <td>0.079476</td>\n",
       "      <td>0.072354</td>\n",
       "      <td>-0.066219</td>\n",
       "      <td>0.03307</td>\n",
       "      <td>0.00089</td>\n",
       "      <td>0.025629</td>\n",
       "      <td>-0.058908</td>\n",
       "      <td>0.029188</td>\n",
       "      <td>0.126229</td>\n",
       "      <td>0.095597</td>\n",
       "      <td>-0.182711</td>\n",
       "      <td>0.044051</td>\n",
       "      <td>-0.013555</td>\n",
       "      <td>0.01428</td>\n",
       "      <td>0.029507</td>\n",
       "      <td>-0.077927</td>\n",
       "      <td>-0.043066</td>\n",
       "      <td>-0.043066</td>\n",
       "      <td>0.338858</td>\n",
       "      <td>0.084956</td>\n",
       "      <td>-0.284016</td>\n",
       "      <td>-0.040185</td>\n",
       "      <td>-0.01312</td>\n",
       "      <td>0.082785</td>\n",
       "      <td>-0.023375</td>\n",
       "      <td>0.10065</td>\n",
       "      <td>-0.058112</td>\n",
       "      <td>0.285583</td>\n",
       "      <td>0.161262</td>\n",
       "      <td>0.02201</td>\n",
       "      <td>-0.081786</td>\n",
       "      <td>-0.047592</td>\n",
       "      <td>-0.023796</td>\n",
       "      <td>0.080394</td>\n",
       "      <td>0.080394</td>\n",
       "      <td>0.073885</td>\n",
       "      <td>-0.047166</td>\n",
       "      <td>-0.196578</td>\n",
       "      <td>0.005641</td>\n",
       "      <td>0.147501</td>\n",
       "      <td>-0.060149</td>\n",
       "      <td>-0.097269</td>\n",
       "      <td>-0.140631</td>\n",
       "      <td>0.020711</td>\n",
       "      <td>-0.067842</td>\n",
       "      <td>0.287079</td>\n",
       "      <td>0.149268</td>\n",
       "      <td>0.090183</td>\n",
       "      <td>-0.138436</td>\n",
       "      <td>0.309549</td>\n",
       "      <td>-0.054012</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136794</td>\n",
       "      <td>-0.26858</td>\n",
       "      <td>0.079516</td>\n",
       "      <td>0.058392</td>\n",
       "      <td>-0.04076</td>\n",
       "      <td>-0.178584</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>0.012771</td>\n",
       "      <td>-0.16728</td>\n",
       "      <td>-0.22212</td>\n",
       "      <td>-0.094598</td>\n",
       "      <td>0.023982</td>\n",
       "      <td>-0.129618</td>\n",
       "      <td>0.013523</td>\n",
       "      <td>0.075654</td>\n",
       "      <td>0.084012</td>\n",
       "      <td>-0.167851</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>0.031317</td>\n",
       "      <td>0.028779</td>\n",
       "      <td>0.134754</td>\n",
       "      <td>-0.211545</td>\n",
       "      <td>0.05439</td>\n",
       "      <td>-0.011439</td>\n",
       "      <td>-0.087032</td>\n",
       "      <td>-0.092204</td>\n",
       "      <td>-0.206332</td>\n",
       "      <td>-0.11572</td>\n",
       "      <td>-0.097904</td>\n",
       "      <td>0.048629</td>\n",
       "      <td>0.230822</td>\n",
       "      <td>0.029898</td>\n",
       "      <td>0.018941</td>\n",
       "      <td>-0.141627</td>\n",
       "      <td>-0.052695</td>\n",
       "      <td>-0.05989</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>0.085665</td>\n",
       "      <td>-0.126007</td>\n",
       "      <td>-0.044015</td>\n",
       "      <td>-0.169554</td>\n",
       "      <td>0.139878</td>\n",
       "      <td>0.144167</td>\n",
       "      <td>-0.076193</td>\n",
       "      <td>-0.057955</td>\n",
       "      <td>-0.135479</td>\n",
       "      <td>0.052299</td>\n",
       "      <td>-0.061283</td>\n",
       "      <td>0.01708</td>\n",
       "      <td>0.076153</td>\n",
       "      <td>-0.02935</td>\n",
       "      <td>0.088058</td>\n",
       "      <td>0.074274</td>\n",
       "      <td>0.04332</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>0.016662</td>\n",
       "      <td>0.046441</td>\n",
       "      <td>0.02926</td>\n",
       "      <td>-0.275626</td>\n",
       "      <td>-0.006527</td>\n",
       "      <td>-0.005471</td>\n",
       "      <td>-0.005422</td>\n",
       "      <td>-0.12967</td>\n",
       "      <td>-0.07661</td>\n",
       "      <td>-0.208533</td>\n",
       "      <td>-0.116666</td>\n",
       "      <td>0.21889</td>\n",
       "      <td>0.117209</td>\n",
       "      <td>0.048756</td>\n",
       "      <td>0.02094</td>\n",
       "      <td>0.00386</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>-0.035903</td>\n",
       "      <td>0.039592</td>\n",
       "      <td>0.039592</td>\n",
       "      <td>-0.009273</td>\n",
       "      <td>0.071776</td>\n",
       "      <td>-0.255808</td>\n",
       "      <td>0.116936</td>\n",
       "      <td>0.134808</td>\n",
       "      <td>0.153929</td>\n",
       "      <td>-0.095655</td>\n",
       "      <td>-0.03791</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>0.045476</td>\n",
       "      <td>-0.238958</td>\n",
       "      <td>0.104134</td>\n",
       "      <td>-0.259499</td>\n",
       "      <td>0.080021</td>\n",
       "      <td>-0.122181</td>\n",
       "      <td>0.213352</td>\n",
       "      <td>0.127354</td>\n",
       "      <td>0.072446</td>\n",
       "      <td>0.04368</td>\n",
       "      <td>-0.003741</td>\n",
       "      <td>0.003081</td>\n",
       "      <td>-0.034064</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>0.010216</td>\n",
       "      <td>-0.033073</td>\n",
       "      <td>0.067897</td>\n",
       "      <td>-0.015314</td>\n",
       "      <td>-0.224128</td>\n",
       "      <td>-0.102437</td>\n",
       "      <td>-0.192308</td>\n",
       "      <td>0.088323</td>\n",
       "      <td>0.021673</td>\n",
       "      <td>-0.510079</td>\n",
       "      <td>0.015877</td>\n",
       "      <td>-0.195509</td>\n",
       "      <td>-0.003902</td>\n",
       "      <td>0.088054</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>-0.391686</td>\n",
       "      <td>-0.127541</td>\n",
       "      <td>0.026809</td>\n",
       "      <td>0.04131</td>\n",
       "      <td>-0.203985</td>\n",
       "      <td>-0.151362</td>\n",
       "      <td>-0.017348</td>\n",
       "      <td>0.149184</td>\n",
       "      <td>-0.088179</td>\n",
       "      <td>-0.131045</td>\n",
       "      <td>0.528672</td>\n",
       "      <td>0.2967</td>\n",
       "      <td>0.564764</td>\n",
       "      <td>0.06383</td>\n",
       "      <td>0.093864</td>\n",
       "      <td>-0.150902</td>\n",
       "      <td>0.058984</td>\n",
       "      <td>-0.131373</td>\n",
       "      <td>-0.060703</td>\n",
       "      <td>-0.177436</td>\n",
       "      <td>0.230716</td>\n",
       "      <td>-0.034772</td>\n",
       "      <td>0.019796</td>\n",
       "      <td>0.019796</td>\n",
       "      <td>0.019796</td>\n",
       "      <td>-0.013792</td>\n",
       "      <td>-0.243116</td>\n",
       "      <td>-0.052454</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>-0.066202</td>\n",
       "      <td>-1.340425</td>\n",
       "      <td>-0.094227</td>\n",
       "      <td>0.03647</td>\n",
       "      <td>0.138289</td>\n",
       "      <td>0.781977</td>\n",
       "      <td>-0.123327</td>\n",
       "      <td>0.083917</td>\n",
       "      <td>-0.011025</td>\n",
       "      <td>0.10334</td>\n",
       "      <td>-0.062911</td>\n",
       "      <td>0.180141</td>\n",
       "      <td>-0.115751</td>\n",
       "      <td>0.14153</td>\n",
       "      <td>-0.340436</td>\n",
       "      <td>0.219864</td>\n",
       "      <td>0.306538</td>\n",
       "      <td>-0.035868</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>-0.256483</td>\n",
       "      <td>0.086969</td>\n",
       "      <td>0.007377</td>\n",
       "      <td>0.147694</td>\n",
       "      <td>-0.140744</td>\n",
       "      <td>0.178801</td>\n",
       "      <td>-0.144593</td>\n",
       "      <td>-0.131837</td>\n",
       "      <td>-0.02958</td>\n",
       "      <td>-0.032533</td>\n",
       "      <td>-0.243308</td>\n",
       "      <td>0.138783</td>\n",
       "      <td>-0.459269</td>\n",
       "      <td>0.39277</td>\n",
       "      <td>0.452557</td>\n",
       "      <td>0.169659</td>\n",
       "      <td>0.506227</td>\n",
       "      <td>0.12897</td>\n",
       "      <td>-0.018501</td>\n",
       "      <td>-0.138186</td>\n",
       "      <td>0.152399</td>\n",
       "      <td>0.230071</td>\n",
       "      <td>-0.074288</td>\n",
       "      <td>0.342786</td>\n",
       "      <td>0.079686</td>\n",
       "      <td>-0.164304</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>0.080122</td>\n",
       "      <td>0.117803</td>\n",
       "      <td>-0.112538</td>\n",
       "      <td>0.093017</td>\n",
       "      <td>0.044463</td>\n",
       "      <td>-0.104195</td>\n",
       "      <td>-0.049628</td>\n",
       "      <td>-0.220199</td>\n",
       "      <td>-0.446343</td>\n",
       "      <td>0.020442</td>\n",
       "      <td>-0.003605</td>\n",
       "      <td>0.098229</td>\n",
       "      <td>-0.134701</td>\n",
       "      <td>0.305972</td>\n",
       "      <td>0.347248</td>\n",
       "      <td>-0.030005</td>\n",
       "      <td>0.205748</td>\n",
       "      <td>-0.259808</td>\n",
       "      <td>0.370935</td>\n",
       "      <td>0.032276</td>\n",
       "      <td>-0.260934</td>\n",
       "      <td>0.056175</td>\n",
       "      <td>-0.227671</td>\n",
       "      <td>0.165453</td>\n",
       "      <td>-0.206087</td>\n",
       "      <td>-0.127777</td>\n",
       "      <td>-0.203745</td>\n",
       "      <td>0.239801</td>\n",
       "      <td>-0.016793</td>\n",
       "      <td>0.269171</td>\n",
       "      <td>-0.044588</td>\n",
       "      <td>-0.023681</td>\n",
       "      <td>-0.050442</td>\n",
       "      <td>0.262803</td>\n",
       "      <td>-0.169904</td>\n",
       "      <td>0.046024</td>\n",
       "      <td>-0.268557</td>\n",
       "      <td>0.034434</td>\n",
       "      <td>-0.22143</td>\n",
       "      <td>-0.001517</td>\n",
       "      <td>0.197624</td>\n",
       "      <td>-0.06664</td>\n",
       "      <td>-0.238371</td>\n",
       "      <td>0.184367</td>\n",
       "      <td>-0.119819</td>\n",
       "      <td>0.106052</td>\n",
       "      <td>-0.254044</td>\n",
       "      <td>0.092819</td>\n",
       "      <td>0.277081</td>\n",
       "      <td>-0.010746</td>\n",
       "      <td>-0.084838</td>\n",
       "      <td>-0.01192</td>\n",
       "      <td>-0.458486</td>\n",
       "      <td>0.034431</td>\n",
       "      <td>-0.439326</td>\n",
       "      <td>-0.057407</td>\n",
       "      <td>-0.057407</td>\n",
       "      <td>-0.202226</td>\n",
       "      <td>-0.202226</td>\n",
       "      <td>0.028847</td>\n",
       "      <td>0.126624</td>\n",
       "      <td>-0.226738</td>\n",
       "      <td>-0.101724</td>\n",
       "      <td>-0.101724</td>\n",
       "      <td>-0.101724</td>\n",
       "      <td>0.226049</td>\n",
       "      <td>0.13371</td>\n",
       "      <td>-0.02243</td>\n",
       "      <td>0.134877</td>\n",
       "      <td>-0.078193</td>\n",
       "      <td>0.022441</td>\n",
       "      <td>-0.167004</td>\n",
       "      <td>0.274191</td>\n",
       "      <td>0.142918</td>\n",
       "      <td>-0.299737</td>\n",
       "      <td>0.140461</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>0.141959</td>\n",
       "      <td>-0.09825</td>\n",
       "      <td>-0.265313</td>\n",
       "      <td>0.308771</td>\n",
       "      <td>-0.247846</td>\n",
       "      <td>-0.079035</td>\n",
       "      <td>0.328105</td>\n",
       "      <td>-0.03544</td>\n",
       "      <td>-0.019421</td>\n",
       "      <td>-0.163447</td>\n",
       "      <td>0.056583</td>\n",
       "      <td>0.05712</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>-0.002115</td>\n",
       "      <td>-0.227527</td>\n",
       "      <td>-0.034896</td>\n",
       "      <td>-0.209908</td>\n",
       "      <td>0.183232</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>0.109675</td>\n",
       "      <td>-0.211242</td>\n",
       "      <td>-0.36167</td>\n",
       "      <td>-0.032158</td>\n",
       "      <td>-0.032158</td>\n",
       "      <td>-0.032158</td>\n",
       "      <td>-0.107253</td>\n",
       "      <td>-0.138227</td>\n",
       "      <td>0.031211</td>\n",
       "      <td>0.200998</td>\n",
       "      <td>-0.013764</td>\n",
       "      <td>-0.263116</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>-0.247455</td>\n",
       "      <td>-0.270831</td>\n",
       "      <td>0.070422</td>\n",
       "      <td>-0.022313</td>\n",
       "      <td>-0.141632</td>\n",
       "      <td>-0.089454</td>\n",
       "      <td>-0.046937</td>\n",
       "      <td>0.354989</td>\n",
       "      <td>-0.191125</td>\n",
       "      <td>0.005009</td>\n",
       "      <td>-0.046838</td>\n",
       "      <td>-0.042338</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>-0.131505</td>\n",
       "      <td>-0.065774</td>\n",
       "      <td>0.228715</td>\n",
       "      <td>-0.024353</td>\n",
       "      <td>0.203087</td>\n",
       "      <td>-0.136054</td>\n",
       "      <td>-0.306048</td>\n",
       "      <td>-0.001915</td>\n",
       "      <td>-0.061226</td>\n",
       "      <td>0.033287</td>\n",
       "      <td>-0.05202</td>\n",
       "      <td>0.47369</td>\n",
       "      <td>-0.158837</td>\n",
       "      <td>0.035811</td>\n",
       "      <td>-0.058971</td>\n",
       "      <td>0.006417</td>\n",
       "      <td>0.030815</td>\n",
       "      <td>0.134422</td>\n",
       "      <td>-0.863954</td>\n",
       "      <td>0.516989</td>\n",
       "      <td>-0.595953</td>\n",
       "      <td>0.440231</td>\n",
       "      <td>0.057658</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.288871</td>\n",
       "      <td>0.014876</td>\n",
       "      <td>0.734128</td>\n",
       "      <td>0.064774</td>\n",
       "      <td>0.08399</td>\n",
       "      <td>0.374523</td>\n",
       "      <td>0.113517</td>\n",
       "      <td>0.241101</td>\n",
       "      <td>-0.011783</td>\n",
       "      <td>0.099255</td>\n",
       "      <td>0.929777</td>\n",
       "      <td>-0.066494</td>\n",
       "      <td>-0.115416</td>\n",
       "      <td>0.008937</td>\n",
       "      <td>-0.564995</td>\n",
       "      <td>-0.14528</td>\n",
       "      <td>-0.829372</td>\n",
       "      <td>0.07865</td>\n",
       "      <td>0.216418</td>\n",
       "      <td>-0.189748</td>\n",
       "      <td>0.109552</td>\n",
       "      <td>0.061852</td>\n",
       "      <td>0.055993</td>\n",
       "      <td>-0.210279</td>\n",
       "      <td>-0.080048</td>\n",
       "      <td>-0.108629</td>\n",
       "      <td>-0.108629</td>\n",
       "      <td>0.470721</td>\n",
       "      <td>0.044068</td>\n",
       "      <td>-0.005501</td>\n",
       "      <td>-0.153481</td>\n",
       "      <td>0.044066</td>\n",
       "      <td>-0.018572</td>\n",
       "      <td>0.163128</td>\n",
       "      <td>0.249898</td>\n",
       "      <td>0.260648</td>\n",
       "      <td>0.015294</td>\n",
       "      <td>-0.03293</td>\n",
       "      <td>0.112291</td>\n",
       "      <td>-0.183449</td>\n",
       "      <td>0.114078</td>\n",
       "      <td>0.501022</td>\n",
       "      <td>0.074917</td>\n",
       "      <td>-0.170533</td>\n",
       "      <td>0.025675</td>\n",
       "      <td>0.085622</td>\n",
       "      <td>0.075545</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.035136</td>\n",
       "      <td>-0.580076</td>\n",
       "      <td>-0.243753</td>\n",
       "      <td>0.248297</td>\n",
       "      <td>-0.022074</td>\n",
       "      <td>0.017928</td>\n",
       "      <td>0.025502</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.007731</td>\n",
       "      <td>0.007545</td>\n",
       "      <td>-0.352387</td>\n",
       "      <td>0.122644</td>\n",
       "      <td>0.108077</td>\n",
       "      <td>0.040919</td>\n",
       "      <td>0.156855</td>\n",
       "      <td>-0.3378</td>\n",
       "      <td>0.047601</td>\n",
       "      <td>-0.121499</td>\n",
       "      <td>0.02446</td>\n",
       "      <td>0.077975</td>\n",
       "      <td>-0.464481</td>\n",
       "      <td>-0.020886</td>\n",
       "      <td>-0.357478</td>\n",
       "      <td>0.127858</td>\n",
       "      <td>-0.190057</td>\n",
       "      <td>0.151928</td>\n",
       "      <td>0.259579</td>\n",
       "      <td>0.04086</td>\n",
       "      <td>-0.05289</td>\n",
       "      <td>-0.167665</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>0.203727</td>\n",
       "      <td>0.030923</td>\n",
       "      <td>-0.047192</td>\n",
       "      <td>0.166768</td>\n",
       "      <td>-0.036382</td>\n",
       "      <td>0.33843</td>\n",
       "      <td>-0.25552</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>-0.166857</td>\n",
       "      <td>0.099266</td>\n",
       "      <td>-0.016461</td>\n",
       "      <td>-0.008326</td>\n",
       "      <td>0.081342</td>\n",
       "      <td>-0.36388</td>\n",
       "      <td>-0.060652</td>\n",
       "      <td>0.23976</td>\n",
       "      <td>-0.013698</td>\n",
       "      <td>-0.130544</td>\n",
       "      <td>0.355948</td>\n",
       "      <td>-0.056412</td>\n",
       "      <td>0.106787</td>\n",
       "      <td>-0.084668</td>\n",
       "      <td>0.138885</td>\n",
       "      <td>-0.297487</td>\n",
       "      <td>0.22545</td>\n",
       "      <td>0.145129</td>\n",
       "      <td>0.214343</td>\n",
       "      <td>0.080091</td>\n",
       "      <td>0.302283</td>\n",
       "      <td>-0.335726</td>\n",
       "      <td>0.155163</td>\n",
       "      <td>-0.212949</td>\n",
       "      <td>0.169459</td>\n",
       "      <td>0.067619</td>\n",
       "      <td>0.074171</td>\n",
       "      <td>0.03195</td>\n",
       "      <td>0.01113</td>\n",
       "      <td>0.017715</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.417368</td>\n",
       "      <td>0.271601</td>\n",
       "      <td>-0.511137</td>\n",
       "      <td>-0.283006</td>\n",
       "      <td>0.784665</td>\n",
       "      <td>-0.021755</td>\n",
       "      <td>-0.527261</td>\n",
       "      <td>-0.095838</td>\n",
       "      <td>0.096313</td>\n",
       "      <td>0.363583</td>\n",
       "      <td>0.104756</td>\n",
       "      <td>-0.033616</td>\n",
       "      <td>-0.016057</td>\n",
       "      <td>-0.016057</td>\n",
       "      <td>-0.016057</td>\n",
       "      <td>-0.020875</td>\n",
       "      <td>0.039515</td>\n",
       "      <td>0.178559</td>\n",
       "      <td>0.058939</td>\n",
       "      <td>0.109902</td>\n",
       "      <td>-0.155147</td>\n",
       "      <td>0.059649</td>\n",
       "      <td>0.094529</td>\n",
       "      <td>-0.119047</td>\n",
       "      <td>-0.291584</td>\n",
       "      <td>0.432402</td>\n",
       "      <td>-0.150712</td>\n",
       "      <td>-0.029039</td>\n",
       "      <td>0.225849</td>\n",
       "      <td>-0.016697</td>\n",
       "      <td>0.157491</td>\n",
       "      <td>0.259206</td>\n",
       "      <td>-0.201174</td>\n",
       "      <td>-0.30915</td>\n",
       "      <td>-0.115108</td>\n",
       "      <td>-0.136204</td>\n",
       "      <td>0.771067</td>\n",
       "      <td>0.086986</td>\n",
       "      <td>0.074171</td>\n",
       "      <td>-0.048134</td>\n",
       "      <td>0.093929</td>\n",
       "      <td>0.014135</td>\n",
       "      <td>0.077032</td>\n",
       "      <td>0.428348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 125000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         aa   aaaaand       aap  aap circumcision      aauw        ab  \\\n",
       "0  0.003033 -0.243533  0.584949  0.027575         -0.031888  0.447751   \n",
       "\n",
       "      aback   abandon  abandoned  abandoning  abandonment     abbey  \\\n",
       "0  0.143809 -0.002159 -0.351781   0.68035     0.244434    -0.271246   \n",
       "\n",
       "   abbreviated  abbreviation       abc  abc news     abctv  \\\n",
       "0  0.087812    -0.006912      0.092327  0.035839  0.339848   \n",
       "\n",
       "   abctv scottmorrisonmp      abd  abdicate  abdicate responsibility  \\\n",
       "0  0.067532               0.03482  0.03482  -0.117718                  \n",
       "\n",
       "   abdication   abdomen  abdominal  abducted  abduction   abelist  aberrant  \\\n",
       "0  0.142861    0.142861 -0.078466  -0.059329 -0.037682   0.090871  0.002733   \n",
       "\n",
       "   aberration     abhor  abhor culture  abhorrent     abide   abiding  \\\n",
       "0 -0.013394   -0.015316  0.038021       0.098304   0.019924 -0.199023   \n",
       "\n",
       "   abiding citizen   abigail   ability  ability based  ability birth  \\\n",
       "0  0.149842         0.148714  0.005406 -0.002942      -1.423937        \n",
       "\n",
       "   ability change  ability choose  ability commit  ability compete  \\\n",
       "0 -0.097382       -0.087434       -0.006748       -0.111227          \n",
       "\n",
       "   ability consent  ability consent reasonable  ability control  \\\n",
       "0 -0.08009         -0.016359                    0.05178           \n",
       "\n",
       "   ability create  ability decision  ability dress  ability dress certain  \\\n",
       "0 -0.220896       -0.010088         -0.058558       0.01246                 \n",
       "\n",
       "   ability earn  ability enjoy  ability experience  ability influence  \\\n",
       "0 -0.016994     -0.015997      -0.11345            -0.11345             \n",
       "\n",
       "   ability informed  ability job  ability le  ability obtain  ability pay  \\\n",
       "0 -0.11345           0.098854    -0.005324   -0.042037       -0.054979      \n",
       "\n",
       "   ability power  ability prevent  ability provide  ability provide care  \\\n",
       "0  0.021139       0.194201         0.424501        -0.08296                \n",
       "\n",
       "   ability refrain  ability refrain personal  \\\n",
       "0  0.044902         0.081207                   \n",
       "\n",
       "   ability refrain personal accountability  ability reproduce  \\\n",
       "0 -0.119315                                -0.030559            \n",
       "\n",
       "   ability responsibility  ability survive  ability understand  \\\n",
       "0  0.028345                0.139491         0.170983             \n",
       "\n",
       "   ability willingness      abit    abject      able  able abandon  able able  \\\n",
       "0  0.170983             0.170983  0.040814 -0.114449  0.033531     -0.131436    \n",
       "\n",
       "   able abortion  able access  able accrue  able accrue wealth  able achieve  \\\n",
       "0 -0.107079       0.150548     0.100568     0.074039           -0.039929       \n",
       "\n",
       "   able afford  able angry  able answer  able answer question  able argue  \\\n",
       "0  0.105973     0.039938   -1.397111    -0.161323             -0.413053     \n",
       "\n",
       "   able argument  able ask  able away  able bodied  able bodied ci  \\\n",
       "0 -0.077746      -0.237168 -0.237168  -0.066151     0.21879          \n",
       "\n",
       "   able bring  able buy  able care  able change  able choose  able claim  \\\n",
       "0  0.175632   -0.326183 -0.040549   0.037573    -0.002807    -0.031812     \n",
       "\n",
       "   able compete  able consent  able consider  able continue  able contribute  \\\n",
       "0 -0.045277     -0.218265      0.109496      -0.01798        0.009982          \n",
       "\n",
       "   able control  able convince  able create  able date  able day  able deal  \\\n",
       "0  0.00401       0.034322      -0.049388    -0.061987   0.199782 -0.275799    \n",
       "\n",
       "   able decide  able decision  able defend  able discriminate  able discus  \\\n",
       "0 -0.060911     0.197067      -0.138953    -0.095473          -0.027915      \n",
       "\n",
       "   able distinguish  able empathize  able evidence  able experience  \\\n",
       "0  0.110358          0.10363         0.0397         0.050348          \n",
       "\n",
       "   able explain  able explain biological  able explain biological basis  \\\n",
       "0  0.001879      0.112641                 0.08501                         \n",
       "\n",
       "   able express  able fight  able financial  able fix  able force  \\\n",
       "0 -0.021393      0.195895   -0.012996       -0.236062 -0.029908     \n",
       "\n",
       "   able forward  able fully  able gain  able handle  able hold  able identify  \\\n",
       "0 -0.060161     -0.014808   -0.030472   0.000573     0.007121  -0.056807        \n",
       "\n",
       "   able incorporate  able job  able learn  able leave  able legally  \\\n",
       "0 -0.056807          0.259255 -0.010853    0.08631    -0.107596       \n",
       "\n",
       "   able lift  able live  able maintain  able manipulate  able meet  \\\n",
       "0 -0.00326    0.026905  -0.030063      -0.011004        -0.156547    \n",
       "\n",
       "   able navigate  able opt  able participate  able perform  able pick  \\\n",
       "0 -0.051091      -0.131074 -0.034017          0.012918      0.224952    \n",
       "\n",
       "   able play  able pregnant  able protect  able prove  able provide  \\\n",
       "0 -0.030311   0.101266       0.033531     -0.035723   -0.025325       \n",
       "\n",
       "   able pull  able raise  able recognize  able record  able refute  \\\n",
       "0  0.02585    0.102706   -0.009168        0.142517    -0.093112      \n",
       "\n",
       "   able relate  able remove  able separate  able separate conversation  \\\n",
       "0 -0.158737    -0.179177    -0.051309      -0.103808                     \n",
       "\n",
       "   able separate conversation fgm  able set  able sexist  able share  \\\n",
       "0 -0.155513                        0.256864  0.175216     0.025905     \n",
       "\n",
       "   able shut  able sign  able solve  able sort  able speak  \\\n",
       "0  0.125598  -0.125761  -0.10457     0.034271  -0.113076     \n",
       "\n",
       "   able speak clearly  able speak clearly unambiguously  able spend  \\\n",
       "0  0.008346           -0.065847                         -0.085608     \n",
       "\n",
       "   able stand  able start  able stay  able study  able support  able survive  \\\n",
       "0 -0.149411   -0.122514    0.00025   -0.135367   -0.340193     -0.22684        \n",
       "\n",
       "   able talk  able tell  able train  able trust  able turn  able understand  \\\n",
       "0 -0.101936  -0.101936  -0.101936   -0.001051    0.011018  -0.039199          \n",
       "\n",
       "   able vote  able watch  able wear     abled   ableism   ableist  \\\n",
       "0 -0.02846    0.387025   -0.107016  -0.054314 -0.054314  0.002411   \n",
       "\n",
       "   ableist slur  abnormal  abnormal behavior  abnormality    aboard   abolish  \\\n",
       "0  0.144977     -0.211097 -0.018556           0.009354    -0.095282 -0.186047   \n",
       "\n",
       "   abolish prison  abolished  abolishing  abolition  abolition capitalism  \\\n",
       "0 -0.524099        0.015689  -0.008753   -0.167259   0.144716               \n",
       "\n",
       "   abolitionism  abolitionist  abominable  abomination  aboriginal     abort  \\\n",
       "0 -0.272834     -0.087354      0.212212    0.004133     0.193009    0.058722   \n",
       "\n",
       "   abort baby  abort body  abort fetus  abort force   aborted  aborted fetus  \\\n",
       "0  0.094241    0.120052   -0.277535    -0.655305    -0.465653  0.273255        \n",
       "\n",
       "   aborting  aborting baby  abortion  abortion able  abortion abortion  \\\n",
       "0  0.010718  0.458838      -0.065199 -0.627907      -0.496817            \n",
       "\n",
       "   abortion access  abortion adoption  abortion aren  abortion argument  \\\n",
       "0 -0.520416         0.11938           -0.298573      -0.137042            \n",
       "\n",
       "   abortion available  abortion baby  abortion based  abortion bc  \\\n",
       "0 -0.09393            -0.166976       0.052455        0.085735      \n",
       "\n",
       "   abortion begin  abortion birth  abortion birth control  abortion bodily  \\\n",
       "0  0.170614        0.261225       -0.269783               -0.054826          \n",
       "\n",
       "   abortion bodily autonomy  abortion body  abortion body choice  \\\n",
       "0 -0.121761                  0.123499      -1.479742               \n",
       "\n",
       "   abortion choice  abortion clinic  abortion completely  abortion consent  \\\n",
       "0  0.099794         0.263282         0.244188             0.071321           \n",
       "\n",
       "   abortion consider  abortion cost  abortion country  abortion debate  \\\n",
       "0 -0.214189          -0.146056      -0.023488         -0.054664          \n",
       "\n",
       "   abortion easy  abortion effective  abortion fine  abortion form  \\\n",
       "0  0.024207      -0.18809            -0.093573      -0.04174         \n",
       "\n",
       "   abortion free  abortion freely  abortion gendered  abortion generally  \\\n",
       "0 -0.155959      -0.143353         0.090762          -0.019061             \n",
       "\n",
       "   abortion happening  abortion illegal  abortion killing  abortion late  \\\n",
       "0 -0.057622            0.166257          0.408203          0.020433        \n",
       "\n",
       "   abortion law  abortion le  abortion legal  abortion let  \\\n",
       "0 -0.15814       0.072865     0.470934       -0.166658       \n",
       "\n",
       "   abortion literally  abortion making  abortion medical  abortion month  \\\n",
       "0  0.001047            0.49776          0.10731          -0.266527         \n",
       "\n",
       "   abortion mother  abortion murder  abortion necessarily  abortion okay  \\\n",
       "0 -0.399055        -0.097801        -0.081926              0.150281        \n",
       "\n",
       "   abortion option  abortion parent  abortion pay  abortion poor  \\\n",
       "0 -0.069697         0.138582         0.104782      0.120072        \n",
       "\n",
       "   abortion position  abortion pregnancy  abortion pregnant  abortion pretty  \\\n",
       "0  0.16378           -0.022614           -0.110706          -0.136473          \n",
       "\n",
       "   abortion prevent  abortion pro  abortion pro choice  abortion rare  \\\n",
       "0 -0.138934         -0.044775      0.099999             0.11268         \n",
       "\n",
       "   abortion reproductive  abortion service  abortion shouldn  abortion simply  \\\n",
       "0 -0.158365              -0.061198         -0.077319         -0.077319          \n",
       "\n",
       "   abortion support  abortion topic  abortion unjust  abortion unjust killing  \\\n",
       "0  0.06619          -0.023671       -0.124976        -0.138493                  \n",
       "\n",
       "   abortion wanted  abortion week  abortion world  abovementioned   abraham  \\\n",
       "0 -0.090144        -0.108994      -0.003449        0.093834       -0.284981   \n",
       "\n",
       "   abrahamic  abrahamic religion    abrams  abrasion  abrasive  abridging  \\\n",
       "0 -0.031728   0.136778           -0.230838 -0.229084  0.037179  0.091815    \n",
       "\n",
       "   abridging freedom  abridging freedom speech    abroad  abruptly   absence  \\\n",
       "0 -0.210392          -0.227497                 -0.227497 -0.227497  0.094825   \n",
       "\n",
       "   absence compelling  absence compelling evidence  absence court  \\\n",
       "0  0.043382           -0.536455                    -0.043676        \n",
       "\n",
       "   absence court order  absence evidence    absent  absent evidence  \\\n",
       "0 -0.043676             0.076108          0.076108 -0.559523          \n",
       "\n",
       "   absent father  absentee  absolute  absolute best  absolute bitch  \\\n",
       "0  0.018427       0.105596  0.10605  -0.118325       0.222178         \n",
       "\n",
       "   absolute critical  absolute critical reflection  \\\n",
       "0  0.998195           0.031494                       \n",
       "\n",
       "   absolute critical reflection mediated  absolute garbage  absolute hell  \\\n",
       "0  0.082953                              -0.162383         -0.039299        \n",
       "\n",
       "   absolute hiv  absolute hiv number  absolute hiv number methodological  \\\n",
       "0 -0.039299     -0.060426            -0.01783                              \n",
       "\n",
       "   absolute lack  absolute power  absolute statement  absolute totally  \\\n",
       "0 -0.01783        0.137868       -0.006951            0.039127           \n",
       "\n",
       "   absolute totally free  absolute totally free oppression  absolute truth  \\\n",
       "0  0.039127               0.039127                          0.016847         \n",
       "\n",
       "   absolute worst  absolutely  absolutely absurd  absolutely arguing  \\\n",
       "0  0.19813        -0.002727   -0.134241          -0.124115             \n",
       "\n",
       "   absolutely awful  absolutely ban  absolutely based  absolutely body  \\\n",
       "0 -0.234706          0.101434       -1.679096         -0.116716          \n",
       "\n",
       "   absolutely called  absolutely change  absolutely choice  absolutely choose  \\\n",
       "0  0.267626          -0.068443          -0.123776          -0.022759            \n",
       "\n",
       "   absolutely clear  absolutely correct  absolutely deserve  \\\n",
       "0 -0.005149         -0.068109            0.160371             \n",
       "\n",
       "   absolutely disgusting  absolutely doubt  absolutely equal  \\\n",
       "0  0.392655               0.464881         -0.104759           \n",
       "\n",
       "   absolutely equal oppression  absolutely equal oppression free  \\\n",
       "0 -0.132916                    -0.094957                           \n",
       "\n",
       "   absolutely essential  absolutely evidence  absolutely fine  \\\n",
       "0 -0.098495             -0.026177            -0.026177          \n",
       "\n",
       "   absolutely fucking  absolutely harassment  absolutely hate  \\\n",
       "0 -0.026177            0.039833              -0.144296          \n",
       "\n",
       "   absolutely horrible  absolutely idea  absolutely important  \\\n",
       "0  0.10202             -0.046132        -0.159564               \n",
       "\n",
       "   absolutely involves  absolutely love  absolutely matter  \\\n",
       "0  0.084196             0.012997         0.161981            \n",
       "\n",
       "   absolutely necessary  absolutely option  absolutely possible  \\\n",
       "0 -0.153378             -0.031059          -0.072652              \n",
       "\n",
       "   absolutely proof  absolutely proof gendered  \\\n",
       "0  0.092178          0.063612                    \n",
       "\n",
       "   absolutely proof gendered difference  absolutely question  \\\n",
       "0 -0.029394                             -0.029394              \n",
       "\n",
       "   absolutely relevant  absolutely ridiculous  absolutely sense  \\\n",
       "0  0.044419            -0.015316              -0.001124           \n",
       "\n",
       "   absolutely shit  absolutely sign  absolutely stand  absolutely support  \\\n",
       "0 -0.006852        -0.03837          0.081718         -0.274572             \n",
       "\n",
       "   absolutely talk  absolutely terrible  absolutely true  \\\n",
       "0  0.121526        -0.03151              0.08789           \n",
       "\n",
       "   absolutely understand  absolutely useless  absolutely whatsoever  \\\n",
       "0  0.100911              -0.152726            0.067252                \n",
       "\n",
       "   absolutely wonderful  absolutely wonderful helping  \\\n",
       "0  0.012061             -0.033388                       \n",
       "\n",
       "   absolutely wonderful helping primary  absolutely zero  absolution  \\\n",
       "0 -0.033388                             -0.2878           0.27899      \n",
       "\n",
       "   absolutist  absolutley  absolutly   absolve  absolved  absolving    absorb  \\\n",
       "0  0.210158   -0.315858   -0.003517  -0.124605  0.051907 -0.063801  -0.144068   \n",
       "\n",
       "   absorbed   absorbs  absorption   abstain  abstaining  abstinence  abstract  \\\n",
       "0 -0.212129 -0.111617  0.034917   -0.035362 -0.181126    0.064944    0.068985   \n",
       "\n",
       "   abstract concept  abstract paper  abstraction  abstractly   absurd  \\\n",
       "0 -0.091769         -0.361114        0.044587    -0.176689    0.42389   \n",
       "\n",
       "   absurd claim  absurd statement  absurdity  absurdly  absurdum  abundance  \\\n",
       "0  0.02052       0.25461           0.0741     0.017153 -0.079558  0.009877    \n",
       "\n",
       "   abundant  abundantly  abundantly clear     abuse  abuse abuse  \\\n",
       "0  0.100538 -0.078111    0.020759          0.176666  0.159543      \n",
       "\n",
       "   abuse abused  abuse abuser  abuse according  abuse accusation  \\\n",
       "0  0.067211     -0.774588     -0.08708          0.023401           \n",
       "\n",
       "   abuse allegation  abuse amp  abuse assault  abuse authority  abuse based  \\\n",
       "0 -0.178037         -0.069889  -0.124833      -0.067405         0.048711      \n",
       "\n",
       "   abuse believed  abuse boy  abuse bullying  abuse called  abuse came  \\\n",
       "0  0.163258       -0.066281   0.184622       -0.169374      0.560434     \n",
       "\n",
       "   abuse cause  abuse childhood  abuse claim  abuse convicted  abuse country  \\\n",
       "0 -0.16392      0.218972         0.204333     0.052009        -0.150673        \n",
       "\n",
       "   abuse culture  abuse depression  abuse discrimination  abuse domestic  \\\n",
       "0 -0.100176       1.382885         -0.04918               0.059883         \n",
       "\n",
       "   abuse domestic violence  abuse dr  abuse end  abuse equal  \\\n",
       "0  0.01296                  0.077377 -0.06871   -0.380138      \n",
       "\n",
       "   abuse especially  abuse example  abuse exploitation  abuse family  \\\n",
       "0  0.045499          0.051549       0.081579            0.056813       \n",
       "\n",
       "   abuse far  abuse form  abuse got  abuse hand  abuse happened  \\\n",
       "0  0.476011   0.166003    0.079476   0.072354   -0.066219         \n",
       "\n",
       "   abuse harassment  abuse horrible  abuse husband  abuse inflicted  \\\n",
       "0  0.03307           0.00089         0.025629      -0.058908          \n",
       "\n",
       "   abuse intimate  abuse intimate partner  abuse kid  abuse kill  abuse kind  \\\n",
       "0  0.029188        0.126229                0.095597  -0.182711    0.044051     \n",
       "\n",
       "   abuse law  abuse le  abuse lead  abuse let  abuse likely  abuse little  \\\n",
       "0 -0.013555   0.01428   0.029507   -0.077927  -0.043066     -0.043066       \n",
       "\n",
       "   abuse maybe  abuse mental  abuse mental health  abuse mother  \\\n",
       "0  0.338858     0.084956     -0.284016            -0.040185       \n",
       "\n",
       "   abuse neglect  abuse number  abuse offender  abuse ok  abuse okay  \\\n",
       "0 -0.01312        0.082785     -0.023375        0.10065  -0.058112     \n",
       "\n",
       "   abuse online  abuse parent  abuse parent court  abuse partner  \\\n",
       "0  0.285583      0.161262      0.02201            -0.081786        \n",
       "\n",
       "   abuse perpetrator  abuse physical  abuse position  abuse power  \\\n",
       "0 -0.047592          -0.023796        0.080394        0.080394      \n",
       "\n",
       "   abuse psychopathic  abuse psychopathic guidance  \\\n",
       "0  0.073885           -0.047166                      \n",
       "\n",
       "   abuse psychopathic guidance stereotype  abuse question  abuse raped  \\\n",
       "0 -0.196578                                0.005641        0.147501      \n",
       "\n",
       "   abuse receive  abuse received  abuse report  abuse seen  abuse sharing  \\\n",
       "0 -0.060149      -0.097269       -0.140631      0.020711   -0.067842        \n",
       "\n",
       "   abuse sharing metoo  abuse sharing metoo platform  abuse shelter  \\\n",
       "0  0.287079             0.149268                      0.090183        \n",
       "\n",
       "   abuse situation  abuse social  abuse source    ...     yeah relationship  \\\n",
       "0 -0.138436         0.309549     -0.054012        ...     0.136794            \n",
       "\n",
       "   yeah saw  yeah seen  yeah sense  yeah sexist  yeah shit  yeah shouldn  \\\n",
       "0 -0.26858   0.079516   0.058392   -0.04076     -0.178584   0.002498       \n",
       "\n",
       "   yeah sorry  yeah sort  yeah sound  yeah tell  yeah thats  yeah thinking  \\\n",
       "0  0.012771   -0.16728   -0.22212    -0.094598   0.023982   -0.129618        \n",
       "\n",
       "   yeah totally  yeah toxic  yeah trans  yeah transphobia  yeah true  \\\n",
       "0  0.013523      0.075654    0.084012   -0.167851         -0.015747    \n",
       "\n",
       "   yeah understand  yeah used  yeah usually  yeah went  yeah won  yeah yeah  \\\n",
       "0  0.031317         0.028779   0.134754     -0.211545   0.05439  -0.011439    \n",
       "\n",
       "   yeah year      year  year abuse  year age  year ago  year ago considered  \\\n",
       "0 -0.087032  -0.092204 -0.206332   -0.11572  -0.097904  0.048629              \n",
       "\n",
       "   year ago culture  year ago difference  year ago got  year ago maybe  \\\n",
       "0  0.230822          0.029898             0.018941     -0.141627         \n",
       "\n",
       "   year ago pay  year ago refused  year ago refused appear  year ago student  \\\n",
       "0 -0.052695     -0.05989           0.098427                 0.085665           \n",
       "\n",
       "   year ago talking  year ago today  year ago year  year ago year old  \\\n",
       "0 -0.126007         -0.044015       -0.169554       0.139878            \n",
       "\n",
       "   year amp  year apart  year assault  year average  year away  year baby  \\\n",
       "0  0.144167 -0.076193   -0.057955     -0.135479      0.052299  -0.061283    \n",
       "\n",
       "   year bar  year based  year basically  year black  year book  year called  \\\n",
       "0  0.01708   0.076153   -0.02935         0.088058    0.074274   0.04332       \n",
       "\n",
       "   year care  year career  year change  year changed  year clean  year close  \\\n",
       "0  0.016662   0.016662     0.016662     0.016662      0.046441    0.02926      \n",
       "\n",
       "   year college  year compared  year considering  year control  \\\n",
       "0 -0.275626     -0.006527      -0.005471         -0.005422       \n",
       "\n",
       "   year control ex  year couldn  year count  year country  year crime  \\\n",
       "0 -0.12967         -0.07661     -0.208533   -0.116666      0.21889      \n",
       "\n",
       "   year culture  year culture cool  year current  year date  year dating  \\\n",
       "0  0.117209      0.048756           0.02094       0.00386    0.024934      \n",
       "\n",
       "   year day  year decade  year degree  year difference  year difficult  \\\n",
       "0 -0.035903  0.039592     0.039592    -0.009273         0.071776         \n",
       "\n",
       "   year doctrine  year doing  year earlier  year early  year education  \\\n",
       "0 -0.255808       0.116936    0.134808      0.153929   -0.095655         \n",
       "\n",
       "   year end  year especially  year eventually  year eventually got  \\\n",
       "0 -0.03791  -0.029834         0.045476        -0.238958              \n",
       "\n",
       "   year example  year expect  year experience  year failed  year fair  \\\n",
       "0  0.104134     -0.259499     0.080021        -0.122181     0.213352    \n",
       "\n",
       "   year fall  year false  year field  year fighting  year finally  year fine  \\\n",
       "0  0.127354   0.072446    0.04368    -0.003741       0.003081     -0.034064    \n",
       "\n",
       "   year free  year friend  year fuck  year fucking  year future  year general  \\\n",
       "0  0.00157    0.010216    -0.033073   0.067897     -0.015314    -0.224128       \n",
       "\n",
       "   year genuinely  year getting  year got  year graduated  year great  \\\n",
       "0 -0.102437       -0.192308      0.088323  0.021673       -0.510079     \n",
       "\n",
       "   year grew  year guilty  year half  year happened  year harassment  \\\n",
       "0  0.015877  -0.195509    -0.003902   0.088054       0.020161          \n",
       "\n",
       "   year hard  year haven  year high  year high school  year hit  year home  \\\n",
       "0 -0.391686  -0.127541    0.026809   0.04131          -0.203985 -0.151362    \n",
       "\n",
       "   year hrt  year huge  year human  year husband  year inappropriate  \\\n",
       "0 -0.017348  0.149184  -0.088179   -0.131045      0.528672             \n",
       "\n",
       "   year income  year instead  year international  year jail  year job  \\\n",
       "0  0.2967       0.564764      0.06383             0.093864  -0.150902   \n",
       "\n",
       "   year judge  year kid  year killing  year later  year law   year le  \\\n",
       "0  0.058984   -0.131373 -0.060703     -0.177436    0.230716 -0.034772   \n",
       "\n",
       "   year left  year let  year likely  year line  year live  year lived  \\\n",
       "0  0.019796   0.019796  0.019796    -0.013792  -0.243116  -0.052454     \n",
       "\n",
       "   year living  year long  year longer  year looked  year lost  year love  \\\n",
       "0  0.006053    -0.066202  -1.340425    -0.094227     0.03647    0.138289    \n",
       "\n",
       "   year marriage  year married  year massive  year max  year maximum  \\\n",
       "0  0.781977      -0.123327      0.083917     -0.011025  0.10334        \n",
       "\n",
       "   year maybe  year medical  year met  year metoo  year middle  year million  \\\n",
       "0 -0.062911    0.180141     -0.115751  0.14153    -0.340436     0.219864       \n",
       "\n",
       "   year minimum  year miserable  year mod  year month  year mrm  year new  \\\n",
       "0  0.306538     -0.035868        0.056957 -0.256483    0.086969  0.007377   \n",
       "\n",
       "   year noticed  year number  year obviously  year old  year old able  \\\n",
       "0  0.147694     -0.140744     0.178801       -0.144593 -0.131837        \n",
       "\n",
       "   year old attractive  year old baby  year old black  year old boy  \\\n",
       "0 -0.02958             -0.032533      -0.243308        0.138783       \n",
       "\n",
       "   year old chose  year old consent  year old considered  year old dating  \\\n",
       "0 -0.459269        0.39277           0.452557             0.169659          \n",
       "\n",
       "   year old daughter  year old florida  year old friend  year old getting  \\\n",
       "0  0.506227           0.12897          -0.018501        -0.138186           \n",
       "\n",
       "   year old got  year old kid  year old le  year old live  year old living  \\\n",
       "0  0.152399      0.230071     -0.074288     0.342786       0.079686          \n",
       "\n",
       "   year old love  year old mature  year old model  year old mom  \\\n",
       "0 -0.164304      -0.002386         0.080122        0.117803       \n",
       "\n",
       "   year old nephew  year old pregnant  year old probably  year old raped  \\\n",
       "0 -0.112538         0.093017           0.044463          -0.104195         \n",
       "\n",
       "   year old single  year old sister  year old son  year old started  \\\n",
       "0 -0.049628        -0.220199        -0.446343      0.020442           \n",
       "\n",
       "   year old student  year old teenager  year old told  year old used  \\\n",
       "0 -0.003605          0.098229          -0.134701       0.305972        \n",
       "\n",
       "   year old working  year old year  year old year old  year older  \\\n",
       "0  0.347248         -0.030005       0.205748          -0.259808     \n",
       "\n",
       "   year oppression  year order  year paid  year parent  year particularly  \\\n",
       "0  0.370935         0.032276   -0.260934   0.056175    -0.227671            \n",
       "\n",
       "   year passed  year past  year pay  year period  year pretty  year prison  \\\n",
       "0  0.165453    -0.206087  -0.127777 -0.203745     0.239801    -0.016793      \n",
       "\n",
       "   year prison fine  year probably  year probation  year quite  year reach  \\\n",
       "0  0.269171         -0.044588      -0.023681       -0.050442    0.262803     \n",
       "\n",
       "   year real  year realize  year receive  year received  year recently  \\\n",
       "0 -0.169904   0.046024     -0.268557      0.034434      -0.22143         \n",
       "\n",
       "   year regarding  year relationship  year released  year remember  \\\n",
       "0 -0.001517        0.197624          -0.06664       -0.238371        \n",
       "\n",
       "   year retirement  year retirement difference  \\\n",
       "0  0.184367        -0.119819                     \n",
       "\n",
       "   year retirement difference retirement  year retirement expectancy  \\\n",
       "0  0.106052                              -0.254044                     \n",
       "\n",
       "   year retirement expectancy pension  year road  year rodger  year row  \\\n",
       "0  0.092819                            0.277081  -0.010746    -0.084838   \n",
       "\n",
       "   year saving  year school  year second  year seen  year self  year senior  \\\n",
       "0 -0.01192     -0.458486     0.034431    -0.439326  -0.057407  -0.057407      \n",
       "\n",
       "   year sentence  year social  year specifically  year spend  year spent  \\\n",
       "0 -0.202226      -0.202226     0.028847           0.126624   -0.226738     \n",
       "\n",
       "   year splc  year stand  year start  year start coming  \\\n",
       "0 -0.101724  -0.101724   -0.101724    0.226049            \n",
       "\n",
       "   year start coming stop  year started  year state  year step  year straight  \\\n",
       "0  0.13371                -0.02243       0.134877   -0.078193   0.022441        \n",
       "\n",
       "   year student  year study  year suck  year support  year taken  year taking  \\\n",
       "0 -0.167004      0.274191    0.142918  -0.299737      0.140461   -0.000395      \n",
       "\n",
       "   year talk  year talking  year teacher  year tell  year telling  \\\n",
       "0  0.141959  -0.09825      -0.265313      0.308771  -0.247846       \n",
       "\n",
       "   year thinking  year thousand  year took  year training  year treatment  \\\n",
       "0 -0.079035       0.328105      -0.03544   -0.019421      -0.163447         \n",
       "\n",
       "   year tried  year true  year trying  year undergrad  year understand  \\\n",
       "0  0.056583    0.05712    0.000297    -0.002115       -0.227527          \n",
       "\n",
       "   year united  year university  year unless  year used  year using  \\\n",
       "0 -0.034896    -0.209908         0.183232    -0.007245   0.109675     \n",
       "\n",
       "   year usually  year waiting  year wanted  year went  year wife  year won  \\\n",
       "0 -0.211242     -0.36167      -0.032158    -0.032158  -0.032158  -0.107253   \n",
       "\n",
       "   year worked  year working  year world  year wouldn  year year  \\\n",
       "0 -0.138227     0.031211      0.200998   -0.013764    -0.263116    \n",
       "\n",
       "   year year ago  year year old  year young  year younger  year zero  \\\n",
       "0 -0.000904      -0.247455      -0.270831    0.070422     -0.022313    \n",
       "\n",
       "     yearly     yearn  yearning     yeast  yeast infection       yee  \\\n",
       "0 -0.141632 -0.089454 -0.046937  0.354989 -0.191125         0.005009   \n",
       "\n",
       "   yee pedophile       yeh    yell    yelled   yelling    yellow  yellow red  \\\n",
       "0 -0.046838      -0.042338  0.0698 -0.131505 -0.065774  0.228715 -0.024353     \n",
       "\n",
       "        yen       yep  yep absolutely  yep concept  yep concept little  \\\n",
       "0  0.203087 -0.136054 -0.306048       -0.001915    -0.061226             \n",
       "\n",
       "   yep concept little fuel  yep toxic      yer  yesallmen  yesterday  \\\n",
       "0  0.033287                -0.05202    0.47369 -0.158837   0.035811    \n",
       "\n",
       "   yesterday evening  yesterday posted  yesterday revealed        yi  \\\n",
       "0 -0.058971           0.006417          0.030815            0.134422   \n",
       "\n",
       "   yiannopoulis  yiannopoulos     yield   yielded     yikes       yin  \\\n",
       "0 -0.863954      0.516989     -0.595953  0.440231  0.057658  0.041778   \n",
       "\n",
       "   yin yang      yinz        yo    yo kid    yoffe      yoga  yoga pant  \\\n",
       "0  0.288871  0.014876  0.734128  0.064774  0.08399  0.374523  0.113517    \n",
       "\n",
       "       yoke      york  york city  york city called  york city called wing  \\\n",
       "0  0.241101 -0.011783  0.099255   0.929777         -0.066494                \n",
       "\n",
       "   york city washington  york city washington dc  york state  \\\n",
       "0 -0.115416              0.008937                -0.564995     \n",
       "\n",
       "   york state excluded  york state excluded attending  york state law  \\\n",
       "0 -0.14528             -0.829372                       0.07865          \n",
       "\n",
       "   york university    yorker  yorkshire      youd     youll     young  \\\n",
       "0  0.216418        -0.189748  0.109552   0.061852  0.055993 -0.210279   \n",
       "\n",
       "   young absolutely  young adult  young age  young american  young angry  \\\n",
       "0 -0.080048         -0.108629    -0.108629   0.470721        0.044068      \n",
       "\n",
       "   young asian  young beautiful  young black  young boy  young brother  \\\n",
       "0 -0.005501    -0.153481         0.044066    -0.018572   0.163128        \n",
       "\n",
       "   young caught  young childless  young coming  young consent  \\\n",
       "0  0.249898      0.260648         0.015294     -0.03293         \n",
       "\n",
       "   young conservative  young couple  young daughter  young day  \\\n",
       "0  0.112291           -0.183449      0.114078        0.501022    \n",
       "\n",
       "   young desperate  young face  young fertile  young fertile attractive  \\\n",
       "0  0.074917        -0.170533    0.025675       0.085622                   \n",
       "\n",
       "   young fertile attractive countless  young fertile attractive period  \\\n",
       "0  0.075545                            0.034                             \n",
       "\n",
       "   young got  young hate  young hot  young human  young infant  young kid  \\\n",
       "0  0.035136  -0.580076   -0.243753   0.248297    -0.022074      0.017928    \n",
       "\n",
       "   young lady  young learn  young living  young looking  young mind  \\\n",
       "0  0.025502    0.010832     0.007731      0.007545      -0.352387     \n",
       "\n",
       "   young murdered  young old  young remember  young son  young sort  \\\n",
       "0  0.122644        0.108077   0.040919        0.156855  -0.3378       \n",
       "\n",
       "   young straight  young student  young teen  young teen boy  young teenager  \\\n",
       "0  0.047601       -0.121499       0.02446     0.077975       -0.464481         \n",
       "\n",
       "   young today  young told  young used  young violence  young world  \\\n",
       "0 -0.020886    -0.357478    0.127858   -0.190057        0.151928      \n",
       "\n",
       "   young year  young year old  young young   younger  younger boy  \\\n",
       "0  0.259579    0.04086        -0.05289     -0.167665  0.006193      \n",
       "\n",
       "   younger brother  younger doing  younger generation  younger older  \\\n",
       "0  0.203727         0.030923      -0.047192            0.166768        \n",
       "\n",
       "   younger older younger  younger sister  younger student  younger year  \\\n",
       "0 -0.036382               0.33843        -0.25552          0.363928       \n",
       "\n",
       "   youngest  youngster     youre     youth  youth age  youth suicide  \\\n",
       "0 -0.166857  0.099266  -0.016461 -0.008326  0.081342  -0.36388         \n",
       "\n",
       "     youtoo  youtube  youtube channel  youtube channel called  \\\n",
       "0 -0.060652  0.23976 -0.013698        -0.130544                 \n",
       "\n",
       "   youtube spotlight  youtube video  youtube viewer  youtuber  youtubers  \\\n",
       "0  0.355948          -0.056412       0.106787       -0.084668  0.138885    \n",
       "\n",
       "   youtubes    youve  youve got    yovino       yoy       ypt       ypu  \\\n",
       "0 -0.297487  0.22545  0.145129   0.214343  0.080091  0.302283 -0.335726   \n",
       "\n",
       "         yr    yr old        yt      yuck     yummy      yup   zambia  \\\n",
       "0  0.155163 -0.212949  0.169459  0.067619  0.074171  0.03195  0.01113   \n",
       "\n",
       "      zarya        ze   zealand  zealand report  zealand report human  \\\n",
       "0  0.017715  0.008347  0.008347  0.417368        0.271601               \n",
       "\n",
       "   zealand support  zealand support submission    zealot   zealous  zeitgeist  \\\n",
       "0 -0.511137        -0.283006                    0.784665 -0.021755 -0.527261    \n",
       "\n",
       "      zelda      zero  zero actual  zero dawn  zero dollar  zero effect  \\\n",
       "0 -0.095838  0.096313  0.363583     0.104756  -0.033616    -0.016057      \n",
       "\n",
       "   zero evidence  zero experience  zero personal  zero personal experience  \\\n",
       "0 -0.016057      -0.016057        -0.020875       0.039515                   \n",
       "\n",
       "   zero personal experience oppression  zero precedent  zero proof  \\\n",
       "0  0.178559                             0.058939        0.109902     \n",
       "\n",
       "   zero responsibility  zero sense  zero sum  zero sum game  \\\n",
       "0 -0.155147             0.059649    0.094529 -0.119047        \n",
       "\n",
       "   zero sum situation  zero sympathy  zero tolerance  zero tolerance policy  \\\n",
       "0 -0.291584            0.432402      -0.150712       -0.029039                \n",
       "\n",
       "   zero value   zeroing  zimbardo       zip    zipper    zizek       zoe  \\\n",
       "0  0.225849   -0.016697  0.157491  0.259206 -0.201174 -0.30915 -0.115108   \n",
       "\n",
       "   zoe quinn    zombie  zombie apocalypse      zone  zone problematic  \\\n",
       "0 -0.136204   0.771067  0.086986           0.074171 -0.048134           \n",
       "\n",
       "        zoo      zoom  zuckerberg    zygote  \n",
       "0  0.093929  0.014135  0.077032    0.428348  \n",
       "\n",
       "[1 rows x 125000 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "military base                    5.529991\n",
       "broad generalization             4.476077\n",
       "article quote                    4.112461\n",
       "shouldn question idea            3.429160\n",
       "bewildered                       3.273144\n",
       "minute slightly increased        3.245495\n",
       "bro jap                          3.156064\n",
       "masculinity boy                  3.101634\n",
       "mra incel                        3.046819\n",
       "shared common                    2.923378\n",
       "extensively                      2.895841\n",
       "experiential authority member    2.846495\n",
       "mother choose                    2.794948\n",
       "semantic                         2.685628\n",
       "hear trans                       2.653834\n",
       "dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most positively correlated with content being from MensRights\n",
    "# some of these make NO sense, might have been good to do more cleaning\n",
    "coef_df.sum(axis=0).sort_values(ascending=False)[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pretty important law             -5.759800\n",
       "tell laugh joke pretty           -4.878412\n",
       "support accused                  -4.871705\n",
       "end jail                         -3.916339\n",
       "designated disposal area         -3.349880\n",
       "open feeling                     -3.347517\n",
       "single conversation              -3.258743\n",
       "argument opinion                 -3.215864\n",
       "haram                            -3.131597\n",
       "dominated dominated              -3.064219\n",
       "argument waste                   -3.058328\n",
       "fair deserve miserable death     -3.045156\n",
       "physical violent abuse sharing   -3.019294\n",
       "muslim absolutely equal          -2.755272\n",
       "britain agonizing day            -2.743639\n",
       "dtype: float64"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words most positively correlated with content being from AskFeminists\n",
    "# these make a little more sense than the MensRights ones...slightly.\n",
    "coef_df.sum(axis=0).sort_values(ascending=True)[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_log = logreg.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11591,)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_log.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11591,)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 4266\n",
      "False Positives: 1525\n",
      "False Negatives: 1303\n",
      "True Positives: 4497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, pred_log)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred_log).ravel()\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up df for comparison\n",
    "y_test_preds = pd.DataFrame(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preds['pred'] = pred_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_MensRights</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7102</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20531</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32532</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_MensRights  pred\n",
       "5938   0                     0   \n",
       "153    0                     0   \n",
       "7102   0                     0   \n",
       "20531  0                     1   \n",
       "32532  1                     1   "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit_MensRights</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20531</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48911</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27098</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17941</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29097</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit_MensRights  pred\n",
       "20531  0                     1   \n",
       "48911  1                     0   \n",
       "27098  0                     1   \n",
       "17941  0                     1   \n",
       "29097  1                     0   "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows where predictions don't match\n",
    "y_test_preds[y_test_preds['subreddit_MensRights'] != y_test_preds['pred']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                   19820                                                                                                                                                                                                                                                   \n",
       "text                    The first clip legitimately doesn't work, and none from the second are JP so I've still not seen proof of your origional claim that JP ever said that. If you send me a functioning clip then I'll change my opinion but it just says video unavailable.\n",
       "type                    comment                                                                                                                                                                                                                                                 \n",
       "removed                 0                                                                                                                                                                                                                                                       \n",
       "deleted                 0                                                                                                                                                                                                                                                       \n",
       "clean_text_stop         the first clip legitimately doesn t work  and none from the second are jp so i ve still not seen proof of your origional claim that jp ever said that  if you send me a functioning clip then i ll change my opinion but it just says video unavailable \n",
       "lems                    the first clip legitimately doesn work and none from the second are jp so i still not seen proof of your origional claim that jp ever said that if you send me a functioning clip then i change my opinion but it just say video unavailable            \n",
       "subreddit_MensRights    1                                                                                                                                                                                                                                                       \n",
       "Name: 57021, dtype: object"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a post misclassified as AskFeminists\n",
    "#this comment is just responding to another user who posted something about JP (Jordan Peterson)\n",
    "# but there's no real context that would help identify which thread this belongs to\n",
    "femvmen.loc[57021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                   19170                                                                     \n",
       "text                    Dude, I can't argue with you when you are so deep in the dogma. Good luck.\n",
       "type                    comment                                                                   \n",
       "removed                 0                                                                         \n",
       "deleted                 0                                                                         \n",
       "clean_text_stop         dude  i can t argue with you when you are so deep in the dogma  good luck \n",
       "lems                    dude i can argue with you when you are so deep in the dogma good luck     \n",
       "subreddit_MensRights    0                                                                         \n",
       "Name: 18964, dtype: object"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a post misclassified as MensRights\n",
    "# not much to go off here for classification, it could really be from either.\n",
    "femvmen.loc[18964]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried all of the below models before selecting the Logistic Regression for further analysis of the coefficients - none of these had as strong a test score as the one on the Logistic Regression. I spent more time with Random Forest, and I could have tried to tune the parameters for Naive Bayes and SVC to improve the scores but didn't have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train_tf2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8530109567768096"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_train_tf2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7464412043827108"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(X_test_tf2, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(kernel= 'rbf', C = 100, gamma = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.05, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9867785350703132"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7386765593995341"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking sample for faster gridsearching\n",
    "sample20000 = femvmen.sample(n=20000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.5018\n",
       "0    0.4982\n",
       "Name: subreddit_MensRights, dtype: float64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample20000['subreddit_MensRights'].value_counts(normalize=True) # about same proportions as full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sample20000['lems']\n",
    "y = sample20000['subreddit_MensRights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size = .30, \n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range = (1, 4), max_features = 125000, stop_words = \n",
    "                        'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14000, 125000)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just trying out basic random forest without tuning parameters or setting max depth\n",
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.981"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6566666666666666"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearching for best parameters - this is the first one I did\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6554285714285715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 90000,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -2,\n",
       " 'oob_score': 'False',\n",
       " 'warm_start': 'True'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'n_estimators': [5, 10, 1],\n",
    "    'max_depth': [10000, 90000, 20000],\n",
    "    'oob_score': ['True', 'False'],\n",
    "    'warm_start': ['True'],\n",
    "    'n_jobs': [-2]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid = params, cv = 3)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6618571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:453: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:458: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 885,\n",
       " 'n_estimators': 10,\n",
       " 'n_jobs': -2,\n",
       " 'oob_score': 'False',\n",
       " 'warm_start': 'True'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# second gridsearch, I repeated this until I narrowed down the best max depth range\n",
    "params = {\n",
    "    'n_estimators': [10],\n",
    "    'max_depth': range(875, 900, 1),\n",
    "    'oob_score': ['False'],\n",
    "    'warm_start': ['True'],\n",
    "    'n_jobs': [-2]\n",
    "}\n",
    "gs = GridSearchCV(rf, param_grid = params, cv = 3)\n",
    "gs.fit(X_train_tf, y_train)\n",
    "print(gs.best_score_)\n",
    "gs.best_params_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting X and y variables to test random forest on full train/test sets\n",
    "X = femvmen['lems']\n",
    "y = femvmen['subreddit_MensRights']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = .25,\n",
    "                                                   stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf = tfidf.fit_transform(X_train)\n",
    "X_test_tf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 880, n_estimators = 1000, n_jobs = -2, oob_score = False,\n",
    "                 warm_start = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=880, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-2,\n",
       "            oob_score=False, random_state=None, verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.984401601251553"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train_tf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7180619780523155"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test_tf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be too easy to say that my best model's inability to crack 77% accuracy means that \"we're really not that different after all\", and if I were to spend more time on this project there are other options I would explore to try and improve my model:\n",
    "\n",
    "1. I'd pull a completely unrelated subreddit to test as a control against the other two subreddits to make sure I was tuning the best model possible.\n",
    "\n",
    "2. Given how important context is for the comments, I would either try to aggregate comments on a post and analyze them together, or set a minimum wordcount on comments to be used in the analysis to give the model a better chance of distinguishing them.\n",
    "\n",
    "3. I could scrape another years' worth of data to add to the model.\n",
    "\n",
    "4. I could do some more cleaning and EDA that might help consolidate slang/similar terms that weren't captured by the lemmatizer, and identify stronger trends in the subreddits that could be leveraged for better prediction. It'd also be interesting to see what difference it might make to fit the TFIDF on just one subreddit first, then transform both subreddits and analyze them together.\n",
    "\n",
    "5. If any of the above helped minimize overfit on the Logistic Regression or Random Forest, I'd give Naive Bayes Multinomial and Support Vector Classifier another shot (and spend more time tuning them).\n",
    "\n",
    "The really interesting questions this project has generated would require a deeper analysis: what's the overlap in people who post on MensRights and AskFeminists? (How many of those are trolls, whose content is removed?) What common themes exist between removed posts (can we write an algorithm for trolling content?) How does sentiment analysis compare across the two subreddits, is there a discernable difference? Is it possible to measure 'extreme' attitudes in a subreddit, and if so can it be mapped over time, against real events happening in the world that might trigger anger on both or one side? Is it possible to follow the posts of single users and see how they change in sentiment and neutrality over time (and does the sub they post on the most make a difference?) Maybe I'll come back to it when I've solidified my modeling skills."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
